{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dfffc96-8ee2-4558-b4ae-578324a716cc",
   "metadata": {},
   "source": [
    "# Paths & knobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab1d2f8-4f1f-43ad-a512-e12ce2410e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 0: paths & knobs ====\n",
    "from pathlib import Path\n",
    "ROOT     = Path(\"/Users/hsiaopingni/Desktop/octaneX\")\n",
    "DATA_DIR = Path(\"/Users/hsiaopingni/Desktop/SLM_RAS-main/HW_TELEMETRY_DATA_COLLECTION/TELEMETRY_DATA\")\n",
    "\n",
    "RES_DIR  = ROOT / \"Results\";            RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR= RES_DIR / \"ROC_PR\";          PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MMI_DIR  = ROOT / \"FeatureRankOUT\"                 # your existing MMI ranks\n",
    "HYB_DIR  = ROOT / \"FeatureRankOUT_HYBRID\";         HYB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXPL_DIR = RES_DIR / \"Explainability\";             EXPL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HYBRID_REPORT_DIR = RES_DIR / \"Hybrid_Reports\";    HYBRID_REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SETUPS        = [\"DDR4\",\"DDR5\"]\n",
    "WINDOW_SIZES  = [32, 64, 128, 256, 512, 1024]\n",
    "KFOLDS_SET    = [3, 5, 10]\n",
    "SUBSPACES     = [\"compute\",\"memory\",\"sensors\"]\n",
    "META          = [\"label\",\"setup\",\"run_id\"]\n",
    "SEED          = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b61b08-3807-4e09-9ebe-cb5d7d0c234d",
   "metadata": {},
   "source": [
    "# Shared helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c70cc6-5d46-49c4-b226-718f042d335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Cell 1: shared helpers ====\n",
    "import re, hashlib, numpy as np, pandas as pd\n",
    "\n",
    "def iter_raw_csvs(root: Path):\n",
    "    for p in root.rglob(\"*.csv\"): yield p\n",
    "\n",
    "def read_csv_clean(p: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(p)\n",
    "    return df.loc[:, ~df.columns.str.startswith(\"Unnamed\")]\n",
    "\n",
    "def detect_setup_from_path(p: Path):\n",
    "    s = str(p).lower()\n",
    "    return \"DDR4\" if \"ddr4\" in s else (\"DDR5\" if \"ddr5\" in s else None)\n",
    "\n",
    "def is_benign_path(p: Path) -> bool:\n",
    "    s = str(p).lower()\n",
    "    if \"benign\" in s: return True\n",
    "    bad = [\"attack\",\"anom\",\"fault\",\"inject\",\"trojan\",\"mal\",\"rh\",\"droop\",\"spectre\",\"trrespass\"]\n",
    "    return not any(b in s for b in bad)\n",
    "\n",
    "def is_anomaly_path(p: Path, anomaly: str) -> bool:\n",
    "    return (anomaly.lower() in str(p).lower()) and (not is_benign_path(p))\n",
    "\n",
    "def collect_raw_pairs_by_setup(data_dir: Path, which: str, anomaly: str|None=None):\n",
    "    out = {\"DDR4\": [], \"DDR5\": []}\n",
    "    for p in iter_raw_csvs(data_dir):\n",
    "        setup = detect_setup_from_path(p)\n",
    "        if setup is None: continue\n",
    "        if which == \"benign\":\n",
    "            if not is_benign_path(p): continue\n",
    "        else:\n",
    "            if anomaly is None or not is_anomaly_path(p, anomaly): continue\n",
    "        try:\n",
    "            out[setup].append((p, read_csv_clean(p)))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] read failed {p}: {e}\")\n",
    "    return out\n",
    "\n",
    "def mk_run_id(path: Path) -> str:\n",
    "    return f\"run_{hashlib.md5(str(path).encode('utf-8')).hexdigest()[:10]}\"\n",
    "\n",
    "def telemetry_cols(df: pd.DataFrame) -> list[str]:\n",
    "    return [c for c in df.columns if c not in META and df[c].dtype.kind in \"fcbiu\"]\n",
    "\n",
    "def window_collapse_means(df: pd.DataFrame, win: int, setup: str, run_id: str, label: str) -> pd.DataFrame:\n",
    "    cols = telemetry_cols(df)\n",
    "    if not cols: return pd.DataFrame()\n",
    "    rows = []\n",
    "    for start in range(0, len(df) - win + 1, win):\n",
    "        means = df.iloc[start:start+win][cols].astype(float).mean(axis=0, numeric_only=True)\n",
    "        row = means.to_frame().T\n",
    "        row[\"setup\"] = setup; row[\"run_id\"] = run_id; row[\"label\"] = label\n",
    "        rows.append(row)\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "def build_windowed_raw_means(pairs, setup: str, win: int, label: str) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for p, df in pairs:\n",
    "        agg = window_collapse_means(df, win=win, setup=setup, run_id=mk_run_id(p), label=label)\n",
    "        if not agg.empty: out.append(agg)\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "# robust scaling (winsor + z) fit on BENIGN\n",
    "def robust_scale_train(Xb_np: np.ndarray, winsor=(2.0, 98.0)):\n",
    "    Q1, Q2 = np.percentile(Xb_np, winsor[0], axis=0), np.percentile(Xb_np, winsor[1], axis=0)\n",
    "    mu = np.clip(Xb_np, Q1, Q2).mean(axis=0)\n",
    "    sd = np.clip(Xb_np, Q1, Q2).std(axis=0, ddof=0) + 1e-9\n",
    "    return mu, sd, Q1, Q2\n",
    "\n",
    "def apply_robust_scale(X: pd.DataFrame, mu, sd, Q1, Q2):\n",
    "    Z = (np.clip(X.values, Q1, Q2) - mu) / sd\n",
    "    return pd.DataFrame(Z, columns=X.columns, index=X.index)\n",
    "\n",
    "# subspace map\n",
    "PATS = {\n",
    "    \"memory\":  [r\"ddr|dram|mem(ory)?\\b\", r\"\\bL1(\\b|_)|L2(\\b|_)|L3(\\b|_)\", r\"(L[123].*(HIT|MISS|MPI))\\b\",\n",
    "               r\"cache|fill|evict|wb|rd|wr|load|store\", r\"bandwidth|bw|throughput|qdepth|queue\",\n",
    "               r\"lat(ency)?|stall.*mem|tCCD|tRCD|tRP|tCL|page|row|col\"],\n",
    "    \"sensors\": [r\"temp|thermal|hot\",\n",
    "                r\"volt|vdd|vcore|vin|vout|cpu[_\\- ]?volt|cpu[_\\- ]?vdd|vdd[_\\- ]?cpu|core[_\\- ]?volt|core[_\\- ]?voltage\",\n",
    "                r\"power|watt|energy|joule\", r\"fan|throttle|current|amps?\"],\n",
    "    \"compute\": [r\"\\bIPC\\b|\\bPhysIPC\\b|\\bEXEC\\b|\\bissue|retire|dispatch\\b\",\n",
    "                r\"\\bINST\\b|INSTnom%|branch|mispred|\\balu\\b|\\barith\\b|\\blogic\\b\",\n",
    "                r\"C0res%|C1res%|C6res%|C7res%|CFREQ|AFREQ|ACYC|CYC|TIME|clk|cycle|freq|util\",\n",
    "                r\"\\bcore|cpu|sm|warp|shader\\b\"],\n",
    "}\n",
    "def subspace_of_feature(name: str) -> str:\n",
    "    b = str(name)\n",
    "    if any(re.search(p, b, flags=re.I) for p in PATS[\"memory\"]):  return \"memory\"\n",
    "    if any(re.search(p, b, flags=re.I) for p in PATS[\"sensors\"]): return \"sensors\"\n",
    "    return \"compute\"\n",
    "\n",
    "# rank loader (MMI or Hybrid)\n",
    "def read_rank_list(rank_dir: Path, setup: str, win: int, kfold: int, sub: str) -> list[str]:\n",
    "    p = rank_dir / f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "    if not p.exists(): return []\n",
    "    df = pd.read_csv(p)\n",
    "    col = \"feature\" if \"feature\" in df.columns else df.columns[0]\n",
    "    return df[col].dropna().astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3139e5-d205-41de-bc8e-770c4b78c337",
   "metadata": {},
   "source": [
    "# Feature ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629e83f-d5d6-4e44-8581-8a9c14c43301",
   "metadata": {},
   "source": [
    "## X-OCTANE: RAW windows (WIN, overlap, z-norm, sampling) → KFold → PC1_vs_PC1 (MI) ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb8c5c02-bbad-4714-a800-431a594ca52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DISCOVER] benign RAW files → DDR4: 13, DDR5: 13\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_32_20_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_64_20_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_128_20_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_256_20_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_512_20_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR4_1024_20_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_32_20_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_64_20_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_128_20_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_256_20_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_512_20_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_3_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_3_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_3_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_5_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_5_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_5_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_10_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_10_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_10_0_sensors.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_20_0_compute.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_20_0_memory.csv\n",
      "[WRITE] /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT/DDR5_1024_20_0_sensors.csv\n",
      "\n",
      "[DONE] RAW PC1_vs_PC1 (MI) K-fold rankings written.\n"
     ]
    }
   ],
   "source": [
    "# Outputs: FeatureRankOUT/DDR{4|5}_{W}_{K}_0_{compute|memory|sensors}.csv  (feature,score)\n",
    "\n",
    "import re, hashlib, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ------------------ PATHS ------------------\n",
    "DATA_DIR = Path(\"/Users/hsiaopingni/Desktop/SLM_RAS-main/HW_TELEMETRY_DATA_COLLECTION/TELEMETRY_DATA\")\n",
    "ROOT     = Path(\"/Users/hsiaopingni/octaneX_v7_4functions\")\n",
    "OUT_DIR  = ROOT / \"FeatureRankOUT\"; OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------ CONFIG ------------------\n",
    "WINDOW_SIZES: List[int] = [32, 64, 128, 256, 512, 1024]\n",
    "OVERLAP: float          = 0.5        # 0.0.. <1.0 ; stride = int(W*(1-OVERLAP))\n",
    "KFOLDS_SET: List[int]   = [3, 5, 10, 20]\n",
    "SEED                    = 42\n",
    "META                    = [\"label\",\"setup\",\"run_id\"]\n",
    "\n",
    "# Sampling inside each window -> one scalar per signal per window (no means)\n",
    "SAMPLING: str           = \"center\"   # {\"center\", \"first\", \"last\", \"offset:<int>\"}\n",
    "\n",
    "# ------------------ File discovery (benign RAW only) ------------------\n",
    "def is_benign_path(p: Path) -> bool:\n",
    "    s = str(p).lower()\n",
    "    if \"benign\" in s: \n",
    "        return True\n",
    "    bad = [\"attack\",\"anom\",\"fault\",\"inject\",\"trojan\",\"mal\",\"rh\",\"droop\",\"spectre\",\"trrespass\"]\n",
    "    return not any(b in s for b in bad)\n",
    "\n",
    "def detect_setup_from_path(p: Path) -> Optional[str]:\n",
    "    s = str(p).lower()\n",
    "    if \"ddr4\" in s: return \"DDR4\"\n",
    "    if \"ddr5\" in s: return \"DDR5\"\n",
    "    return None\n",
    "\n",
    "def iter_raw_csvs(root: Path):\n",
    "    for p in root.rglob(\"*.csv\"):\n",
    "        yield p\n",
    "\n",
    "def read_csv_clean(p: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(p)\n",
    "    df = df.loc[:, ~df.columns.str.startswith(\"Unnamed\")]\n",
    "    return df\n",
    "\n",
    "def mk_run_id(path: Path) -> str:\n",
    "    return f\"run_{hashlib.md5(str(path).encode('utf-8')).hexdigest()[:10]}\"\n",
    "\n",
    "def unify_benign_by_setup(data_dir: Path) -> Dict[str, List[Tuple[Path, pd.DataFrame]]]:\n",
    "    buckets = {\"DDR4\": [], \"DDR5\": []}\n",
    "    for p in iter_raw_csvs(data_dir):\n",
    "        if not is_benign_path(p): \n",
    "            continue\n",
    "        setup = detect_setup_from_path(p)\n",
    "        if setup is None:\n",
    "            continue\n",
    "        try:\n",
    "            df = read_csv_clean(p)\n",
    "            if len(df) > 0:\n",
    "                buckets[setup].append((p, df))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] failed reading {p}: {e}\")\n",
    "    print(f\"[DISCOVER] benign RAW files → DDR4: {len(buckets['DDR4'])}, DDR5: {len(buckets['DDR5'])}\")\n",
    "    return buckets\n",
    "\n",
    "# ------------------ Subspace mapping ------------------\n",
    "PATS = {\n",
    "    \"memory\": [\n",
    "        r\"ddr|dram|mem(ory)?\\b\", r\"\\bL1(\\b|_)|L2(\\b|_)|L3(\\b|_)\", r\"(L[123].*(HIT|MISS|MPI))\\b\",\n",
    "        r\"cache|fill|evict|wb|rd|wr|load|store\", r\"bandwidth|bw|throughput|qdepth|queue\",\n",
    "        r\"lat(ency)?|stall.*mem|tCCD|tRCD|tRP|tCL|page|row|col\",\n",
    "    ],\n",
    "    \"sensors\": [\n",
    "        r\"temp|thermal|hot\",\n",
    "        r\"volt|vdd|vcore|vin|vout|cpu[_\\- ]?volt|cpu[_\\- ]?vdd|vdd[_\\- ]?cpu|core[_\\- ]?volt|core[_\\- ]?voltage\",\n",
    "        r\"power|watt|energy|joule\",\n",
    "        r\"fan|throttle|current|amps?\",\n",
    "    ],\n",
    "    \"compute\": [\n",
    "        r\"\\bIPC\\b|\\bPhysIPC\\b|\\bEXEC\\b|\\bissue|retire|dispatch\\b\",\n",
    "        r\"\\bINST\\b|INSTnom%|branch|mispred|\\balu\\b|\\barith\\b|\\blogic\\b\",\n",
    "        r\"C0res%|C1res%|C6res%|C7res%|CFREQ|AFREQ|ACYC|CYC|TIME|clk|cycle|freq|util\",\n",
    "        r\"\\bcore|cpu|sm|warp|shader\\b\",\n",
    "    ],\n",
    "}\n",
    "def subspace_of_feature(base_feat: str) -> str:\n",
    "    b = str(base_feat)\n",
    "    if any(re.search(p, b, flags=re.I) for p in PATS[\"memory\"]):  return \"memory\"\n",
    "    if any(re.search(p, b, flags=re.I) for p in PATS[\"sensors\"]): return \"sensors\"\n",
    "    return \"compute\"\n",
    "\n",
    "# ------------------ RAW → overlapped windows → z-norm → sampling ------------------\n",
    "def telemetry_cols(df: pd.DataFrame) -> list[str]:\n",
    "    return [c for c in df.columns if c not in META and pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "def _stride(win: int, overlap: float) -> int:\n",
    "    step = max(1, int(round(win * (1.0 - overlap))))\n",
    "    return max(1, min(step, win))  # never 0; allow step==win for non-overlap\n",
    "\n",
    "def _pick_index(start: int, win: int) -> int:\n",
    "    if SAMPLING == \"first\":\n",
    "        return start\n",
    "    if SAMPLING == \"last\":\n",
    "        return start + win - 1\n",
    "    if SAMPLING.startswith(\"offset:\"):\n",
    "        try:\n",
    "            off = int(SAMPLING.split(\":\",1)[1])\n",
    "        except Exception:\n",
    "            off = win // 2\n",
    "        off = np.clip(off, 0, win-1)\n",
    "        return start + off\n",
    "    # default = \"center\"\n",
    "    return start + (win // 2)\n",
    "\n",
    "def build_window_samples(df: pd.DataFrame, win: int, overlap: float,\n",
    "                         setup: str, run_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Produce one scalar per signal per window by sampling a single time index inside each window\n",
    "    after global z-normalization per signal (no within-window mean).\n",
    "    \"\"\"\n",
    "    cols = telemetry_cols(df)\n",
    "    if not cols:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    X = df[cols].astype(float)\n",
    "    # Global z-norm per signal (over full run)\n",
    "    X = (X - X.mean(axis=0)) / (X.std(axis=0, ddof=0) + 1e-9)\n",
    "    X = X.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "    n = len(X)\n",
    "    step = _stride(win, overlap)\n",
    "    rows = []\n",
    "    for start in range(0, n - win + 1, step):\n",
    "        idx = _pick_index(start, win)\n",
    "        sample = X.iloc[idx:idx+1].copy()\n",
    "        sample[\"setup\"] = setup\n",
    "        sample[\"run_id\"] = run_id\n",
    "        sample[\"label\"] = \"BENIGN\"\n",
    "        rows.append(sample)\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "def build_all_window_samples(pairs: List[Tuple[Path, pd.DataFrame]],\n",
    "                             setup: str, win: int, overlap: float) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for p, df in pairs:\n",
    "        rid = mk_run_id(p)\n",
    "        part = build_window_samples(df, win=win, overlap=overlap, setup=setup, run_id=rid)\n",
    "        if not part.empty:\n",
    "            out.append(part)\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "# ------------------ PC1 vs PC1 (MI) ------------------\n",
    "def _zscore_df(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    Z = (X - X.mean(axis=0)) / (X.std(axis=0, ddof=0) + 1e-9)\n",
    "    return Z.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
    "\n",
    "def _quantile_bin(x: np.ndarray, q: int = 16) -> np.ndarray:\n",
    "    x = np.asarray(x, float)\n",
    "    mask = np.isfinite(x)\n",
    "    if mask.sum() == 0:\n",
    "        return np.full_like(x, -1, dtype=int)\n",
    "    edges = np.quantile(x[mask], np.linspace(0,1,q+1))\n",
    "    edges[0]  -= 1e-9; edges[-1] += 1e-9\n",
    "    out = np.full_like(x, -1, dtype=int)\n",
    "    out[mask] = np.digitize(x[mask], edges[1:-1], right=False)\n",
    "    return out\n",
    "\n",
    "def _discrete_mi(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    mask = (a >= 0) & (b >= 0)\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    A = a[mask].astype(int); B = b[mask].astype(int)\n",
    "    na, nb = A.max()+1, B.max()+1\n",
    "    idx = (A * nb + B)\n",
    "    binc = np.bincount(idx, minlength=na*nb).astype(float).reshape(na, nb)\n",
    "    joint = binc / binc.sum()\n",
    "    pa = joint.sum(axis=1, keepdims=True); pb = joint.sum(axis=0, keepdims=True)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        ratio = joint / (pa @ pb)\n",
    "        mi = np.nansum(joint * np.log(ratio + 1e-12))\n",
    "    return float(max(mi, 0.0))\n",
    "\n",
    "def pc1_vs_pc1_mi_scores(df_windows: pd.DataFrame, qbins: int = 16) -> pd.Series:\n",
    "    \"\"\"\n",
    "    For each feature i:\n",
    "      - per-signal PC1 = z-scored column Xi (samples = windows)\n",
    "      - others PC1     = PCA(X_{-i}) first PC scores\n",
    "      - score_i        = MI(qbin(Xi), qbin(PC1_{others}))\n",
    "    \"\"\"\n",
    "    feat_cols = [c for c in df_windows.columns if c not in META and pd.api.types.is_numeric_dtype(df_windows[c])]\n",
    "    if not feat_cols:\n",
    "        return pd.Series(dtype=float)\n",
    "\n",
    "    X = _zscore_df(df_windows[feat_cols].astype(float))\n",
    "    names = list(X.columns)\n",
    "    X_arr = X.values\n",
    "    n, d = X_arr.shape\n",
    "    scores = []\n",
    "\n",
    "    for i in range(d):\n",
    "        if d == 1:\n",
    "            scores.append(0.0); continue\n",
    "        # Per-signal PC1 (1-D) = standardized Xi\n",
    "        xi = X_arr[:, i]\n",
    "        # Others PC1\n",
    "        others = np.delete(X_arr, i, axis=1)\n",
    "        # If degenerate, fallback to mean of others (still no window mean used anywhere; it's across windows)\n",
    "        try:\n",
    "            pca = PCA(n_components=1, svd_solver=\"auto\", random_state=SEED)\n",
    "            pc1_scores = pca.fit_transform(others).ravel()\n",
    "        except Exception:\n",
    "            pc1_scores = others.mean(axis=1)\n",
    "\n",
    "        ai = _quantile_bin(xi, q=qbins)\n",
    "        bi = _quantile_bin(pc1_scores, q=qbins)\n",
    "        scores.append(_discrete_mi(ai, bi))\n",
    "\n",
    "    s = pd.Series(scores, index=names, name=\"score\").sort_values(ascending=False)\n",
    "    return s\n",
    "\n",
    "def _normalize_01(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(float)\n",
    "    lo, hi = float(s.min()), float(s.max())\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi):\n",
    "        return s.fillna(0.0)\n",
    "    if hi == lo:\n",
    "        return pd.Series(np.zeros(len(s)), index=s.index)\n",
    "    return (s - lo) / (hi - lo)\n",
    "\n",
    "def split_and_save(scores: pd.Series, setup: str, win: int, kfold: int):\n",
    "    s_norm = _normalize_01(scores)\n",
    "    buckets = {\"compute\": [], \"memory\": [], \"sensors\": []}\n",
    "    for feat, sc in s_norm.items():\n",
    "        buckets[subspace_of_feature(feat)].append((feat, sc))\n",
    "    for sub, pairs in buckets.items():\n",
    "        if not pairs:\n",
    "            continue\n",
    "        df_sub = (\n",
    "            pd.DataFrame(pairs, columns=[\"feature\",\"score\"])\n",
    "              .sort_values(\"score\", ascending=False, kind=\"mergesort\")\n",
    "        )\n",
    "        out_path = OUT_DIR / f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "        df_sub.to_csv(out_path, index=False)\n",
    "        print(f\"[WRITE] {out_path}\")\n",
    "\n",
    "# ------------------ MAIN ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    buckets = unify_benign_by_setup(DATA_DIR)\n",
    "\n",
    "    for setup in [\"DDR4\",\"DDR5\"]:\n",
    "        pairs = buckets.get(setup, [])\n",
    "        if not pairs:\n",
    "            print(f\"[WARN] No benign RAW {setup} files found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for W in WINDOW_SIZES:\n",
    "            df_win = build_all_window_samples(pairs, setup=setup, win=W, overlap=OVERLAP)\n",
    "            if df_win.empty:\n",
    "                print(f\"[SKIP] {setup} win={W}: no window samples produced.\")\n",
    "                continue\n",
    "\n",
    "            # type guard for meta\n",
    "            for m in META:\n",
    "                if m in df_win.columns:\n",
    "                    df_win[m] = df_win[m].astype(str)\n",
    "\n",
    "            for K in KFOLDS_SET:\n",
    "                # Prefer grouping by run_id to avoid leakage\n",
    "                if \"run_id\" in df_win.columns and df_win[\"run_id\"].nunique() >= K:\n",
    "                    splitter = GroupKFold(n_splits=K)\n",
    "                    splits = splitter.split(df_win, groups=df_win[\"run_id\"].values)\n",
    "                else:\n",
    "                    splitter = KFold(n_splits=K, shuffle=True, random_state=SEED)\n",
    "                    splits = splitter.split(df_win)\n",
    "\n",
    "                fold_scores = []\n",
    "                for tr_idx, _ in splits:\n",
    "                    df_tr = df_win.iloc[tr_idx]\n",
    "                    s = pc1_vs_pc1_mi_scores(df_tr, qbins=16)\n",
    "                    fold_scores.append(s)\n",
    "\n",
    "                if not fold_scores:\n",
    "                    print(f\"[SKIP] {setup} win={W} k={K}: no folds produced.\")\n",
    "                    continue\n",
    "\n",
    "                M = pd.concat(fold_scores, axis=1).fillna(0.0)\n",
    "                mean_s = M.mean(axis=1).sort_values(ascending=False)\n",
    "                split_and_save(mean_s, setup=setup, win=W, kfold=K)\n",
    "\n",
    "    print(\"\\n[DONE] RAW PC1_vs_PC1 (MI) K-fold rankings written.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7df7d-c209-42af-a7e3-317180cfb43e",
   "metadata": {},
   "source": [
    "# **X-OCTANE paper: FULL MULTI-WIN / MULTI-K PIPELINE (AUTHENTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b134d86e-fa31-400f-89e6-f6695ca7de04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Cannot write to /Volumes/Untitled/octaneX_results. Using local: /Users/hsiaopingni/octaneX_v7_4functions/Results\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=32 KF=3 (group 1)\n",
      "\n",
      "[RUN] DDR4 RH WIN=32 KF=3 (group 2)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=32 KF=5 (group 3)\n",
      "\n",
      "[RUN] DDR4 RH WIN=32 KF=5 (group 4)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=32 KF=10 (group 5)\n",
      "\n",
      "[RUN] DDR4 RH WIN=32 KF=10 (group 6)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=64 KF=3 (group 7)\n",
      "\n",
      "[RUN] DDR4 RH WIN=64 KF=3 (group 8)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=64 KF=5 (group 9)\n",
      "\n",
      "[RUN] DDR4 RH WIN=64 KF=5 (group 10)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=64 KF=10 (group 11)\n",
      "\n",
      "[RUN] DDR4 RH WIN=64 KF=10 (group 12)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=128 KF=3 (group 13)\n",
      "\n",
      "[RUN] DDR4 RH WIN=128 KF=3 (group 14)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=128 KF=5 (group 15)\n",
      "\n",
      "[RUN] DDR4 RH WIN=128 KF=5 (group 16)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=128 KF=10 (group 17)\n",
      "\n",
      "[RUN] DDR4 RH WIN=128 KF=10 (group 18)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=512 KF=3 (group 19)\n",
      "\n",
      "[RUN] DDR4 RH WIN=512 KF=3 (group 20)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=512 KF=5 (group 21)\n",
      "\n",
      "[RUN] DDR4 RH WIN=512 KF=5 (group 22)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=512 KF=10 (group 23)\n",
      "\n",
      "[RUN] DDR4 RH WIN=512 KF=10 (group 24)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=1024 KF=3 (group 25)\n",
      "\n",
      "[RUN] DDR4 RH WIN=1024 KF=3 (group 26)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=1024 KF=5 (group 27)\n",
      "\n",
      "[RUN] DDR4 RH WIN=1024 KF=5 (group 28)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=1024 KF=10 (group 29)\n",
      "\n",
      "[RUN] DDR4 RH WIN=1024 KF=10 (group 30)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=32 KF=3 (group 31)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=32 KF=3 (group 32)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=32 KF=5 (group 33)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=32 KF=5 (group 34)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=32 KF=10 (group 35)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=32 KF=10 (group 36)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=64 KF=3 (group 37)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=64 KF=3 (group 38)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=64 KF=5 (group 39)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=64 KF=5 (group 40)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=64 KF=10 (group 41)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=64 KF=10 (group 42)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=128 KF=3 (group 43)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=128 KF=3 (group 44)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=128 KF=5 (group 45)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=128 KF=5 (group 46)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=128 KF=10 (group 47)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=128 KF=10 (group 48)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=512 KF=3 (group 49)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=512 KF=3 (group 50)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=512 KF=5 (group 51)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=512 KF=5 (group 52)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=512 KF=10 (group 53)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=512 KF=10 (group 54)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=1024 KF=3 (group 55)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=1024 KF=3 (group 56)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=1024 KF=5 (group 57)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=1024 KF=5 (group 58)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=1024 KF=10 (group 59)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=1024 KF=10 (group 60)\n",
      "\n",
      "[OK] ALLRUNS metrics → /Users/hsiaopingni/octaneX_v7_4functions/Results/per_run_metrics_all_PIPELINE.csv\n",
      "[DONE] Pipeline + DSE plots + weight tables per platform\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ===============================================================================================\n",
    "# FULL MULTI-WIN / MULTI-K PIPELINE (AUTHENTIC)\n",
    "# + DSE PLOTS (all cases)\n",
    "# + TABLE_SUBSPACEWEIGHTS (all cases)\n",
    "#\n",
    "# Runs:\n",
    "#   - WINS   = [32, 64, 128, 512, 1024]\n",
    "#   - KFOLDS = [3, 5, 10]\n",
    "#   - SETUPS:\n",
    "#       DDR4 anomalies: DROOP, RH\n",
    "#       DDR5 anomalies: DROOP, SPECTRE\n",
    "#   - PCT sweep: 10..100 step 10\n",
    "#\n",
    "# Produces:\n",
    "#   1) Results/per_run_metrics_all_PIPELINE.csv\n",
    "#   2) DSE plots for every (setup, anomaly, win, kfold):\n",
    "#        Results/DesignSpace/PaperStyle_ALLRUNS/<setup>/<anomaly>/WIN{win}_KF{kfold}/...\n",
    "#   3) Weight tables for every (setup, win, kfold):\n",
    "#        Results/Table_SubspaceWeights/<setup>/WIN{win}_KF{kfold}/...\n",
    "#\n",
    "# Key behaviors:\n",
    "#   - Workload-matched benign pairing by filename suffix (_dft/_oe/_tr etc.)\n",
    "#   - DROOP: append CPU Voltage + transient features to sensors selection (does NOT override)\n",
    "#   - Weight selection: PR-first on holdout split over candidate weight table\n",
    "#   - Scoring: draft-math dE/dC + aM/aJ (with softplus positivity for aJ)\n",
    "#\n",
    "# ===============================================================================================\n",
    "\n",
    "import gc, re, hashlib, warnings, unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ===============================================================================================\n",
    "# 0) PATHS\n",
    "# ===============================================================================================\n",
    "DATA_DIR = Path(\"/Users/hsiaopingni/Desktop/SLM_RAS-main/HW_TELEMETRY_DATA_COLLECTION/TELEMETRY_DATA\")\n",
    "ROOT     = Path(\"/Users/hsiaopingni/octaneX_v7_4functions\")\n",
    "RES_DIR  = ROOT / \"Results\"\n",
    "RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXT_DRIVE = Path(\"/Volumes/Untitled\")\n",
    "EXT_RES   = EXT_DRIVE / \"octaneX_results\"\n",
    "\n",
    "def _can_write_dir(p: Path) -> bool:\n",
    "    try:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        t = p / \".write_test\"\n",
    "        t.write_text(\"ok\")\n",
    "        t.unlink()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if _can_write_dir(EXT_RES):\n",
    "    RES_DIR = EXT_RES\n",
    "    RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[OK] Using external results dir: {RES_DIR}\")\n",
    "else:\n",
    "    print(f\"[WARN] Cannot write to {EXT_RES}. Using local: {RES_DIR}\")\n",
    "\n",
    "RANK_DIRS = [\n",
    "    ROOT / \"FeatureRankOUT\",\n",
    "    EXT_DRIVE / \"FeatureRankOUT\",\n",
    "    EXT_DRIVE / \"octaneX\" / \"FeatureRankOUT\",\n",
    "]\n",
    "\n",
    "# ===============================================================================================\n",
    "# 1) RUN CONFIG\n",
    "# ===============================================================================================\n",
    "SETUPS = [\"DDR4\", \"DDR5\"]\n",
    "ANOMALIES_BY_SETUP = {\"DDR4\": [\"DROOP\", \"RH\"], \"DDR5\": [\"DROOP\", \"SPECTRE\"]}\n",
    "\n",
    "WINS   = [32, 64, 128, 512, 1024]\n",
    "KFOLDS = [3, 5, 10]\n",
    "PCT_SWEEP = list(range(10, 101, 10))\n",
    "\n",
    "OVERLAP_RATIO       = 0.50\n",
    "OVERLAP_RATIO_DROOP = 0.80\n",
    "\n",
    "ROBUST_WINSOR = (2.0, 98.0)\n",
    "SEED = 1337\n",
    "\n",
    "GC_EVERY_N_PCTS   = 3\n",
    "GC_EVERY_N_GROUPS = 1\n",
    "\n",
    "META = [\"label\", \"setup\", \"run_id\"]\n",
    "DROOP_META_COLS = [\"droop_center_found\", \"droop_best_score_z\", \"droop_frac_ge_thr\", \"droop_vcols\"]\n",
    "\n",
    "BALANCE_EVAL_FOR_METRICS = True\n",
    "\n",
    "DROP_LOW_VARIANCE_COLS = True\n",
    "LOW_VAR_EPS = 1e-10\n",
    "\n",
    "# DROOP boost (small, optional)\n",
    "DROOP_BOOST = True\n",
    "DROOP_BOOST_ALPHA = 0.20\n",
    "DROOP_TRANSIENT_PAT = re.compile(r\"__(drop|range|slope|min|max|std)\", re.I)\n",
    "\n",
    "METHOD_ORDER = [\"dC_aJ\", \"dC_aM\", \"dE_aJ\", \"dE_aM\"]\n",
    "\n",
    "# ===============================================================================================\n",
    "# 2) PLOT CONFIG (draft-style, no overlaps)\n",
    "# ===============================================================================================\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 13.5,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"figure.titlesize\": 20,\n",
    "})\n",
    "\n",
    "METHOD_TITLE = {\n",
    "    \"dC_aJ\": r\"Scoring function $d_C(X)$ with Aggregate $a_J$\",\n",
    "    \"dC_aM\": r\"Scoring function $d_C(X)$ with Aggregate $a_M$\",\n",
    "    \"dE_aJ\": r\"Scoring function $d_E(X)$ with Aggregate $a_J$\",\n",
    "    \"dE_aM\": r\"Scoring function $d_E(X)$ with Aggregate $a_M$\",\n",
    "}\n",
    "\n",
    "PLOT_YMIN = 0.0\n",
    "PLOT_YMAX = 1.02\n",
    "ALPHA_MINMAX = 0.28\n",
    "ALPHA_IQR    = 0.60\n",
    "GRID_ALPHA   = 0.22\n",
    "GRID_LS      = \"--\"\n",
    "GRID_LW      = 0.6\n",
    "\n",
    "FIGSIZE = (13.6, 8.4)\n",
    "WSPACE  = 0.30\n",
    "HSPACE  = 0.42\n",
    "TOP     = 0.88\n",
    "BOTTOM  = 0.20\n",
    "LEFT    = 0.075\n",
    "RIGHT   = 0.985\n",
    "SUPTITLE_Y = 0.975\n",
    "LEGEND_Y   = 0.055\n",
    "\n",
    "BAND_EPS = 0.008\n",
    "def _ensure_visible_band(lo: np.ndarray, hi: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    lo = lo.astype(float).copy()\n",
    "    hi = hi.astype(float).copy()\n",
    "    flat = (np.abs(hi - lo) < 1e-12)\n",
    "    if np.any(flat):\n",
    "        lo2 = lo - BAND_EPS/2\n",
    "        hi2 = hi + BAND_EPS/2\n",
    "        lo[flat] = np.clip(lo2[flat], 0.0, 1.0)\n",
    "        hi[flat] = np.clip(hi2[flat], 0.0, 1.02)\n",
    "    return lo, hi\n",
    "\n",
    "# ===============================================================================================\n",
    "# 3) WEIGHT CANDIDATES (Table-3 + extras) + LOOKUP\n",
    "# ===============================================================================================\n",
    "def build_weight_table() -> pd.DataFrame:\n",
    "    # Paper Table 3 is (wM,wC,wS) -> store as (wC,wM,wS)\n",
    "    paper_cases = [\n",
    "        (\"T3_01\", \"Case 1\",  1,    0,    0),\n",
    "        (\"T3_02\", \"Case 2\",  0,    1,    0),\n",
    "        (\"T3_03\", \"Case 3\",  0,    0,    1),\n",
    "        (\"T3_04\", \"Case 4\",  1/3,  1/3,  1/3),\n",
    "        (\"T3_05\", \"Case 5\",  1/4,  1/4,  2/4),\n",
    "        (\"T3_06\", \"Case 6\",  1/5,  1/5,  3/5),\n",
    "        (\"T3_07\", \"Case 7\",  1/6,  1/6,  4/6),\n",
    "        (\"T3_08\", \"Case 8\",  1/8,  2/8,  5/8),\n",
    "        (\"T3_09\", \"Case 9\",  1/8,  1/8,  6/8),\n",
    "        (\"T3_10\", \"Case 10\", 1/10, 1/10, 8/10),\n",
    "        (\"T3_11\", \"Case 11\", 2/3,  1/3,  1/3),\n",
    "        (\"T3_12\", \"Case 12\", 3/4,  1/4,  1/4),\n",
    "        (\"T3_13\", \"Case 13\", 5/8,  2/8,  1/8),\n",
    "        (\"T3_14\", \"Case 14\", 6/8,  1/8,  1/8),\n",
    "        (\"T3_15\", \"Case 15\", 1/20, 1/20, 18/20),\n",
    "        (\"T3_16\", \"Case 16\", 1/40, 1/40, 38/40),\n",
    "    ]\n",
    "    rows = []\n",
    "    for cid, name, wM, wC, wS in paper_cases:\n",
    "        rows.append({\n",
    "            \"weight_case_id\": cid,\n",
    "            \"weight_case_name\": name,\n",
    "            \"wC\": float(wC),\n",
    "            \"wM\": float(wM),\n",
    "            \"wS\": float(wS),\n",
    "            \"source\": \"Table3\",\n",
    "        })\n",
    "\n",
    "    extras = [\n",
    "        (\"EX_01\", \"SensorOnly\",   0.0, 0.0, 1.0),\n",
    "        (\"EX_02\", \"Sensor90\",     0.05, 0.05, 0.90),\n",
    "        (\"EX_03\", \"Sensor95\",     0.025, 0.025, 0.95),\n",
    "        (\"EX_04\", \"Sensor98\",     0.01, 0.01, 0.98),\n",
    "    ]\n",
    "    for cid, name, wC, wM, wS in extras:\n",
    "        rows.append({\n",
    "            \"weight_case_id\": cid,\n",
    "            \"weight_case_name\": name,\n",
    "            \"wC\": float(wC),\n",
    "            \"wM\": float(wM),\n",
    "            \"wS\": float(wS),\n",
    "            \"source\": \"Extra\",\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"_key\"] = df.apply(lambda r: (round(r[\"wC\"], 6), round(r[\"wM\"], 6), round(r[\"wS\"], 6)), axis=1)\n",
    "    df = df.drop_duplicates(\"_key\").drop(columns=[\"_key\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "WEIGHT_TABLE = build_weight_table()\n",
    "\n",
    "def _weight_lookup(w: Tuple[float,float,float]) -> Tuple[str,str,str]:\n",
    "    wC,wM,wS = (round(float(w[0]),6), round(float(w[1]),6), round(float(w[2]),6))\n",
    "    m = WEIGHT_TABLE[(WEIGHT_TABLE[\"wC\"].round(6)==wC) &\n",
    "                     (WEIGHT_TABLE[\"wM\"].round(6)==wM) &\n",
    "                     (WEIGHT_TABLE[\"wS\"].round(6)==wS)]\n",
    "    if len(m)>0:\n",
    "        r = m.iloc[0]\n",
    "        return str(r[\"weight_case_id\"]), str(r[\"weight_case_name\"]), str(r[\"source\"])\n",
    "    return (\"CUSTOM\",\"Custom\",\"Search\")\n",
    "\n",
    "# ===============================================================================================\n",
    "# 4) IO + workload parsing\n",
    "# ===============================================================================================\n",
    "RUNID2PATH: Dict[str,str] = {}\n",
    "\n",
    "def detect_setup_from_path(p: Path):\n",
    "    s = str(p).lower()\n",
    "    if \"ddr4\" in s: return \"DDR4\"\n",
    "    if \"ddr5\" in s: return \"DDR5\"\n",
    "    return None\n",
    "\n",
    "def is_benign_path(p: Path) -> bool:\n",
    "    s = str(p).lower()\n",
    "    if \"benign\" in s:\n",
    "        return True\n",
    "    bad = [\"attack\",\"anom\",\"fault\",\"inject\",\"trojan\",\"mal\",\"rh\",\"droop\",\"spectre\",\"trrespass\"]\n",
    "    return not any(b in s for b in bad)\n",
    "\n",
    "def is_anomaly_path(p: Path, anomaly: str) -> bool:\n",
    "    return (anomaly.lower() in str(p).lower()) and (not is_benign_path(p))\n",
    "\n",
    "def iter_raw_csvs(root: Path):\n",
    "    for p in root.rglob(\"*.csv\"):\n",
    "        yield p\n",
    "\n",
    "def read_csv_clean(p: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(p)\n",
    "    return df.loc[:, ~df.columns.str.startswith(\"Unnamed\")]\n",
    "\n",
    "def mk_run_id(path: Path) -> str:\n",
    "    rid = f\"run_{hashlib.md5(str(path).encode('utf-8')).hexdigest()[:10]}\"\n",
    "    RUNID2PATH[rid] = str(path)\n",
    "    return rid\n",
    "\n",
    "_WORKLOADS = [\"dft\",\"dj\",\"dp\",\"gl\",\"gs\",\"ha\",\"ja\",\"mm\",\"ni\",\"oe\",\"pi\",\"sh\",\"tr\"]\n",
    "def workload_from_path(p: Path) -> str:\n",
    "    stem = (p.stem or \"\").lower()\n",
    "    m = re.search(r\"_([a-z]{2,3})$\", stem)\n",
    "    if m and m.group(1) in _WORKLOADS:\n",
    "        return m.group(1).upper()\n",
    "    for tok in _WORKLOADS:\n",
    "        if tok in stem:\n",
    "            return tok.upper()\n",
    "    return \"UNK\"\n",
    "\n",
    "def collect_raw_pairs_by_setup(data_dir: Path, which: str, anomaly: Optional[str] = None):\n",
    "    out = {\"DDR4\": [], \"DDR5\": []}\n",
    "    for p in iter_raw_csvs(data_dir):\n",
    "        setup = detect_setup_from_path(p)\n",
    "        if setup is None:\n",
    "            continue\n",
    "        try:\n",
    "            if which == \"benign\":\n",
    "                if not is_benign_path(p):\n",
    "                    continue\n",
    "            else:\n",
    "                if anomaly is None or not is_anomaly_path(p, anomaly):\n",
    "                    continue\n",
    "            df = read_csv_clean(p)\n",
    "            out[setup].append((p, df))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] read failed {p}: {e}\")\n",
    "    return out\n",
    "\n",
    "def telemetry_cols(df: pd.DataFrame):\n",
    "    exclude = set(META) | set(DROOP_META_COLS) | {\"workload\"}\n",
    "    return [c for c in df.columns if (c not in exclude) and (df[c].dtype.kind in \"fcbiu\")]\n",
    "\n",
    "def drop_low_variance_cols(df: pd.DataFrame, cols: List[str], eps: float = 1e-10) -> List[str]:\n",
    "    v = df[cols].astype(float).var(axis=0, ddof=0)\n",
    "    keep = v[v > eps].index.tolist()\n",
    "    return keep\n",
    "\n",
    "# ===============================================================================================\n",
    "# 5) Scaling\n",
    "# ===============================================================================================\n",
    "def robust_scale_train(Xb_np: np.ndarray, winsor=(2.0, 98.0)):\n",
    "    Q1, Q2 = np.percentile(Xb_np, winsor[0], axis=0), np.percentile(Xb_np, winsor[1], axis=0)\n",
    "    Xb_clip = np.clip(Xb_np, Q1, Q2)\n",
    "    mu = Xb_clip.mean(axis=0)\n",
    "    sd = Xb_clip.std(axis=0, ddof=0) + 1e-9\n",
    "    return mu, sd, Q1, Q2\n",
    "\n",
    "def apply_robust_scale(X: pd.DataFrame, mu, sd, Q1, Q2):\n",
    "    Xc = np.clip(X.to_numpy(dtype=float), Q1, Q2)\n",
    "    Z  = (Xc - mu) / sd\n",
    "    return pd.DataFrame(Z, columns=X.columns, index=X.index)\n",
    "\n",
    "# ===============================================================================================\n",
    "# 6) Windowing (adds sensor transient feats incl CPU Voltage)\n",
    "# ===============================================================================================\n",
    "SENSOR_PAT = re.compile(r\"cpu\\s*voltage|volt|vdd|vcore|vin|vout|power|energy|joule|current|amps?|temp|thermal|hot\", re.I)\n",
    "\n",
    "def window_collapse_means(df: pd.DataFrame, win: int, setup: str, run_id: str, label: str,\n",
    "                          overlap_ratio: float, src_path: str=\"\") -> pd.DataFrame:\n",
    "    cols_all = telemetry_cols(df)\n",
    "    if not cols_all:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    sensor_cols = [c for c in cols_all if SENSOR_PAT.search(c)]\n",
    "    if \"CPU Voltage\" in cols_all and \"CPU Voltage\" not in sensor_cols:\n",
    "        sensor_cols.append(\"CPU Voltage\")\n",
    "\n",
    "    n = len(df)\n",
    "    stride = max(1, int(round(win * (1 - overlap_ratio))))\n",
    "    starts = list(range(0, n - win + 1, stride))\n",
    "    if not starts:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for start in starts:\n",
    "        chunk = df.iloc[start:start+win]\n",
    "        X = chunk[cols_all].astype(float)\n",
    "        means = X.mean(axis=0, numeric_only=True)\n",
    "\n",
    "        tfeat = {}\n",
    "        if sensor_cols:\n",
    "            Xs = X[sensor_cols]\n",
    "            mins  = Xs.min(axis=0)\n",
    "            maxs  = Xs.max(axis=0)\n",
    "            stds  = Xs.std(axis=0, ddof=0)\n",
    "            means_s = Xs.mean(axis=0)\n",
    "            drops  = (means_s - mins)\n",
    "            ranges = (maxs - mins)\n",
    "            slope  = (Xs.iloc[-1] - Xs.iloc[0])\n",
    "            for c in sensor_cols:\n",
    "                tfeat[f\"{c}__min\"]   = float(mins[c])\n",
    "                tfeat[f\"{c}__max\"]   = float(maxs[c])\n",
    "                tfeat[f\"{c}__std\"]   = float(stds[c])\n",
    "                tfeat[f\"{c}__drop\"]  = float(drops[c])\n",
    "                tfeat[f\"{c}__range\"] = float(ranges[c])\n",
    "                tfeat[f\"{c}__slope\"] = float(slope[c])\n",
    "\n",
    "        row = pd.concat([means, pd.Series(tfeat)]).to_frame().T\n",
    "        row[\"setup\"]    = setup\n",
    "        row[\"run_id\"]   = run_id\n",
    "        row[\"label\"]    = label\n",
    "        row[\"workload\"] = workload_from_path(Path(src_path))\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "def build_windowed_raw_means(pairs, setup: str, win: int, label: str, overlap_ratio: float) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for p, df in pairs:\n",
    "        rid = mk_run_id(p)\n",
    "        agg = window_collapse_means(df, win=win, setup=setup, run_id=rid, label=label,\n",
    "                                    overlap_ratio=overlap_ratio, src_path=str(p))\n",
    "        if not agg.empty:\n",
    "            out.append(agg)\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "# ===============================================================================================\n",
    "# 7) Rank lists + robust mapping + CPU Voltage family append for DROOP\n",
    "# ===============================================================================================\n",
    "def _read_rank_feats(p: Path):\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "    except Exception:\n",
    "        return []\n",
    "    if df.empty:\n",
    "        return []\n",
    "    col = \"feature\" if \"feature\" in df.columns else df.columns[0]\n",
    "    return df[col].dropna().astype(str).tolist()\n",
    "\n",
    "def load_full_rank_lists(setup: str, win: int, kfold: int):\n",
    "    def _find_file(setup, win, kfold, sub):\n",
    "        fname = f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "        for d in RANK_DIRS:\n",
    "            pp = d / fname\n",
    "            if pp.exists():\n",
    "                return pp\n",
    "        return None\n",
    "    out = {}\n",
    "    for sub in [\"compute\",\"memory\",\"sensors\"]:\n",
    "        pp = _find_file(setup, win, kfold, sub)\n",
    "        out[sub] = _read_rank_feats(pp) if pp else []\n",
    "    return out\n",
    "\n",
    "def slice_by_percent(full_lists: Dict[str, List[str]], pct: int):\n",
    "    sel = {}\n",
    "    frac = pct / 100.0\n",
    "    for sub in (\"compute\",\"memory\",\"sensors\"):\n",
    "        feats = full_lists.get(sub, []) or []\n",
    "        k = int(np.ceil(len(feats) * frac))\n",
    "        if frac > 0 and len(feats) > 0:\n",
    "            k = max(1, k)\n",
    "        sel[sub] = feats[:min(len(feats), max(0, k))]\n",
    "    return sel\n",
    "\n",
    "ALIAS_MAP = {\n",
    "    \"voltage\": [\"cpu\",\"voltage\",\"volt\",\"vcore\",\"vdd\",\"vin\",\"vout\"],\n",
    "    \"power\": [\"power\",\"energy\",\"joules\",\"current\",\"amps\"],\n",
    "    \"temperature\": [\"temp\",\"thermal\",\"hot\"],\n",
    "    \"frequency\": [\"freq\",\"afreq\",\"cfreq\",\"clock\",\"clk\",\"mhz\",\"ghz\"],\n",
    "    \"ipc\": [\"ipc\",\"physipc\"],\n",
    "    \"cache\": [\"l1\",\"l2\",\"l3\",\"hit\",\"miss\",\"evict\",\"fill\",\"mpi\"],\n",
    "    \"bandwidth\": [\"read\",\"write\",\"bw\",\"bandwidth\"],\n",
    "}\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s)).lower()\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        out.append(ch if ch.isalnum() else \"_\")\n",
    "    s = \"\".join(out)\n",
    "    while \"__\" in s:\n",
    "        s = s.replace(\"__\",\"_\")\n",
    "    return s.strip(\"_\")\n",
    "\n",
    "def _tokens(s: str):\n",
    "    t = _norm(s).replace(\"_\",\" \").split()\n",
    "    exp = []\n",
    "    for tok in t:\n",
    "        exp.append(tok)\n",
    "        for k, aliases in ALIAS_MAP.items():\n",
    "            if tok in aliases:\n",
    "                exp.append(k)\n",
    "    return set(exp)\n",
    "\n",
    "def build_column_index(cols: List[str]):\n",
    "    norm2orig = {}\n",
    "    tok_index = {}\n",
    "    for c in cols:\n",
    "        nc = _norm(c)\n",
    "        norm2orig.setdefault(nc, c)\n",
    "        tok_index[c] = _tokens(c)\n",
    "    return norm2orig, tok_index\n",
    "\n",
    "def map_ranks_to_existing(ranked_list: List[str], norm2orig, tok_index):\n",
    "    mapped, seen = [], set()\n",
    "    for r in ranked_list:\n",
    "        nr = _norm(r)\n",
    "        cand = norm2orig.get(nr, None)\n",
    "        if cand and cand not in seen:\n",
    "            mapped.append(cand); seen.add(cand); continue\n",
    "        rtoks = _tokens(r)\n",
    "        best_c, best_j = None, 0.0\n",
    "        for c, ctoks in tok_index.items():\n",
    "            inter = len(rtoks & ctoks)\n",
    "            union = len(rtoks | ctoks) or 1\n",
    "            j = inter / union\n",
    "            if j > best_j:\n",
    "                best_j, best_c = j, c\n",
    "        if best_c and best_j >= 0.60 and best_c not in seen:\n",
    "            mapped.append(best_c); seen.add(best_c)\n",
    "    return mapped\n",
    "\n",
    "def intersect_selection_with_columns_robust(sel_raw: Dict[str, List[str]], xb_cols: set):\n",
    "    norm2orig, tok_index = build_column_index(list(xb_cols))\n",
    "    out = {}\n",
    "    for sub in (\"compute\",\"memory\",\"sensors\"):\n",
    "        ranked = sel_raw.get(sub, []) or []\n",
    "        mapped = map_ranks_to_existing(ranked, norm2orig, tok_index)\n",
    "        out[sub] = [c for c in mapped if c in xb_cols]\n",
    "    return out\n",
    "\n",
    "def force_cpu_voltage_family(sel: Dict[str, List[str]], xb_cols: set) -> Dict[str, List[str]]:\n",
    "    out = dict(sel)\n",
    "    sensors_now = out.get(\"sensors\", []) or []\n",
    "    must = []\n",
    "    if \"CPU Voltage\" in xb_cols:\n",
    "        must.append(\"CPU Voltage\")\n",
    "    for suf in [\"__min\",\"__max\",\"__std\",\"__drop\",\"__range\",\"__slope\"]:\n",
    "        c = f\"CPU Voltage{suf}\"\n",
    "        if c in xb_cols:\n",
    "            must.append(c)\n",
    "    out[\"sensors\"] = list(dict.fromkeys(sensors_now + must))\n",
    "    return out\n",
    "\n",
    "# ===============================================================================================\n",
    "# 8) Balanced evaluation helper  ✅ ADDED (fixes NameError)\n",
    "# ===============================================================================================\n",
    "def balanced_concat(df_ben: pd.DataFrame, df_anom: pd.DataFrame, seed: int = 1337):\n",
    "    \"\"\"\n",
    "    Create a balanced evaluation set by sampling the same number of windows from BENIGN and ANOM.\n",
    "    Returns:\n",
    "      df_eval : concatenated dataframe (benign first, anomaly second)\n",
    "      y_true  : numpy array labels (0 benign, 1 anomaly) aligned with df_eval rows\n",
    "    \"\"\"\n",
    "    if df_ben is None or df_ben.empty:\n",
    "        raise ValueError(\"balanced_concat(): df_ben is empty\")\n",
    "    if df_anom is None or df_anom.empty:\n",
    "        raise ValueError(\"balanced_concat(): df_anom is empty\")\n",
    "\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    nb = int(len(df_ben))\n",
    "    na = int(len(df_anom))\n",
    "    n  = int(min(nb, na))\n",
    "\n",
    "    # sample without replacement if possible; otherwise sample with replacement\n",
    "    replace_b = nb < n\n",
    "    replace_a = na < n\n",
    "\n",
    "    idx_b = rng.choice(nb, size=n, replace=replace_b)\n",
    "    idx_a = rng.choice(na, size=n, replace=replace_a)\n",
    "\n",
    "    ben_s = df_ben.iloc[idx_b].copy().reset_index(drop=True)\n",
    "    anm_s = df_anom.iloc[idx_a].copy().reset_index(drop=True)\n",
    "\n",
    "    df_eval = pd.concat([ben_s, anm_s], ignore_index=True)\n",
    "    y_true  = np.concatenate([np.zeros(len(ben_s), dtype=int),\n",
    "                              np.ones(len(anm_s), dtype=int)])\n",
    "    return df_eval, y_true\n",
    "\n",
    "# ===============================================================================================\n",
    "# 9) Weight selection helpers\n",
    "# ===============================================================================================\n",
    "def split_holdout(y, test_size=0.3, seed=SEED):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    idx = np.arange(len(y))\n",
    "    _, val_idx = next(sss.split(idx, y))\n",
    "    return val_idx\n",
    "\n",
    "def pick_best_w_per_method_PRfirst(y_true: np.ndarray, scores_by_w: Dict[Tuple[float,float,float], Dict[str,np.ndarray]], val_idx: np.ndarray):\n",
    "    yy = y_true[val_idx]\n",
    "    best = {m: (-1.0, -1.0, -1.0, None) for m in [\"dE_aM\",\"dE_aJ\",\"dC_aM\",\"dC_aJ\"]}\n",
    "    for w, md in scores_by_w.items():\n",
    "        wS = float(w[2])\n",
    "        for m, s in md.items():\n",
    "            sv = np.asarray(s)[val_idx]\n",
    "            pr  = float(average_precision_score(yy, sv))\n",
    "            roc = float(roc_auc_score(yy, sv))\n",
    "            key = (pr, roc, wS)\n",
    "            if key > best[m][:3]:\n",
    "                best[m] = (pr, roc, wS, w)\n",
    "    return {m: best[m][3] for m in best}\n",
    "\n",
    "# ===============================================================================================\n",
    "# 10) Scoring (draft math)\n",
    "# ===============================================================================================\n",
    "def _build_refs(Xb_base: pd.DataFrame, sel: Dict[str, List[str]], eps: float = 1e-12):\n",
    "    refs = {}\n",
    "    for sub in (\"compute\",\"memory\",\"sensors\"):\n",
    "        cols = [c for c in (sel.get(sub) or []) if c in Xb_base.columns]\n",
    "        if not cols:\n",
    "            refs[sub] = None\n",
    "            continue\n",
    "        X = Xb_base[cols].astype(float).values\n",
    "        mu = X.mean(axis=0)\n",
    "        var = X.var(axis=0, ddof=0)\n",
    "        var = np.where(var <= eps, eps, var)\n",
    "        w = 1.0 / var\n",
    "        denomC = float(np.sum((w * mu) ** 2))\n",
    "        refs[sub] = {\"cols\": cols, \"mu\": mu, \"w\": w, \"denomC\": max(denomC, eps)}\n",
    "    return refs\n",
    "\n",
    "def _score_dE_parts(Xe_base: pd.DataFrame, refs: dict) -> Dict[str, np.ndarray]:\n",
    "    n = len(Xe_base)\n",
    "    out = {}\n",
    "    for sub in (\"compute\",\"memory\",\"sensors\"):\n",
    "        r = refs.get(sub)\n",
    "        if r is None:\n",
    "            out[sub] = np.zeros(n)\n",
    "            continue\n",
    "        X = Xe_base[r[\"cols\"]].astype(float).values\n",
    "        D = X - r[\"mu\"]\n",
    "        s = np.sum((D * D) * r[\"w\"], axis=1)\n",
    "        out[sub] = np.log1p(np.maximum(s, 0.0))\n",
    "    return out\n",
    "\n",
    "def _score_dC_parts(Xe_base: pd.DataFrame, refs: dict, eps: float = 1e-12) -> Dict[str, np.ndarray]:\n",
    "    n = len(Xe_base)\n",
    "    out = {}\n",
    "    for sub in (\"compute\",\"memory\",\"sensors\"):\n",
    "        r = refs.get(sub)\n",
    "        if r is None:\n",
    "            out[sub] = np.zeros(n)\n",
    "            continue\n",
    "        X = Xe_base[r[\"cols\"]].astype(float).values\n",
    "        w = r[\"w\"]\n",
    "        mu = r[\"mu\"]\n",
    "        term1 = np.sum(w * (X * X), axis=1)\n",
    "        dot   = np.sum(X * (w * mu), axis=1)\n",
    "        term2 = (dot * dot) / max(r[\"denomC\"], eps)\n",
    "        s = term1 - term2\n",
    "        out[sub] = np.log1p(np.maximum(s, 0.0))\n",
    "    return out\n",
    "\n",
    "def fit_parts_normalizer_on_benign(Xb_base: pd.DataFrame, refs: dict) -> dict:\n",
    "    eps = 1e-9\n",
    "    partsE_b = _score_dE_parts(Xb_base, refs)\n",
    "    partsC_b = _score_dC_parts(Xb_base, refs)\n",
    "    norm = {\"dE\": {}, \"dC\": {}}\n",
    "    for sub in (\"compute\",\"memory\",\"sensors\"):\n",
    "        vE = np.asarray(partsE_b[sub], float)\n",
    "        vC = np.asarray(partsC_b[sub], float)\n",
    "        norm[\"dE\"][sub] = (float(np.nanmean(vE)), float(np.nanstd(vE) + eps))\n",
    "        norm[\"dC\"][sub] = (float(np.nanmean(vC)), float(np.nanstd(vC) + eps))\n",
    "    return norm\n",
    "\n",
    "def apply_parts_normalizer(parts: Dict[str, np.ndarray], norm_sub: dict) -> Dict[str, np.ndarray]:\n",
    "    out = {}\n",
    "    for sub, v in parts.items():\n",
    "        mu, sd = norm_sub.get(sub, (0.0, 1.0))\n",
    "        v = np.asarray(v, float)\n",
    "        out[sub] = (v - mu) / (sd if sd > 0 else 1.0)\n",
    "    return out\n",
    "\n",
    "def _aggregate_aM(parts: Dict[str, np.ndarray], weights: Tuple[float,float,float]) -> np.ndarray:\n",
    "    wC, wM, wS = map(float, weights)\n",
    "    den = (wC + wM + wS) if (wC + wM + wS) > 0 else 1.0\n",
    "    return (wC*parts[\"compute\"] + wM*parts[\"memory\"] + wS*parts[\"sensors\"]) / den\n",
    "\n",
    "def _aggregate_aJ(parts: Dict[str, np.ndarray], weights: Tuple[float,float,float], eps: float = 1e-12) -> np.ndarray:\n",
    "    wC, wM, wS = map(float, weights)\n",
    "    def softplus(x):\n",
    "        x = np.asarray(x, float)\n",
    "        return np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0.0)\n",
    "    sC = np.maximum(softplus(parts[\"compute\"]), eps)\n",
    "    sM = np.maximum(softplus(parts[\"memory\"]),  eps)\n",
    "    sS = np.maximum(softplus(parts[\"sensors\"]), eps)\n",
    "    return np.exp(wC*np.log(sC) + wM*np.log(sM) + wS*np.log(sS))\n",
    "\n",
    "def score_four_methods_benign_norm(Xe_base: pd.DataFrame, refs: dict, weights: Tuple[float,float,float], benign_norm: dict):\n",
    "    parts_E = apply_parts_normalizer(_score_dE_parts(Xe_base, refs), benign_norm[\"dE\"])\n",
    "    parts_C = apply_parts_normalizer(_score_dC_parts(Xe_base, refs), benign_norm[\"dC\"])\n",
    "    return {\n",
    "        \"dE_aM\": _aggregate_aM(parts_E, weights),\n",
    "        \"dE_aJ\": _aggregate_aJ(parts_E, weights),\n",
    "        \"dC_aM\": _aggregate_aM(parts_C, weights),\n",
    "        \"dC_aJ\": _aggregate_aJ(parts_C, weights),\n",
    "    }\n",
    "\n",
    "def droop_boost_score(Xe_base: pd.DataFrame, sel: Dict[str, List[str]]) -> np.ndarray:\n",
    "    cols = [c for c in (sel.get(\"sensors\") or []) if (\"CPU Voltage\" in c and DROOP_TRANSIENT_PAT.search(c))]\n",
    "    if not cols:\n",
    "        return np.zeros(len(Xe_base))\n",
    "    Z = Xe_base[cols].astype(float).to_numpy()\n",
    "    return np.log1p(np.mean(np.abs(Z), axis=1))\n",
    "\n",
    "# ===============================================================================================\n",
    "# 11) DSE plots\n",
    "# ===============================================================================================\n",
    "def summarize_for_dse(per_run_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"pct\"]\n",
    "    def q1(x): return float(np.nanpercentile(x, 25))\n",
    "    def q3(x): return float(np.nanpercentile(x, 75))\n",
    "    g = (per_run_df.groupby(keys, as_index=False)\n",
    "            .agg(\n",
    "                roc_auc_median=(\"roc_auc\",\"median\"),\n",
    "                roc_auc_q1=(\"roc_auc\", q1),\n",
    "                roc_auc_q3=(\"roc_auc\", q3),\n",
    "                roc_auc_min=(\"roc_auc\",\"min\"),\n",
    "                roc_auc_max=(\"roc_auc\",\"max\"),\n",
    "                auc_pr_median=(\"auc_pr\",\"median\"),\n",
    "                auc_pr_q1=(\"auc_pr\", q1),\n",
    "                auc_pr_q3=(\"auc_pr\", q3),\n",
    "                auc_pr_min=(\"auc_pr\",\"min\"),\n",
    "                auc_pr_max=(\"auc_pr\",\"max\"),\n",
    "                n_runs=(\"run_id\",\"nunique\"),\n",
    "            ))\n",
    "    return g.sort_values(keys).reset_index(drop=True)\n",
    "\n",
    "def plot_dse_grid(summary_case: pd.DataFrame, metric: str, setup: str, anomaly: str,\n",
    "                  win: int, kfold: int, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    x_ticks = sorted(summary_case[\"pct\"].unique().tolist())\n",
    "    if not x_ticks:\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=FIGSIZE, sharex=False, sharey=False)\n",
    "    axes = axes.flatten()\n",
    "    fig.subplots_adjust(left=LEFT, right=RIGHT, top=TOP, bottom=BOTTOM, wspace=WSPACE, hspace=HSPACE)\n",
    "\n",
    "    xlab = \"Top-ranked features used (%)\"\n",
    "    ylab = \"ROC-AUC\" if metric == \"roc_auc\" else \"AUC-PR\"\n",
    "\n",
    "    def _clip01(a): return np.clip(np.asarray(a, float), 0.0, 1.0)\n",
    "\n",
    "    for i, method in enumerate(METHOD_ORDER):\n",
    "        ax = axes[i]\n",
    "        ax.set_title(METHOD_TITLE.get(method, method), pad=7)\n",
    "        ax.set_xlim(min(x_ticks), max(x_ticks))\n",
    "        ax.set_ylim(PLOT_YMIN, PLOT_YMAX)\n",
    "        ax.set_xticks(x_ticks)\n",
    "        ax.grid(True, alpha=GRID_ALPHA, linestyle=GRID_LS, linewidth=GRID_LW)\n",
    "\n",
    "        sub = summary_case[summary_case[\"method\"] == method].copy().sort_values(\"pct\")\n",
    "        if sub.empty:\n",
    "            ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "        else:\n",
    "            x = sub[\"pct\"].to_numpy(dtype=float)\n",
    "            med  = _clip01(sub[f\"{metric}_median\"].to_numpy(dtype=float))\n",
    "            q1   = _clip01(sub[f\"{metric}_q1\"].to_numpy(dtype=float))\n",
    "            q3   = _clip01(sub[f\"{metric}_q3\"].to_numpy(dtype=float))\n",
    "            vmin = _clip01(sub[f\"{metric}_min\"].to_numpy(dtype=float))\n",
    "            vmax = _clip01(sub[f\"{metric}_max\"].to_numpy(dtype=float))\n",
    "\n",
    "            lo_mm, hi_mm = np.minimum(vmin, vmax), np.maximum(vmin, vmax)\n",
    "            lo_iq, hi_iq = np.minimum(q1, q3),     np.maximum(q1, q3)\n",
    "            lo_mm, hi_mm = _ensure_visible_band(lo_mm, hi_mm)\n",
    "            lo_iq, hi_iq = _ensure_visible_band(lo_iq, hi_iq)\n",
    "\n",
    "            ax.fill_between(x, lo_mm, hi_mm, alpha=ALPHA_MINMAX, label=\"min-max\", linewidth=0.0)\n",
    "            ax.fill_between(x, lo_iq, hi_iq, alpha=ALPHA_IQR,   label=\"IQR\",     linewidth=0.0)\n",
    "            ax.plot(x, med, color=\"black\", marker=\"o\", linewidth=2.0, markersize=5.5, zorder=5, label=\"Median\")\n",
    "\n",
    "        ax.set_xlabel(xlab, labelpad=6)\n",
    "        ax.set_ylabel(ylab, labelpad=6)\n",
    "        ax.tick_params(axis=\"x\", labelbottom=True, pad=3)\n",
    "        ax.tick_params(axis=\"y\", labelleft=True, pad=3)\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"lower center\", bbox_to_anchor=(0.5, LEGEND_Y), ncol=3, frameon=False)\n",
    "    fig.suptitle(f\"{setup}: {anomaly} (WIN={win}, K={kfold})\", y=SUPTITLE_Y)\n",
    "\n",
    "    base = f\"DSE_{metric.upper()}_{setup}_{anomaly}_WIN{int(win)}_KF{int(kfold)}\"\n",
    "    fig.savefig(out_dir / f\"{base}.png\", dpi=600, bbox_inches=\"tight\", pad_inches=0.06)\n",
    "    fig.savefig(out_dir / f\"{base}.pdf\", bbox_inches=\"tight\", pad_inches=0.06)\n",
    "    plt.close(fig)\n",
    "\n",
    "def write_all_dse_plots(per_run_df: pd.DataFrame):\n",
    "    out_root = RES_DIR / \"DesignSpace\" / \"PaperStyle_ALLRUNS\"\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "    dse = summarize_for_dse(per_run_df)\n",
    "    for (setup, anomaly, win, kfold), sub in dse.groupby([\"setup\",\"anomaly\",\"win\",\"kfold\"]):\n",
    "        out_dir = out_root / str(setup) / str(anomaly) / f\"WIN{int(win)}_KF{int(kfold)}\"\n",
    "        plot_dse_grid(sub, \"roc_auc\", str(setup), str(anomaly), int(win), int(kfold), out_dir)\n",
    "        plot_dse_grid(sub, \"auc_pr\",  str(setup), str(anomaly), int(win), int(kfold), out_dir)\n",
    "\n",
    "# ===============================================================================================\n",
    "# 12) Subspace-weight tables like your Table 13/14 (per setup,win,kfold)\n",
    "# ===============================================================================================\n",
    "def build_workload_weight_tables(per_run_df: pd.DataFrame, setup: str, win: int, kfold: int) -> None:\n",
    "    base_dir = RES_DIR / \"Table_SubspaceWeights\" / setup / f\"WIN{win}_KF{kfold}\"\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rep_method = \"dE_aM\"  # for paper-style merged table\n",
    "\n",
    "    for anomaly in ANOMALIES_BY_SETUP[setup]:\n",
    "        df = per_run_df[\n",
    "            (per_run_df[\"setup\"]==setup) &\n",
    "            (per_run_df[\"anomaly\"]==anomaly) &\n",
    "            (per_run_df[\"win\"]==win) &\n",
    "            (per_run_df[\"kfold\"]==kfold)\n",
    "        ].copy()\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        df[\"_wkey\"] = df.apply(lambda r: (round(r[\"wC\"],6), round(r[\"wM\"],6), round(r[\"wS\"],6)), axis=1)\n",
    "\n",
    "        rows = []\n",
    "        for (wl, method), g in df.groupby([\"workload\",\"method\"]):\n",
    "            mode_key = g[\"_wkey\"].value_counts().idxmax()\n",
    "            wC, wM, wS = mode_key\n",
    "            wid, wname, wsrc = _weight_lookup((wC,wM,wS))\n",
    "            rows.append({\n",
    "                \"workload\": wl,\n",
    "                \"method\": method,\n",
    "                \"chosen_wC\": wC, \"chosen_wM\": wM, \"chosen_wS\": wS,\n",
    "                \"weight_case_id\": wid,\n",
    "                \"weight_case_name\": wname,\n",
    "                \"weight_case_source\": wsrc,\n",
    "                \"n_rows\": int(len(g)),\n",
    "                \"median_roc\": float(np.nanmedian(g[\"roc_auc\"])),\n",
    "                \"median_pr\": float(np.nanmedian(g[\"auc_pr\"])),\n",
    "            })\n",
    "\n",
    "        df_out = pd.DataFrame(rows).sort_values([\"workload\",\"method\"]).reset_index(drop=True)\n",
    "        (base_dir / f\"Table_SubspaceWeights_{setup}_WIN{win}_KF{kfold}_{anomaly}.csv\").write_text(\n",
    "            df_out.to_csv(index=False)\n",
    "        )\n",
    "\n",
    "    droop = \"DROOP\"\n",
    "    other = [a for a in ANOMALIES_BY_SETUP[setup] if a != droop]\n",
    "    other = other[0] if other else \"\"\n",
    "\n",
    "    def _pick_weight_for(wl: str, anom: str) -> Optional[Tuple[float,float,float]]:\n",
    "        df = per_run_df[\n",
    "            (per_run_df[\"setup\"]==setup) &\n",
    "            (per_run_df[\"anomaly\"]==anom) &\n",
    "            (per_run_df[\"win\"]==win) &\n",
    "            (per_run_df[\"kfold\"]==kfold) &\n",
    "            (per_run_df[\"workload\"]==wl) &\n",
    "            (per_run_df[\"method\"]==rep_method)\n",
    "        ].copy()\n",
    "        if df.empty:\n",
    "            return None\n",
    "        df[\"_wkey\"] = df.apply(lambda r: (round(r[\"wC\"],6), round(r[\"wM\"],6), round(r[\"wS\"],6)), axis=1)\n",
    "        return df[\"_wkey\"].value_counts().idxmax()\n",
    "\n",
    "    all_wl = sorted(per_run_df[\n",
    "        (per_run_df[\"setup\"]==setup) &\n",
    "        (per_run_df[\"win\"]==win) &\n",
    "        (per_run_df[\"kfold\"]==kfold)\n",
    "    ][\"workload\"].astype(str).unique().tolist())\n",
    "\n",
    "    def fmt(w):\n",
    "        if w is None:\n",
    "            return \"\"\n",
    "        return f\"({w[0]:g}, {w[1]:g}, {w[2]:g})\"\n",
    "\n",
    "    rows = []\n",
    "    for wl in all_wl:\n",
    "        wd = _pick_weight_for(wl, droop)\n",
    "        wo = _pick_weight_for(wl, other) if other else None\n",
    "        rows.append({\n",
    "            \"Workload\": wl,\n",
    "            droop: fmt(wd),\n",
    "            other if other else \"Other\": fmt(wo),\n",
    "        })\n",
    "\n",
    "    paper_out = pd.DataFrame(rows)\n",
    "    (base_dir / f\"Table_SubspaceWeights_{setup}_WIN{win}_KF{kfold}_PAPERSTYLE.csv\").write_text(\n",
    "        paper_out.to_csv(index=False)\n",
    "    )\n",
    "\n",
    "# ===============================================================================================\n",
    "# 13) Pipeline core\n",
    "# ===============================================================================================\n",
    "def run_full_pipeline() -> pd.DataFrame:\n",
    "    all_rows = []\n",
    "    group_counter = 0\n",
    "\n",
    "    benign_pairs = collect_raw_pairs_by_setup(DATA_DIR, which=\"benign\")\n",
    "    anomaly_pairs_cache: Dict[Tuple[str,str], List[Tuple[Path,pd.DataFrame]]] = {}\n",
    "\n",
    "    def get_anom_pairs(setup: str, anomaly: str):\n",
    "        key = (setup, anomaly.upper())\n",
    "        if key not in anomaly_pairs_cache:\n",
    "            anomaly_pairs_cache[key] = collect_raw_pairs_by_setup(DATA_DIR, which=\"anomaly\", anomaly=anomaly)[setup]\n",
    "        return anomaly_pairs_cache[key]\n",
    "\n",
    "    for setup in SETUPS:\n",
    "        anomalies = ANOMALIES_BY_SETUP.get(setup, [])\n",
    "        ben_pairs_all = benign_pairs.get(setup, [])\n",
    "        if not ben_pairs_all:\n",
    "            print(f\"[SKIP] {setup}: no benign RAW files\")\n",
    "            continue\n",
    "\n",
    "        for win in WINS:\n",
    "            df_ben_all = build_windowed_raw_means(ben_pairs_all, setup=setup, win=win, label=\"BENIGN\",\n",
    "                                                  overlap_ratio=OVERLAP_RATIO)\n",
    "            if df_ben_all.empty:\n",
    "                print(f\"[SKIP] {setup} WIN={win}: benign windowed empty\")\n",
    "                continue\n",
    "\n",
    "            Xb_cols = telemetry_cols(df_ben_all)\n",
    "            if DROP_LOW_VARIANCE_COLS:\n",
    "                Xb_cols = drop_low_variance_cols(df_ben_all, Xb_cols, eps=LOW_VAR_EPS)\n",
    "            if not Xb_cols:\n",
    "                print(f\"[SKIP] {setup} WIN={win}: no telemetry cols after low-var drop\")\n",
    "                continue\n",
    "\n",
    "            Xb = df_ben_all[Xb_cols].astype(float)\n",
    "            mu_s, sd_s, Q1, Q2 = robust_scale_train(Xb.values, winsor=ROBUST_WINSOR)\n",
    "            Xb_base = apply_robust_scale(Xb, mu_s, sd_s, Q1, Q2)\n",
    "            xb_cols_set = set(Xb_base.columns)\n",
    "\n",
    "            ben_by_wl = {}\n",
    "            for wl in df_ben_all[\"workload\"].astype(str).unique():\n",
    "                ben_by_wl[wl] = df_ben_all[df_ben_all[\"workload\"].astype(str)==wl].copy()\n",
    "\n",
    "            for kfold in KFOLDS:\n",
    "                # write candidates table once per setup/win/kfold\n",
    "                wdir = RES_DIR / \"Table_SubspaceWeights\" / setup / f\"WIN{win}_KF{kfold}\"\n",
    "                wdir.mkdir(parents=True, exist_ok=True)\n",
    "                WEIGHT_TABLE.to_csv(wdir / \"Table_SubspaceWeights_CANDIDATES.csv\", index=False)\n",
    "\n",
    "                full_lists = load_full_rank_lists(setup, win, kfold)\n",
    "\n",
    "                for anomaly in anomalies:\n",
    "                    group_counter += 1\n",
    "                    print(f\"\\n[RUN] {setup} {anomaly} WIN={win} KF={kfold} (group {group_counter})\")\n",
    "\n",
    "                    overlap = OVERLAP_RATIO_DROOP if anomaly.upper() == \"DROOP\" else OVERLAP_RATIO\n",
    "                    an_pairs_all = get_anom_pairs(setup, anomaly)\n",
    "                    if not an_pairs_all:\n",
    "                        print(f\"[SKIP] {setup} {anomaly}: no anomaly RAW files\")\n",
    "                        continue\n",
    "\n",
    "                    df_anom = build_windowed_raw_means(an_pairs_all, setup=setup, win=win, label=anomaly,\n",
    "                                                       overlap_ratio=overlap)\n",
    "                    if df_anom.empty:\n",
    "                        print(f\"[SKIP] {setup} {anomaly} WIN={win}: anomaly windowed empty\")\n",
    "                        continue\n",
    "\n",
    "                    run_ids = sorted(df_anom[\"run_id\"].astype(str).unique())\n",
    "\n",
    "                    for rid in run_ids:\n",
    "                        dfa_run = df_anom[df_anom[\"run_id\"].astype(str) == rid].copy()\n",
    "                        if dfa_run.empty:\n",
    "                            continue\n",
    "\n",
    "                        wl = str(dfa_run[\"workload\"].iloc[0]) if \"workload\" in dfa_run.columns else \"UNK\"\n",
    "                        df_ben = ben_by_wl.get(wl, df_ben_all)\n",
    "                        if df_ben.empty:\n",
    "                            df_ben = df_ben_all\n",
    "\n",
    "                        # balanced evaluation ✅ (now defined)\n",
    "                        df_eval, y_true = balanced_concat(df_ben, dfa_run, seed=SEED)\n",
    "\n",
    "                        # Use only columns that exist in eval (prevents KeyError if any mismatch)\n",
    "                        use_cols = [c for c in Xb_cols if c in df_eval.columns]\n",
    "                        if not use_cols:\n",
    "                            continue\n",
    "\n",
    "                        Xe_all  = df_eval[use_cols].astype(float)\n",
    "                        Xe_base = apply_robust_scale(Xe_all, mu_s, sd_s, Q1, Q2)\n",
    "\n",
    "                        for idx_p, pct in enumerate(PCT_SWEEP, start=1):\n",
    "                            sel_raw = slice_by_percent(full_lists, pct)\n",
    "                            sel = intersect_selection_with_columns_robust(sel_raw, xb_cols_set)\n",
    "\n",
    "                            if (not sel.get(\"compute\")) or (not sel.get(\"memory\")) or (not sel.get(\"sensors\")):\n",
    "                                continue\n",
    "\n",
    "                            if anomaly.upper() == \"DROOP\":\n",
    "                                sel = force_cpu_voltage_family(sel, xb_cols_set)\n",
    "\n",
    "                            refs = _build_refs(Xb_base, sel, eps=1e-12)\n",
    "                            benign_norm = fit_parts_normalizer_on_benign(Xb_base, refs)\n",
    "\n",
    "                            val_idx = split_holdout(y_true, test_size=0.3, seed=SEED)\n",
    "\n",
    "                            scores_by_w = {}\n",
    "                            for _, wr in WEIGHT_TABLE.iterrows():\n",
    "                                w = (float(wr[\"wC\"]), float(wr[\"wM\"]), float(wr[\"wS\"]))\n",
    "                                scores_by_w[w] = score_four_methods_benign_norm(Xe_base, refs, w, benign_norm)\n",
    "\n",
    "                            best_w_for = pick_best_w_per_method_PRfirst(y_true, scores_by_w, val_idx)\n",
    "\n",
    "                            for method in METHOD_ORDER:\n",
    "                                w_star = best_w_for[method]\n",
    "                                y_score = score_four_methods_benign_norm(Xe_base, refs, w_star, benign_norm)[method]\n",
    "                                y_score = np.nan_to_num(y_score, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                                if DROOP_BOOST and anomaly.upper() == \"DROOP\":\n",
    "                                    boost = droop_boost_score(Xe_base, sel)\n",
    "                                    y_score = (1.0 - DROOP_BOOST_ALPHA) * y_score + DROOP_BOOST_ALPHA * boost\n",
    "\n",
    "                                roc = float(np.clip(roc_auc_score(y_true, y_score), 0.0, 1.0))\n",
    "                                pr  = float(np.clip(average_precision_score(y_true, y_score), 0.0, 1.0))\n",
    "\n",
    "                                wid, wname, wsrc = _weight_lookup(w_star)\n",
    "\n",
    "                                all_rows.append({\n",
    "                                    \"setup\": setup,\n",
    "                                    \"anomaly\": anomaly,\n",
    "                                    \"win\": int(win),\n",
    "                                    \"kfold\": int(kfold),\n",
    "                                    \"pct\": int(pct),\n",
    "                                    \"run_id\": str(rid),\n",
    "                                    \"workload\": str(wl),\n",
    "                                    \"method\": method,\n",
    "                                    \"roc_auc\": roc,\n",
    "                                    \"auc_pr\": pr,\n",
    "                                    \"weight_case_id\": wid,\n",
    "                                    \"weight_case_name\": wname,\n",
    "                                    \"weight_case_source\": wsrc,\n",
    "                                    \"wC\": float(w_star[0]),\n",
    "                                    \"wM\": float(w_star[1]),\n",
    "                                    \"wS\": float(w_star[2]),\n",
    "                                })\n",
    "\n",
    "                            if idx_p % GC_EVERY_N_PCTS == 0:\n",
    "                                gc.collect()\n",
    "\n",
    "                    if group_counter % GC_EVERY_N_GROUPS == 0:\n",
    "                        gc.collect()\n",
    "\n",
    "    per_run_df = pd.DataFrame(all_rows)\n",
    "\n",
    "    out_all = RES_DIR / \"per_run_metrics_all_PIPELINE.csv\"\n",
    "    per_run_df.to_csv(out_all, index=False)\n",
    "    print(f\"\\n[OK] ALLRUNS metrics → {out_all}\")\n",
    "\n",
    "    # Write workload weight tables per setup/win/kfold\n",
    "    for setup in SETUPS:\n",
    "        for win in WINS:\n",
    "            for kfold in KFOLDS:\n",
    "                build_workload_weight_tables(per_run_df, setup=setup, win=win, kfold=kfold)\n",
    "\n",
    "    # Write all DSE plots\n",
    "    write_all_dse_plots(per_run_df)\n",
    "\n",
    "    return per_run_df\n",
    "\n",
    "# ===============================================================================================\n",
    "# 14) MAIN\n",
    "# ===============================================================================================\n",
    "def main():\n",
    "    run_full_pipeline()\n",
    "    print(\"[DONE] Pipeline + DSE plots + weight tables per platform\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e708ff-4a19-41c7-a7be-23fc18e9f44f",
   "metadata": {},
   "source": [
    "## Find BEST (win, kfold, pct) across full 10–100% sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35c6dc27-4008-4225-a8b3-c0c060d0a9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wrote summary derived from pipeline → /Users/hsiaopingni/octaneX_v7_4functions/Results/per_case_summary_from_PIPELINE.csv\n",
      "\n",
      "[SCAN-A] Best across all WIN×KF×PCT per (setup, anomaly, method)\n",
      "  [BEST • DDR4–DROOP–dC_aJ] WIN=512  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [BEST • DDR4–DROOP–dC_aM] WIN=512  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [BEST • DDR4–DROOP–dE_aJ] WIN=512  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [BEST • DDR4–DROOP–dE_aM] WIN=512  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [BEST • DDR4–RH–dC_aJ] WIN=32  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [BEST • DDR4–RH–dC_aM] WIN=32  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [BEST • DDR4–RH–dE_aJ] WIN=32  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [BEST • DDR4–RH–dE_aM] WIN=32  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [BEST • DDR5–DROOP–dC_aJ] WIN=1024  K=5  %=20  AUCPR_med=0.997  ROC_med=0.997\n",
      "  [BEST • DDR5–DROOP–dC_aM] WIN=1024  K=5  %=20  AUCPR_med=0.990  ROC_med=0.988\n",
      "  [BEST • DDR5–DROOP–dE_aJ] WIN=1024  K=5  %=20  AUCPR_med=0.997  ROC_med=0.997\n",
      "  [BEST • DDR5–DROOP–dE_aM] WIN=1024  K=5  %=20  AUCPR_med=0.990  ROC_med=0.988\n",
      "  [BEST • DDR5–SPECTRE–dC_aJ] WIN=64  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [BEST • DDR5–SPECTRE–dC_aM] WIN=64  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [BEST • DDR5–SPECTRE–dE_aJ] WIN=64  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [BEST • DDR5–SPECTRE–dE_aM] WIN=64  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "\n",
      "[OK] Saved per-(setup,anomaly,method) winners → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_in_DesignSpace_Post_per_testcase_PER_METHOD.csv\n",
      "\n",
      "[SCAN-B] Best PCT per (setup, anomaly, win, kfold, method)\n",
      "  DDR4/DROOP  WIN=32  K=3  dC_aJ -> best %=80  AUCPR_med=0.879, ROC_med=0.942\n",
      "  DDR4/DROOP  WIN=32  K=3  dC_aM -> best %=80  AUCPR_med=0.882, ROC_med=0.952\n",
      "  DDR4/DROOP  WIN=32  K=3  dE_aJ -> best %=80  AUCPR_med=0.879, ROC_med=0.942\n",
      "  DDR4/DROOP  WIN=32  K=3  dE_aM -> best %=80  AUCPR_med=0.882, ROC_med=0.952\n",
      "  DDR4/DROOP  WIN=32  K=5  dC_aJ -> best %=80  AUCPR_med=0.879, ROC_med=0.942\n",
      "  DDR4/DROOP  WIN=32  K=5  dC_aM -> best %=80  AUCPR_med=0.882, ROC_med=0.952\n",
      "  DDR4/DROOP  WIN=32  K=5  dE_aJ -> best %=80  AUCPR_med=0.879, ROC_med=0.942\n",
      "  DDR4/DROOP  WIN=32  K=5  dE_aM -> best %=80  AUCPR_med=0.882, ROC_med=0.952\n",
      "  DDR4/DROOP  WIN=32  K=10  dC_aJ -> best %=60  AUCPR_med=0.880, ROC_med=0.946\n",
      "  DDR4/DROOP  WIN=32  K=10  dC_aM -> best %=80  AUCPR_med=0.882, ROC_med=0.952\n",
      "  DDR4/DROOP  WIN=32  K=10  dE_aJ -> best %=60  AUCPR_med=0.880, ROC_med=0.946\n",
      "  DDR4/DROOP  WIN=32  K=10  dE_aM -> best %=80  AUCPR_med=0.882, ROC_med=0.952\n",
      "  DDR4/DROOP  WIN=64  K=3  dC_aJ -> best %=10  AUCPR_med=0.945, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=64  K=3  dC_aM -> best %=10  AUCPR_med=0.946, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=64  K=3  dE_aJ -> best %=10  AUCPR_med=0.945, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=64  K=3  dE_aM -> best %=10  AUCPR_med=0.946, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=64  K=5  dC_aJ -> best %=10  AUCPR_med=0.945, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=64  K=5  dC_aM -> best %=10  AUCPR_med=0.946, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=64  K=5  dE_aJ -> best %=10  AUCPR_med=0.945, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=64  K=5  dE_aM -> best %=10  AUCPR_med=0.946, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=64  K=10  dC_aJ -> best %=10  AUCPR_med=0.945, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=64  K=10  dC_aM -> best %=10  AUCPR_med=0.946, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=64  K=10  dE_aJ -> best %=10  AUCPR_med=0.945, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=64  K=10  dE_aM -> best %=10  AUCPR_med=0.946, ROC_med=0.974\n",
      "  DDR4/DROOP  WIN=128  K=3  dC_aJ -> best %=40  AUCPR_med=0.969, ROC_med=0.980\n",
      "  DDR4/DROOP  WIN=128  K=3  dC_aM -> best %=40  AUCPR_med=0.971, ROC_med=0.980\n",
      "  DDR4/DROOP  WIN=128  K=3  dE_aJ -> best %=40  AUCPR_med=0.969, ROC_med=0.980\n",
      "  DDR4/DROOP  WIN=128  K=3  dE_aM -> best %=40  AUCPR_med=0.971, ROC_med=0.980\n",
      "  DDR4/DROOP  WIN=128  K=5  dC_aJ -> best %=40  AUCPR_med=0.965, ROC_med=0.979\n",
      "  DDR4/DROOP  WIN=128  K=5  dC_aM -> best %=40  AUCPR_med=0.964, ROC_med=0.979\n",
      "  DDR4/DROOP  WIN=128  K=5  dE_aJ -> best %=40  AUCPR_med=0.965, ROC_med=0.979\n",
      "  DDR4/DROOP  WIN=128  K=5  dE_aM -> best %=40  AUCPR_med=0.964, ROC_med=0.979\n",
      "  DDR4/DROOP  WIN=128  K=10  dC_aJ -> best %=10  AUCPR_med=0.965, ROC_med=0.982\n",
      "  DDR4/DROOP  WIN=128  K=10  dC_aM -> best %=40  AUCPR_med=0.963, ROC_med=0.978\n",
      "  DDR4/DROOP  WIN=128  K=10  dE_aJ -> best %=10  AUCPR_med=0.965, ROC_med=0.982\n",
      "  DDR4/DROOP  WIN=128  K=10  dE_aM -> best %=40  AUCPR_med=0.963, ROC_med=0.978\n",
      "  DDR4/DROOP  WIN=512  K=3  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=512  K=3  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=512  K=3  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=512  K=3  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=512  K=5  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=512  K=5  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=512  K=5  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=512  K=5  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=512  K=10  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=512  K=10  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=512  K=10  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=512  K=10  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=3  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=3  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=3  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=3  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=5  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=5  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=5  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=5  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=10  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=10  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=10  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/DROOP  WIN=1024  K=10  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=3  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=3  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=3  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=3  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=5  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=5  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=5  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=5  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=10  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=10  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=10  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=32  K=10  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=3  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=3  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=3  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=3  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=5  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=5  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=5  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=5  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=10  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=10  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=10  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=64  K=10  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=3  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=3  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=3  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=3  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=5  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=5  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=5  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=5  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=10  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=10  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=10  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=128  K=10  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=3  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=3  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=3  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=3  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=5  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=5  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=5  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=5  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=10  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=10  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=10  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=512  K=10  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=3  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=3  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=3  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=3  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=5  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=5  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=5  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=5  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=10  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=10  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=10  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR4/RH  WIN=1024  K=10  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/DROOP  WIN=32  K=3  dC_aJ -> best %=10  AUCPR_med=0.822, ROC_med=0.913\n",
      "  DDR5/DROOP  WIN=32  K=3  dC_aM -> best %=10  AUCPR_med=0.834, ROC_med=0.911\n",
      "  DDR5/DROOP  WIN=32  K=3  dE_aJ -> best %=10  AUCPR_med=0.822, ROC_med=0.913\n",
      "  DDR5/DROOP  WIN=32  K=3  dE_aM -> best %=10  AUCPR_med=0.834, ROC_med=0.911\n",
      "  DDR5/DROOP  WIN=32  K=5  dC_aJ -> best %=10  AUCPR_med=0.833, ROC_med=0.920\n",
      "  DDR5/DROOP  WIN=32  K=5  dC_aM -> best %=10  AUCPR_med=0.836, ROC_med=0.910\n",
      "  DDR5/DROOP  WIN=32  K=5  dE_aJ -> best %=10  AUCPR_med=0.833, ROC_med=0.920\n",
      "  DDR5/DROOP  WIN=32  K=5  dE_aM -> best %=10  AUCPR_med=0.836, ROC_med=0.910\n",
      "  DDR5/DROOP  WIN=32  K=10  dC_aJ -> best %=10  AUCPR_med=0.833, ROC_med=0.920\n",
      "  DDR5/DROOP  WIN=32  K=10  dC_aM -> best %=10  AUCPR_med=0.836, ROC_med=0.910\n",
      "  DDR5/DROOP  WIN=32  K=10  dE_aJ -> best %=10  AUCPR_med=0.833, ROC_med=0.920\n",
      "  DDR5/DROOP  WIN=32  K=10  dE_aM -> best %=10  AUCPR_med=0.836, ROC_med=0.910\n",
      "  DDR5/DROOP  WIN=64  K=3  dC_aJ -> best %=10  AUCPR_med=0.807, ROC_med=0.906\n",
      "  DDR5/DROOP  WIN=64  K=3  dC_aM -> best %=10  AUCPR_med=0.828, ROC_med=0.909\n",
      "  DDR5/DROOP  WIN=64  K=3  dE_aJ -> best %=10  AUCPR_med=0.807, ROC_med=0.906\n",
      "  DDR5/DROOP  WIN=64  K=3  dE_aM -> best %=10  AUCPR_med=0.828, ROC_med=0.909\n",
      "  DDR5/DROOP  WIN=64  K=5  dC_aJ -> best %=10  AUCPR_med=0.809, ROC_med=0.907\n",
      "  DDR5/DROOP  WIN=64  K=5  dC_aM -> best %=10  AUCPR_med=0.830, ROC_med=0.910\n",
      "  DDR5/DROOP  WIN=64  K=5  dE_aJ -> best %=10  AUCPR_med=0.809, ROC_med=0.907\n",
      "  DDR5/DROOP  WIN=64  K=5  dE_aM -> best %=10  AUCPR_med=0.830, ROC_med=0.910\n",
      "  DDR5/DROOP  WIN=64  K=10  dC_aJ -> best %=10  AUCPR_med=0.812, ROC_med=0.907\n",
      "  DDR5/DROOP  WIN=64  K=10  dC_aM -> best %=10  AUCPR_med=0.831, ROC_med=0.910\n",
      "  DDR5/DROOP  WIN=64  K=10  dE_aJ -> best %=10  AUCPR_med=0.812, ROC_med=0.907\n",
      "  DDR5/DROOP  WIN=64  K=10  dE_aM -> best %=10  AUCPR_med=0.831, ROC_med=0.910\n",
      "  DDR5/DROOP  WIN=128  K=3  dC_aJ -> best %=90  AUCPR_med=0.817, ROC_med=0.877\n",
      "  DDR5/DROOP  WIN=128  K=3  dC_aM -> best %=90  AUCPR_med=0.841, ROC_med=0.887\n",
      "  DDR5/DROOP  WIN=128  K=3  dE_aJ -> best %=90  AUCPR_med=0.817, ROC_med=0.877\n",
      "  DDR5/DROOP  WIN=128  K=3  dE_aM -> best %=90  AUCPR_med=0.841, ROC_med=0.887\n",
      "  DDR5/DROOP  WIN=128  K=5  dC_aJ -> best %=90  AUCPR_med=0.817, ROC_med=0.877\n",
      "  DDR5/DROOP  WIN=128  K=5  dC_aM -> best %=90  AUCPR_med=0.841, ROC_med=0.887\n",
      "  DDR5/DROOP  WIN=128  K=5  dE_aJ -> best %=90  AUCPR_med=0.817, ROC_med=0.877\n",
      "  DDR5/DROOP  WIN=128  K=5  dE_aM -> best %=90  AUCPR_med=0.841, ROC_med=0.887\n",
      "  DDR5/DROOP  WIN=128  K=10  dC_aJ -> best %=90  AUCPR_med=0.817, ROC_med=0.877\n",
      "  DDR5/DROOP  WIN=128  K=10  dC_aM -> best %=90  AUCPR_med=0.841, ROC_med=0.887\n",
      "  DDR5/DROOP  WIN=128  K=10  dE_aJ -> best %=90  AUCPR_med=0.817, ROC_med=0.877\n",
      "  DDR5/DROOP  WIN=128  K=10  dE_aM -> best %=90  AUCPR_med=0.841, ROC_med=0.887\n",
      "  DDR5/DROOP  WIN=512  K=3  dC_aJ -> best %=100  AUCPR_med=0.952, ROC_med=0.947\n",
      "  DDR5/DROOP  WIN=512  K=3  dC_aM -> best %=10  AUCPR_med=0.948, ROC_med=0.954\n",
      "  DDR5/DROOP  WIN=512  K=3  dE_aJ -> best %=100  AUCPR_med=0.952, ROC_med=0.947\n",
      "  DDR5/DROOP  WIN=512  K=3  dE_aM -> best %=10  AUCPR_med=0.948, ROC_med=0.954\n",
      "  DDR5/DROOP  WIN=512  K=5  dC_aJ -> best %=100  AUCPR_med=0.952, ROC_med=0.947\n",
      "  DDR5/DROOP  WIN=512  K=5  dC_aM -> best %=10  AUCPR_med=0.956, ROC_med=0.963\n",
      "  DDR5/DROOP  WIN=512  K=5  dE_aJ -> best %=100  AUCPR_med=0.952, ROC_med=0.947\n",
      "  DDR5/DROOP  WIN=512  K=5  dE_aM -> best %=10  AUCPR_med=0.956, ROC_med=0.963\n",
      "  DDR5/DROOP  WIN=512  K=10  dC_aJ -> best %=100  AUCPR_med=0.952, ROC_med=0.947\n",
      "  DDR5/DROOP  WIN=512  K=10  dC_aM -> best %=10  AUCPR_med=0.954, ROC_med=0.961\n",
      "  DDR5/DROOP  WIN=512  K=10  dE_aJ -> best %=100  AUCPR_med=0.952, ROC_med=0.947\n",
      "  DDR5/DROOP  WIN=512  K=10  dE_aM -> best %=10  AUCPR_med=0.954, ROC_med=0.961\n",
      "  DDR5/DROOP  WIN=1024  K=3  dC_aJ -> best %=10  AUCPR_med=0.992, ROC_med=0.991\n",
      "  DDR5/DROOP  WIN=1024  K=3  dC_aM -> best %=40  AUCPR_med=0.987, ROC_med=0.988\n",
      "  DDR5/DROOP  WIN=1024  K=3  dE_aJ -> best %=10  AUCPR_med=0.992, ROC_med=0.991\n",
      "  DDR5/DROOP  WIN=1024  K=3  dE_aM -> best %=40  AUCPR_med=0.987, ROC_med=0.988\n",
      "  DDR5/DROOP  WIN=1024  K=5  dC_aJ -> best %=20  AUCPR_med=0.997, ROC_med=0.997\n",
      "  DDR5/DROOP  WIN=1024  K=5  dC_aM -> best %=20  AUCPR_med=0.990, ROC_med=0.988\n",
      "  DDR5/DROOP  WIN=1024  K=5  dE_aJ -> best %=20  AUCPR_med=0.997, ROC_med=0.997\n",
      "  DDR5/DROOP  WIN=1024  K=5  dE_aM -> best %=20  AUCPR_med=0.990, ROC_med=0.988\n",
      "  DDR5/DROOP  WIN=1024  K=10  dC_aJ -> best %=10  AUCPR_med=0.992, ROC_med=0.991\n",
      "  DDR5/DROOP  WIN=1024  K=10  dC_aM -> best %=60  AUCPR_med=0.989, ROC_med=0.988\n",
      "  DDR5/DROOP  WIN=1024  K=10  dE_aJ -> best %=10  AUCPR_med=0.992, ROC_med=0.991\n",
      "  DDR5/DROOP  WIN=1024  K=10  dE_aM -> best %=60  AUCPR_med=0.989, ROC_med=0.988\n",
      "  DDR5/SPECTRE  WIN=32  K=3  dC_aJ -> best %=70  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=32  K=3  dC_aM -> best %=70  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=32  K=3  dE_aJ -> best %=70  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=32  K=3  dE_aM -> best %=70  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=32  K=5  dC_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=32  K=5  dC_aM -> best %=70  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=32  K=5  dE_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=32  K=5  dE_aM -> best %=70  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=32  K=10  dC_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=32  K=10  dC_aM -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=32  K=10  dE_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=32  K=10  dE_aM -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=3  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=3  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=3  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=3  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=5  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=5  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=5  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=5  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=10  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=10  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=10  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=64  K=10  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=3  dC_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=3  dC_aM -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=3  dE_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=3  dE_aM -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=5  dC_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=5  dC_aM -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=5  dE_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=5  dE_aM -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=10  dC_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=10  dC_aM -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=10  dE_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=128  K=10  dE_aM -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=3  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=3  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=3  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=3  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=5  dC_aJ -> best %=30  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=5  dC_aM -> best %=30  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=5  dE_aJ -> best %=30  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=5  dE_aM -> best %=30  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=10  dC_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=10  dC_aM -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=10  dE_aJ -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=512  K=10  dE_aM -> best %=20  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=3  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=3  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=3  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=3  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=5  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=5  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=5  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=5  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=10  dC_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=10  dC_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=10  dE_aJ -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "  DDR5/SPECTRE  WIN=1024  K=10  dE_aM -> best %=10  AUCPR_med=1.000, ROC_med=1.000\n",
      "\n",
      "[OK] Saved best pct per (win,kfold,method) → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_pct_per_win_kfold_PER_METHOD.csv\n",
      "\n",
      "[SCAN-C] Headline BEST per (setup, anomaly) across ALL methods too\n",
      "  [HEADLINE • DDR4–DROOP] dC_aJ  WIN=512  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [HEADLINE • DDR4–RH] dC_aJ  WIN=32  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [HEADLINE • DDR5–DROOP] dC_aJ  WIN=1024  K=5  %=20  AUCPR_med=0.997  ROC_med=0.997\n",
      "  [HEADLINE • DDR5–SPECTRE] dC_aJ  WIN=64  K=3  %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "\n",
      "[OK] Saved headline winners → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_in_DesignSpace_Post_HEADLINE_per_testcase.csv\n"
     ]
    }
   ],
   "source": [
    "# === Cell: Find BEST (win, kfold, pct) from NEW pipeline outputs ===========\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT    = pl.Path(\"/Users/hsiaopingni/octaneX_v7_4functions\")\n",
    "RES_DIR = ROOT / \"Results\"\n",
    "\n",
    "PER_RUN = RES_DIR / \"per_run_metrics_all_PIPELINE.csv\"\n",
    "if not PER_RUN.exists():\n",
    "    raise FileNotFoundError(f\"Missing per-run CSV from pipeline: {PER_RUN}\")\n",
    "\n",
    "df = pd.read_csv(PER_RUN).copy()\n",
    "\n",
    "# Defensive: keep only the grid you actually run\n",
    "valid_wins   = {32, 64, 128, 512, 1024}\n",
    "valid_kfolds = {3, 5, 10}\n",
    "df = df[df[\"win\"].isin(valid_wins) & df[\"kfold\"].isin(valid_kfolds)].copy()\n",
    "\n",
    "# Fill NaNs for robust sorting\n",
    "df[\"auc_pr_filled\"]  = df[\"auc_pr\"].fillna(-np.inf)\n",
    "df[\"roc_auc_filled\"] = df[\"roc_auc\"].fillna(-np.inf)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 0) Build a SUMMARY table from per-run (median/IQR/min/max across run_id)\n",
    "#    This replaces your old per_workload_summary...csv\n",
    "# ---------------------------------------------------------------------------\n",
    "def q1(x): return float(np.nanpercentile(x, 25))\n",
    "def q3(x): return float(np.nanpercentile(x, 75))\n",
    "\n",
    "# Summary per (setup, anomaly, win, kfold, method, pct)\n",
    "sum_keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"pct\"]\n",
    "sumdf = (df.groupby(sum_keys, as_index=False)\n",
    "           .agg(\n",
    "               auc_pr_median=(\"auc_pr\", \"median\"),\n",
    "               auc_pr_q1=(\"auc_pr\", q1),\n",
    "               auc_pr_q3=(\"auc_pr\", q3),\n",
    "               auc_pr_min=(\"auc_pr\", \"min\"),\n",
    "               auc_pr_max=(\"auc_pr\", \"max\"),\n",
    "               roc_auc_median=(\"roc_auc\", \"median\"),\n",
    "               roc_auc_q1=(\"roc_auc\", q1),\n",
    "               roc_auc_q3=(\"roc_auc\", q3),\n",
    "               roc_auc_min=(\"roc_auc\", \"min\"),\n",
    "               roc_auc_max=(\"roc_auc\", \"max\"),\n",
    "               n_runs=(\"run_id\",\"nunique\"),\n",
    "           ))\n",
    "\n",
    "sumdf[\"auc_pr_iqr\"]  = sumdf[\"auc_pr_q3\"]  - sumdf[\"auc_pr_q1\"]\n",
    "sumdf[\"roc_auc_iqr\"] = sumdf[\"roc_auc_q3\"] - sumdf[\"roc_auc_q1\"]\n",
    "\n",
    "# For sorting\n",
    "sumdf[\"auc_pr_median_filled\"]  = sumdf[\"auc_pr_median\"].fillna(-np.inf)\n",
    "sumdf[\"roc_auc_median_filled\"] = sumdf[\"roc_auc_median\"].fillna(-np.inf)\n",
    "\n",
    "# Save summary (optional but useful)\n",
    "SUMMARY_OUT = RES_DIR / \"per_case_summary_from_PIPELINE.csv\"\n",
    "sumdf.to_csv(SUMMARY_OUT, index=False)\n",
    "print(f\"[OK] Wrote summary derived from pipeline → {SUMMARY_OUT}\")\n",
    "\n",
    "ANOMALIES_BY_SETUP = {\"DDR4\": [\"DROOP\",\"RH\"], \"DDR5\": [\"DROOP\",\"SPECTRE\"]}\n",
    "METHOD_ORDER = [\"dC_aJ\", \"dC_aM\", \"dE_aJ\", \"dE_aM\"]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# A) Best (win,kfold,pct) per TEST CASE per METHOD\n",
    "#    (setup, anomaly, method)  across whole sweep\n",
    "#    Sort: AUCPR_median desc, then ROC_median desc, then smaller pct\n",
    "# ---------------------------------------------------------------------------\n",
    "winners_rows = []\n",
    "print(\"\\n[SCAN-A] Best across all WIN×KF×PCT per (setup, anomaly, method)\")\n",
    "for setup, anomalies in ANOMALIES_BY_SETUP.items():\n",
    "    for anomaly in anomalies:\n",
    "        for method in METHOD_ORDER:\n",
    "            sub = sumdf[\n",
    "                (sumdf[\"setup\"]==setup) &\n",
    "                (sumdf[\"anomaly\"]==anomaly) &\n",
    "                (sumdf[\"method\"]==method)\n",
    "            ].copy()\n",
    "            if sub.empty:\n",
    "                print(f\"  [WARN] No rows for {setup}–{anomaly}–{method}\")\n",
    "                continue\n",
    "\n",
    "            sub = sub.sort_values(\n",
    "                [\"auc_pr_median_filled\", \"roc_auc_median_filled\", \"pct\"],\n",
    "                ascending=[False, False, True]\n",
    "            )\n",
    "            best = sub.iloc[0]\n",
    "\n",
    "            winners_rows.append({\n",
    "                \"setup\": setup,\n",
    "                \"anomaly\": anomaly,\n",
    "                \"method\": method,\n",
    "                \"win\": int(best[\"win\"]),\n",
    "                \"kfold\": int(best[\"kfold\"]),\n",
    "                \"best_pct_by_median\": int(best[\"pct\"]),\n",
    "                \"auc_pr_median\": float(best[\"auc_pr_median\"]),\n",
    "                \"roc_auc_median\": float(best[\"roc_auc_median\"]),\n",
    "                \"auc_pr_iqr\": float(best[\"auc_pr_iqr\"]),\n",
    "                \"roc_auc_iqr\": float(best[\"roc_auc_iqr\"]),\n",
    "                \"min_auc_pr\": float(best[\"auc_pr_min\"]),\n",
    "                \"max_auc_pr\": float(best[\"auc_pr_max\"]),\n",
    "                \"min_roc_auc\": float(best[\"roc_auc_min\"]),\n",
    "                \"max_roc_auc\": float(best[\"roc_auc_max\"]),\n",
    "                \"n_runs\": int(best[\"n_runs\"]),\n",
    "            })\n",
    "\n",
    "            print(f\"  [BEST • {setup}–{anomaly}–{method}] \"\n",
    "                  f\"WIN={int(best['win'])}  K={int(best['kfold'])}  %={int(best['pct'])}  \"\n",
    "                  f\"AUCPR_med={best['auc_pr_median']:.3f}  ROC_med={best['roc_auc_median']:.3f}\")\n",
    "\n",
    "winners = pd.DataFrame(winners_rows).sort_values([\"setup\",\"anomaly\",\"method\"])\n",
    "WINNERS_CSV = RES_DIR / \"BEST_in_DesignSpace_Post_per_testcase_PER_METHOD.csv\"\n",
    "winners.to_csv(WINNERS_CSV, index=False)\n",
    "print(f\"\\n[OK] Saved per-(setup,anomaly,method) winners → {WINNERS_CSV}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# B) For every (setup, anomaly, win, kfold, method), find BEST pct\n",
    "# ---------------------------------------------------------------------------\n",
    "best_pct_rows = []\n",
    "gcols = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\"]\n",
    "print(\"\\n[SCAN-B] Best PCT per (setup, anomaly, win, kfold, method)\")\n",
    "for keys, g in sumdf.groupby(gcols, dropna=False):\n",
    "    g = g.sort_values(\n",
    "        [\"auc_pr_median_filled\", \"roc_auc_median_filled\", \"pct\"],\n",
    "        ascending=[False, False, True]\n",
    "    )\n",
    "    top = g.iloc[0]\n",
    "    best_pct_rows.append({\n",
    "        \"setup\": keys[0], \"anomaly\": keys[1],\n",
    "        \"win\": int(keys[2]), \"kfold\": int(keys[3]),\n",
    "        \"method\": keys[4],\n",
    "        \"best_pct_by_median\": int(top[\"pct\"]),\n",
    "        \"auc_pr_median\": float(top[\"auc_pr_median\"]),\n",
    "        \"roc_auc_median\": float(top[\"roc_auc_median\"]),\n",
    "        \"n_runs\": int(top[\"n_runs\"]),\n",
    "    })\n",
    "    print(f\"  {keys[0]}/{keys[1]}  WIN={int(keys[2])}  K={int(keys[3])}  {keys[4]} \"\n",
    "          f\"-> best %={int(top['pct'])}  AUCPR_med={top['auc_pr_median']:.3f}, ROC_med={top['roc_auc_median']:.3f}\")\n",
    "\n",
    "best_pct = pd.DataFrame(best_pct_rows).sort_values([\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\"])\n",
    "BEST_PCT_CSV = RES_DIR / \"BEST_pct_per_win_kfold_PER_METHOD.csv\"\n",
    "best_pct.to_csv(BEST_PCT_CSV, index=False)\n",
    "print(f\"\\n[OK] Saved best pct per (win,kfold,method) → {BEST_PCT_CSV}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# C) (Optional) One \"headline\" BEST per (setup, anomaly) by taking BEST METHOD too\n",
    "#    Sort: AUCPR_median desc, then ROC_median desc, then smaller pct\n",
    "# ---------------------------------------------------------------------------\n",
    "headline_rows = []\n",
    "print(\"\\n[SCAN-C] Headline BEST per (setup, anomaly) across ALL methods too\")\n",
    "for setup, anomalies in ANOMALIES_BY_SETUP.items():\n",
    "    for anomaly in anomalies:\n",
    "        sub = sumdf[(sumdf[\"setup\"]==setup) & (sumdf[\"anomaly\"]==anomaly)].copy()\n",
    "        if sub.empty:\n",
    "            print(f\"  [WARN] No rows for {setup}–{anomaly}\")\n",
    "            continue\n",
    "        sub = sub.sort_values(\n",
    "            [\"auc_pr_median_filled\", \"roc_auc_median_filled\", \"pct\"],\n",
    "            ascending=[False, False, True]\n",
    "        )\n",
    "        best = sub.iloc[0]\n",
    "        headline_rows.append({\n",
    "            \"setup\": setup,\n",
    "            \"anomaly\": anomaly,\n",
    "            \"method\": str(best[\"method\"]),\n",
    "            \"win\": int(best[\"win\"]),\n",
    "            \"kfold\": int(best[\"kfold\"]),\n",
    "            \"best_pct_by_median\": int(best[\"pct\"]),\n",
    "            \"auc_pr_median\": float(best[\"auc_pr_median\"]),\n",
    "            \"roc_auc_median\": float(best[\"roc_auc_median\"]),\n",
    "            \"auc_pr_iqr\": float(best[\"auc_pr_iqr\"]),\n",
    "            \"roc_auc_iqr\": float(best[\"roc_auc_iqr\"]),\n",
    "            \"n_runs\": int(best[\"n_runs\"]),\n",
    "        })\n",
    "        print(f\"  [HEADLINE • {setup}–{anomaly}] \"\n",
    "              f\"{best['method']}  WIN={int(best['win'])}  K={int(best['kfold'])}  %={int(best['pct'])}  \"\n",
    "              f\"AUCPR_med={best['auc_pr_median']:.3f}  ROC_med={best['roc_auc_median']:.3f}\")\n",
    "\n",
    "headline = pd.DataFrame(headline_rows).sort_values([\"setup\",\"anomaly\"])\n",
    "HEADLINE_CSV = RES_DIR / \"BEST_in_DesignSpace_Post_HEADLINE_per_testcase.csv\"\n",
    "headline.to_csv(HEADLINE_CSV, index=False)\n",
    "print(f\"\\n[OK] Saved headline winners → {HEADLINE_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c18ce26-4fe6-49d9-baaf-433302f5a84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wrote best_pct (per method) derived from pipeline → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_pct_per_win_kfold_PER_METHOD_from_PIPELINE.csv\n",
      "\n",
      "[SCAN] Best (WIN, K) per platform by mean AUCPR across anomalies\n",
      "  [PLATFORM BEST • DDR4]  WIN=512  K=3  mean AUCPR=1.000  mean ROC=1.000  (across 2 anomalies)\n",
      "      DDR4/DROOP: best method=dC_aJ  best %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "      DDR4/RH: best method=dC_aJ  best %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "  [PLATFORM BEST • DDR5]  WIN=1024  K=5  mean AUCPR=0.999  mean ROC=0.998  (across 2 anomalies)\n",
      "      DDR5/SPECTRE: best method=dC_aJ  best %=10  AUCPR_med=1.000  ROC_med=1.000\n",
      "      DDR5/DROOP: best method=dC_aJ  best %=20  AUCPR_med=0.997  ROC_med=0.997\n",
      "\n",
      "[OK] Saved platform winners → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_win_kfold_per_platform.csv\n",
      "[OK] Saved per-anomaly details for chosen (WIN,K) → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_pct_for_chosen_win_kfold_per_platform.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# C) Pick ONE (win, kfold) per platform (DDR4, DDR5)  ✅ UPDATED FOR NEW PIPELINE\n",
    "#    Uses your NEW pipeline outputs:\n",
    "#      - per_run_metrics_all_PIPELINE.csv  (per-run rows)\n",
    "#      - or BEST_pct_per_win_kfold_PER_METHOD.csv  (if you already created it)\n",
    "#\n",
    "#    Criterion (default):\n",
    "#      1) For each (setup, anomaly, win, kfold, method): pick BEST pct (PR-first, ROC, smaller pct)\n",
    "#      2) For each (setup, win, kfold): aggregate across anomalies by taking the BEST method per anomaly\n",
    "#         then average AUCPR across anomalies (tie avg ROC, smaller win, smaller k)\n",
    "#\n",
    "#    Outputs:\n",
    "#      - Results/BEST_win_kfold_per_platform.csv\n",
    "#      - Results/BEST_pct_for_chosen_win_kfold_per_platform.csv\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT    = Path(\"/Users/hsiaopingni/octaneX_v7_4functions\")\n",
    "RES_DIR = ROOT / \"Results\"\n",
    "\n",
    "PER_RUN = RES_DIR / \"per_run_metrics_all_PIPELINE.csv\"\n",
    "if not PER_RUN.exists():\n",
    "    raise FileNotFoundError(f\"Missing pipeline per-run CSV: {PER_RUN}\")\n",
    "\n",
    "df = pd.read_csv(PER_RUN).copy()\n",
    "\n",
    "ANOMALIES_BY_SETUP = {\"DDR4\": [\"DROOP\",\"RH\"], \"DDR5\": [\"DROOP\",\"SPECTRE\"]}\n",
    "METHOD_ORDER = [\"dC_aJ\", \"dC_aM\", \"dE_aJ\", \"dE_aM\"]\n",
    "\n",
    "# Optional defensive filters (match pipeline grid)\n",
    "valid_wins   = {32, 64, 128, 512, 1024}\n",
    "valid_kfolds = {3, 5, 10}\n",
    "df = df[df[\"win\"].isin(valid_wins) & df[\"kfold\"].isin(valid_kfolds)].copy()\n",
    "df = df[df[\"setup\"].isin([\"DDR4\",\"DDR5\"])].copy()\n",
    "df = df[df[\"method\"].isin(METHOD_ORDER)].copy()\n",
    "\n",
    "# Fill for robust ranking\n",
    "df[\"auc_pr_filled\"]  = pd.to_numeric(df[\"auc_pr\"], errors=\"coerce\").fillna(-np.inf)\n",
    "df[\"roc_auc_filled\"] = pd.to_numeric(df[\"roc_auc\"], errors=\"coerce\").fillna(-np.inf)\n",
    "\n",
    "# ---------- 0) Summarize per-case at pct level (median across run_id) ----------\n",
    "def q1(x): return float(np.nanpercentile(x, 25))\n",
    "def q3(x): return float(np.nanpercentile(x, 75))\n",
    "\n",
    "sum_keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"pct\"]\n",
    "sumdf = (df.groupby(sum_keys, as_index=False)\n",
    "           .agg(\n",
    "               auc_pr_median=(\"auc_pr\", \"median\"),\n",
    "               roc_auc_median=(\"roc_auc\", \"median\"),\n",
    "               auc_pr_q1=(\"auc_pr\", q1),\n",
    "               auc_pr_q3=(\"auc_pr\", q3),\n",
    "               roc_auc_q1=(\"roc_auc\", q1),\n",
    "               roc_auc_q3=(\"roc_auc\", q3),\n",
    "               n_runs=(\"run_id\",\"nunique\"),\n",
    "           ))\n",
    "\n",
    "sumdf[\"auc_pr_median\"]  = pd.to_numeric(sumdf[\"auc_pr_median\"], errors=\"coerce\").clip(0.0, 1.0)\n",
    "sumdf[\"roc_auc_median\"] = pd.to_numeric(sumdf[\"roc_auc_median\"], errors=\"coerce\").clip(0.0, 1.0)\n",
    "\n",
    "sumdf[\"auc_pr_median_filled\"]  = sumdf[\"auc_pr_median\"].fillna(-np.inf)\n",
    "sumdf[\"roc_auc_median_filled\"] = sumdf[\"roc_auc_median\"].fillna(-np.inf)\n",
    "\n",
    "# ---------- 1) For each (setup, anomaly, win, kfold, method): choose best pct ----------\n",
    "# Sort: AUCPR median desc, then ROC median desc, then smaller pct\n",
    "best_pct = (sumdf.sort_values(\n",
    "                [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\n",
    "                 \"auc_pr_median_filled\",\"roc_auc_median_filled\",\"pct\"],\n",
    "                ascending=[True,True,True,True,True, False,False,True]\n",
    "            )\n",
    "            .groupby([\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\"], as_index=False)\n",
    "            .head(1)\n",
    "            .rename(columns={\"pct\":\"best_pct_by_median\"}))\n",
    "\n",
    "# Keep only anomalies defined per platform (defensive)\n",
    "def _keep_defined_anoms(row):\n",
    "    setup = str(row[\"setup\"]).upper()\n",
    "    anom  = str(row[\"anomaly\"]).upper()\n",
    "    return setup in ANOMALIES_BY_SETUP and anom in [a.upper() for a in ANOMALIES_BY_SETUP[setup]]\n",
    "\n",
    "best_pct = best_pct[best_pct.apply(_keep_defined_anoms, axis=1)].copy()\n",
    "\n",
    "# Save (optional, helpful for debugging / later steps)\n",
    "BEST_PCT_PER_METHOD_CSV = RES_DIR / \"BEST_pct_per_win_kfold_PER_METHOD_from_PIPELINE.csv\"\n",
    "best_pct.to_csv(BEST_PCT_PER_METHOD_CSV, index=False)\n",
    "print(f\"[OK] Wrote best_pct (per method) derived from pipeline → {BEST_PCT_PER_METHOD_CSV}\")\n",
    "\n",
    "print(\"\\n[SCAN] Best (WIN, K) per platform by mean AUCPR across anomalies\")\n",
    "platform_rows = []\n",
    "chosen_rows = []\n",
    "\n",
    "# ---------- 2) Platform selection logic ----------\n",
    "for setup in [\"DDR4\", \"DDR5\"]:\n",
    "    sub = best_pct[best_pct[\"setup\"].astype(str).str.upper() == setup].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"  [WARN] No rows for setup={setup} in derived best_pct table\")\n",
    "        continue\n",
    "\n",
    "    # For each (setup, anomaly, win, kfold), pick BEST METHOD\n",
    "    # Sort: AUCPR desc, ROC desc, then a stable method preference order\n",
    "    method_rank = {m:i for i,m in enumerate(METHOD_ORDER)}\n",
    "    sub[\"_mrank\"] = sub[\"method\"].map(method_rank).fillna(999).astype(int)\n",
    "\n",
    "    best_method_per_anom = (sub.sort_values(\n",
    "                                [\"setup\",\"anomaly\",\"win\",\"kfold\",\n",
    "                                 \"auc_pr_median_filled\",\"roc_auc_median_filled\",\"_mrank\",\n",
    "                                 \"best_pct_by_median\"],\n",
    "                                ascending=[True,True,True,True, False,False, True, True]\n",
    "                            )\n",
    "                            .groupby([\"setup\",\"anomaly\",\"win\",\"kfold\"], as_index=False)\n",
    "                            .head(1))\n",
    "\n",
    "    # Aggregate per (win, kfold) across anomalies (mean of anomaly-bests)\n",
    "    agg = (best_method_per_anom.groupby([\"setup\",\"win\",\"kfold\"], as_index=False)\n",
    "                           .agg(mean_auc_pr=(\"auc_pr_median_filled\", \"mean\"),\n",
    "                                mean_roc_auc=(\"roc_auc_median_filled\", \"mean\"),\n",
    "                                n_anoms=(\"anomaly\", \"nunique\")))\n",
    "\n",
    "    # Prefer combos that cover more anomalies, then mean AUCPR, mean ROC, smaller win/k\n",
    "    agg = agg.sort_values(\n",
    "        [\"n_anoms\", \"mean_auc_pr\", \"mean_roc_auc\", \"win\", \"kfold\"],\n",
    "        ascending=[False, False, False, True, True]\n",
    "    )\n",
    "    best_combo = agg.iloc[0]\n",
    "    win_best = int(best_combo[\"win\"])\n",
    "    kf_best  = int(best_combo[\"kfold\"])\n",
    "\n",
    "    platform_rows.append({\n",
    "        \"setup\": setup,\n",
    "        \"win\": win_best,\n",
    "        \"kfold\": kf_best,\n",
    "        \"mean_auc_pr_across_anomalies\": float(best_combo[\"mean_auc_pr\"]),\n",
    "        \"mean_roc_auc_across_anomalies\": float(best_combo[\"mean_roc_auc\"]),\n",
    "        \"num_anomalies_covered\": int(best_combo[\"n_anoms\"]),\n",
    "        \"aggregation_note\": \"anomaly-wise best method @ best pct, then mean across anomalies\",\n",
    "    })\n",
    "\n",
    "    # For chosen (win,kfold), list anomaly-wise best pct + best method + metrics\n",
    "    chosen_detail = best_method_per_anom[\n",
    "        (best_method_per_anom[\"win\"] == win_best) &\n",
    "        (best_method_per_anom[\"kfold\"] == kf_best)\n",
    "    ].copy()\n",
    "\n",
    "    # Sort for display: higher AUCPR, higher ROC, smaller pct\n",
    "    chosen_detail = chosen_detail.sort_values(\n",
    "        [\"auc_pr_median_filled\", \"roc_auc_median_filled\", \"best_pct_by_median\"],\n",
    "        ascending=[False, False, True]\n",
    "    )\n",
    "\n",
    "    print(f\"  [PLATFORM BEST • {setup}]  WIN={win_best}  K={kf_best}  \"\n",
    "          f\"mean AUCPR={float(best_combo['mean_auc_pr']):.3f}  mean ROC={float(best_combo['mean_roc_auc']):.3f}  \"\n",
    "          f\"(across {int(best_combo['n_anoms'])} anomalies)\")\n",
    "\n",
    "    for _, r in chosen_detail.iterrows():\n",
    "        pct_best = int(r[\"best_pct_by_median\"])\n",
    "        aucpr = float(r[\"auc_pr_median\"]) if np.isfinite(r[\"auc_pr_median\"]) else float(\"nan\")\n",
    "        roc   = float(r[\"roc_auc_median\"]) if np.isfinite(r[\"roc_auc_median\"]) else float(\"nan\")\n",
    "        method = str(r[\"method\"])\n",
    "        anom = str(r[\"anomaly\"])\n",
    "        print(f\"      {setup}/{anom}: best method={method}  best %={pct_best}  \"\n",
    "              f\"AUCPR_med={aucpr:.3f}  ROC_med={roc:.3f}\")\n",
    "\n",
    "        chosen_rows.append({\n",
    "            \"setup\": setup,\n",
    "            \"anomaly\": anom,\n",
    "            \"win\": win_best,\n",
    "            \"kfold\": kf_best,\n",
    "            \"best_method\": method,\n",
    "            \"best_pct_by_median\": pct_best,\n",
    "            \"auc_pr_median\": aucpr,\n",
    "            \"roc_auc_median\": roc,\n",
    "            \"n_runs\": int(r[\"n_runs\"]) if \"n_runs\" in r else np.nan,\n",
    "        })\n",
    "\n",
    "# ---------- 3) Save outputs ----------\n",
    "platform_best = pd.DataFrame(platform_rows)\n",
    "PLATFORM_BEST_CSV = RES_DIR / \"BEST_win_kfold_per_platform.csv\"\n",
    "platform_best.to_csv(PLATFORM_BEST_CSV, index=False)\n",
    "\n",
    "chosen_pct = pd.DataFrame(chosen_rows)\n",
    "CHOSEN_PCT_CSV = RES_DIR / \"BEST_pct_for_chosen_win_kfold_per_platform.csv\"\n",
    "chosen_pct.to_csv(CHOSEN_PCT_CSV, index=False)\n",
    "\n",
    "print(f\"\\n[OK] Saved platform winners → {PLATFORM_BEST_CSV}\")\n",
    "print(f\"[OK] Saved per-anomaly details for chosen (WIN,K) → {CHOSEN_PCT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c303f39-ef31-4f60-b6c8-6df9189adbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SCAN] Choosing ONE (WIN,K) per platform (mean AUCPR across anomalies)\n",
      "  [PLATFORM BEST • DDR4] WIN=512 K=3 mean AUCPR=1.000 mean ROC=1.000 (anoms covered=2)\n",
      "      DDR4/DROOP: method=dC_aJ  best %=10  AUCPR_med=1.000 ROC_med=1.000\n",
      "      DDR4/RH: method=dC_aJ  best %=10  AUCPR_med=1.000 ROC_med=1.000\n",
      "  [PLATFORM BEST • DDR5] WIN=1024 K=5 mean AUCPR=0.999 mean ROC=0.998 (anoms covered=2)\n",
      "      DDR5/SPECTRE: method=dC_aJ  best %=10  AUCPR_med=1.000 ROC_med=1.000\n",
      "      DDR5/DROOP: method=dC_aJ  best %=20  AUCPR_med=0.997 ROC_med=0.997\n",
      "\n",
      "[OK] Wrote per-platform winners → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
      "[OK] Wrote per-platform anomaly details → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
      "\n",
      "[PLATFORM WINNERS]\n",
      "setup  win  kfold  auc_pr_median  roc_auc_median  num_anomalies_covered\n",
      " DDR4  512      3       1.000000        1.000000                      2\n",
      " DDR5 1024      5       0.998538        0.998457                      2\n"
     ]
    }
   ],
   "source": [
    "# === Generate BEST_in_DesignSpace_Post_per_platform.csv (PER PLATFORM, not per testcase) ===\n",
    "# Input:  Results/per_run_metrics_all_PIPELINE.csv  (from your new pipeline)\n",
    "# Output: Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
    "#         Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#\n",
    "# What \"per platform\" means here:\n",
    "#   For each platform (DDR4, DDR5), pick ONE (WIN, K) that is best on average across that\n",
    "#   platform's anomalies, where each anomaly contributes its BEST (method, pct) under that (WIN,K)\n",
    "#   using PR-first (median AUCPR), tie ROC, tie smaller pct.\n",
    "#\n",
    "# Notes:\n",
    "# - Uses medians across run_id (so it's stable).\n",
    "# - Uses anomalies set:\n",
    "#     DDR4: DROOP, RH\n",
    "#     DDR5: DROOP, SPECTRE\n",
    "# - Uses method order as stable tie-break: dC_aJ, dC_aM, dE_aJ, dE_aM\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/Users/hsiaopingni/octaneX_v7_4functions\")\n",
    "RES_DIR = ROOT / \"Results\"\n",
    "PER_RUN = RES_DIR / \"per_run_metrics_all_PIPELINE.csv\"\n",
    "\n",
    "if not PER_RUN.exists():\n",
    "    raise FileNotFoundError(f\"Missing pipeline per-run CSV: {PER_RUN}\")\n",
    "\n",
    "df = pd.read_csv(PER_RUN).copy()\n",
    "\n",
    "ANOMALIES_BY_SETUP = {\"DDR4\": [\"DROOP\",\"RH\"], \"DDR5\": [\"DROOP\",\"SPECTRE\"]}\n",
    "METHOD_ORDER = [\"dC_aJ\", \"dC_aM\", \"dE_aJ\", \"dE_aM\"]\n",
    "\n",
    "# Defensive grid (match your pipeline)\n",
    "valid_wins   = {32, 64, 128, 512, 1024}\n",
    "valid_kfolds = {3, 5, 10}\n",
    "\n",
    "# Basic cleaning / typing\n",
    "df = df[df[\"setup\"].isin([\"DDR4\",\"DDR5\"])].copy()\n",
    "df = df[df[\"win\"].isin(valid_wins) & df[\"kfold\"].isin(valid_kfolds)].copy()\n",
    "df = df[df[\"method\"].isin(METHOD_ORDER)].copy()\n",
    "\n",
    "for c in [\"win\",\"kfold\",\"pct\",\"auc_pr\",\"roc_auc\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\"]).copy()\n",
    "df[\"auc_pr\"]  = pd.to_numeric(df[\"auc_pr\"], errors=\"coerce\").clip(0.0, 1.0)\n",
    "df[\"roc_auc\"] = pd.to_numeric(df[\"roc_auc\"], errors=\"coerce\").clip(0.0, 1.0)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Summarize per (setup, anomaly, win, kfold, method, pct) over run_id\n",
    "# -------------------------------------------------------------------\n",
    "def q1(x): return float(np.nanpercentile(x, 25))\n",
    "def q3(x): return float(np.nanpercentile(x, 75))\n",
    "\n",
    "sum_keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"pct\"]\n",
    "sumdf = (df.groupby(sum_keys, as_index=False)\n",
    "           .agg(\n",
    "               auc_pr_median=(\"auc_pr\",\"median\"),\n",
    "               roc_auc_median=(\"roc_auc\",\"median\"),\n",
    "               auc_pr_q1=(\"auc_pr\", q1),\n",
    "               auc_pr_q3=(\"auc_pr\", q3),\n",
    "               roc_auc_q1=(\"roc_auc\", q1),\n",
    "               roc_auc_q3=(\"roc_auc\", q3),\n",
    "               auc_pr_min=(\"auc_pr\",\"min\"),\n",
    "               auc_pr_max=(\"auc_pr\",\"max\"),\n",
    "               roc_auc_min=(\"roc_auc\",\"min\"),\n",
    "               roc_auc_max=(\"roc_auc\",\"max\"),\n",
    "               n_runs=(\"run_id\",\"nunique\"),\n",
    "           ))\n",
    "sumdf[\"auc_pr_iqr\"]  = sumdf[\"auc_pr_q3\"]  - sumdf[\"auc_pr_q1\"]\n",
    "sumdf[\"roc_auc_iqr\"] = sumdf[\"roc_auc_q3\"] - sumdf[\"roc_auc_q1\"]\n",
    "\n",
    "sumdf[\"auc_pr_median_filled\"]  = sumdf[\"auc_pr_median\"].fillna(-np.inf)\n",
    "sumdf[\"roc_auc_median_filled\"] = sumdf[\"roc_auc_median\"].fillna(-np.inf)\n",
    "\n",
    "# Keep only defined anomalies\n",
    "def _keep_defined(row):\n",
    "    setup = str(row[\"setup\"]).upper()\n",
    "    anom  = str(row[\"anomaly\"]).upper()\n",
    "    return setup in ANOMALIES_BY_SETUP and anom in [a.upper() for a in ANOMALIES_BY_SETUP[setup]]\n",
    "\n",
    "sumdf = sumdf[sumdf.apply(_keep_defined, axis=1)].copy()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) For each (setup, anomaly, win, kfold, method): pick BEST pct\n",
    "#     PR-first, ROC-second, smaller pct\n",
    "# -------------------------------------------------------------------\n",
    "best_pct = (sumdf.sort_values(\n",
    "                [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\n",
    "                 \"auc_pr_median_filled\",\"roc_auc_median_filled\",\"pct\"],\n",
    "                ascending=[True,True,True,True,True, False,False, True]\n",
    "            )\n",
    "            .groupby([\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\"], as_index=False)\n",
    "            .head(1)\n",
    "            .rename(columns={\"pct\":\"best_pct_by_median\"}))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) For each (setup, anomaly, win, kfold): pick BEST method (using its BEST pct)\n",
    "#     PR-first, ROC-second, stable method order, then smaller pct\n",
    "# -------------------------------------------------------------------\n",
    "method_rank = {m:i for i,m in enumerate(METHOD_ORDER)}\n",
    "best_pct[\"_mrank\"] = best_pct[\"method\"].map(method_rank).fillna(999).astype(int)\n",
    "\n",
    "best_method = (best_pct.sort_values(\n",
    "                    [\"setup\",\"anomaly\",\"win\",\"kfold\",\n",
    "                     \"auc_pr_median_filled\",\"roc_auc_median_filled\",\"_mrank\",\"best_pct_by_median\"],\n",
    "                    ascending=[True,True,True,True, False,False, True, True]\n",
    "               )\n",
    "               .groupby([\"setup\",\"anomaly\",\"win\",\"kfold\"], as_index=False)\n",
    "               .head(1))\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) For each platform, pick ONE (win,kfold) by averaging anomaly-wise best\n",
    "#     Prefer combos that cover more anomalies, then mean AUCPR, mean ROC,\n",
    "#     then smaller win, smaller kfold\n",
    "# -------------------------------------------------------------------\n",
    "platform_rows = []\n",
    "detail_rows = []\n",
    "\n",
    "print(\"\\n[SCAN] Choosing ONE (WIN,K) per platform (mean AUCPR across anomalies)\")\n",
    "\n",
    "for setup in [\"DDR4\",\"DDR5\"]:\n",
    "    sub = best_method[best_method[\"setup\"].astype(str).str.upper()==setup].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"  [WARN] No rows for platform={setup}\")\n",
    "        continue\n",
    "\n",
    "    agg = (sub.groupby([\"setup\",\"win\",\"kfold\"], as_index=False)\n",
    "             .agg(mean_auc_pr=(\"auc_pr_median_filled\",\"mean\"),\n",
    "                  mean_roc_auc=(\"roc_auc_median_filled\",\"mean\"),\n",
    "                  n_anoms=(\"anomaly\",\"nunique\")))\n",
    "\n",
    "    agg = agg.sort_values(\n",
    "        [\"n_anoms\",\"mean_auc_pr\",\"mean_roc_auc\",\"win\",\"kfold\"],\n",
    "        ascending=[False, False, False, True, True]\n",
    "    )\n",
    "    top = agg.iloc[0]\n",
    "    win_best = int(top[\"win\"])\n",
    "    kf_best  = int(top[\"kfold\"])\n",
    "\n",
    "    platform_rows.append({\n",
    "        \"setup\": setup,\n",
    "        \"win\": win_best,\n",
    "        \"kfold\": kf_best,\n",
    "        \"best_pct_by_median\": np.nan,  # not single pct at platform-level; varies by anomaly\n",
    "        \"method\": \"BEST_PER_ANOMALY\",  # varies by anomaly\n",
    "        \"auc_pr_median\": float(top[\"mean_auc_pr\"]),\n",
    "        \"roc_auc_median\": float(top[\"mean_roc_auc\"]),\n",
    "        \"auc_pr_iqr\": np.nan,\n",
    "        \"roc_auc_iqr\": np.nan,\n",
    "        \"auc_pr_min\": np.nan,\n",
    "        \"auc_pr_max\": np.nan,\n",
    "        \"roc_auc_min\": np.nan,\n",
    "        \"roc_auc_max\": np.nan,\n",
    "        \"num_anomalies_covered\": int(top[\"n_anoms\"]),\n",
    "        \"selection_note\": \"mean across anomalies; each anomaly uses its best (method,pct) under (win,kfold)\",\n",
    "    })\n",
    "\n",
    "    chosen = sub[(sub[\"win\"]==win_best) & (sub[\"kfold\"]==kf_best)].copy()\n",
    "    chosen = chosen.sort_values(\n",
    "        [\"auc_pr_median_filled\",\"roc_auc_median_filled\",\"best_pct_by_median\"],\n",
    "        ascending=[False, False, True]\n",
    "    )\n",
    "\n",
    "    print(f\"  [PLATFORM BEST • {setup}] WIN={win_best} K={kf_best} \"\n",
    "          f\"mean AUCPR={float(top['mean_auc_pr']):.3f} mean ROC={float(top['mean_roc_auc']):.3f} \"\n",
    "          f\"(anoms covered={int(top['n_anoms'])})\")\n",
    "\n",
    "    for _, r in chosen.iterrows():\n",
    "        detail_rows.append({\n",
    "            \"setup\": setup,\n",
    "            \"anomaly\": str(r[\"anomaly\"]),\n",
    "            \"win\": win_best,\n",
    "            \"kfold\": kf_best,\n",
    "            \"best_pct_by_median\": int(r[\"best_pct_by_median\"]),\n",
    "            \"method\": str(r[\"method\"]),\n",
    "            \"auc_pr_median\": float(r[\"auc_pr_median\"]),\n",
    "            \"roc_auc_median\": float(r[\"roc_auc_median\"]),\n",
    "            \"auc_pr_iqr\": float(r[\"auc_pr_iqr\"]),\n",
    "            \"roc_auc_iqr\": float(r[\"roc_auc_iqr\"]),\n",
    "            \"auc_pr_min\": float(r[\"auc_pr_min\"]),\n",
    "            \"auc_pr_max\": float(r[\"auc_pr_max\"]),\n",
    "            \"roc_auc_min\": float(r[\"roc_auc_min\"]),\n",
    "            \"roc_auc_max\": float(r[\"roc_auc_max\"]),\n",
    "            \"n_runs\": int(r[\"n_runs\"]),\n",
    "        })\n",
    "        print(f\"      {setup}/{r['anomaly']}: method={r['method']}  best %={int(r['best_pct_by_median'])}  \"\n",
    "              f\"AUCPR_med={float(r['auc_pr_median']):.3f} ROC_med={float(r['roc_auc_median']):.3f}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) Save outputs\n",
    "# -------------------------------------------------------------------\n",
    "platform_best = pd.DataFrame(platform_rows).sort_values([\"setup\"]).reset_index(drop=True)\n",
    "details = pd.DataFrame(detail_rows).sort_values([\"setup\",\"anomaly\"]).reset_index(drop=True)\n",
    "\n",
    "OUT_PLATFORM = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform.csv\"\n",
    "OUT_DETAILS  = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "\n",
    "platform_best.to_csv(OUT_PLATFORM, index=False)\n",
    "details.to_csv(OUT_DETAILS, index=False)\n",
    "\n",
    "print(f\"\\n[OK] Wrote per-platform winners → {OUT_PLATFORM}\")\n",
    "print(f\"[OK] Wrote per-platform anomaly details → {OUT_DETAILS}\")\n",
    "\n",
    "print(\"\\n[PLATFORM WINNERS]\")\n",
    "print(platform_best[[\"setup\",\"win\",\"kfold\",\"auc_pr_median\",\"roc_auc_median\",\"num_anomalies_covered\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c338ee-cc76-4843-a45c-223e995b33be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e41bdb-87b2-4e96-89a8-8eb3531000b8",
   "metadata": {},
   "source": [
    "# Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a8d102-c6c0-4af9-840f-1f42afaec47d",
   "metadata": {},
   "source": [
    "### SHAP explainability for BEST cases only (per test case) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e07fdd8-00df-4a85-b155-bb5f22b8f857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Loaded platform winners (details) rows: 4\n",
      "setup anomaly  win  kfold  best_pct_by_median method\n",
      " DDR4   DROOP  512      3                  10  dC_aJ\n",
      " DDR4      RH  512      3                  10  dC_aJ\n",
      " DDR5   DROOP 1024     10                  10  dC_aJ\n",
      " DDR5 SPECTRE 1024     10                  10  dC_aJ\n",
      "\n",
      "[RUN] SHAP explainability (BEST PLATFORM) → DDR4/DROOP  WIN=512  K=3  PCT=10  METHOD=dC_aJ\n",
      "[EXPL] full → /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_DDR4_DROOP_WIN512_KF3_PCT10_MdC_aJ.csv\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_compute_DDR4_DROOP_WIN512_KF3_PCT10_MdC_aJ.png\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_memory_DDR4_DROOP_WIN512_KF3_PCT10_MdC_aJ.png\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_sensors_DDR4_DROOP_WIN512_KF3_PCT10_MdC_aJ.png\n",
      "\n",
      "[RUN] SHAP explainability (BEST PLATFORM) → DDR4/RH  WIN=512  K=3  PCT=10  METHOD=dC_aJ\n",
      "[EXPL] full → /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_DDR4_RH_WIN512_KF3_PCT10_MdC_aJ.csv\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_compute_DDR4_RH_WIN512_KF3_PCT10_MdC_aJ.png\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_memory_DDR4_RH_WIN512_KF3_PCT10_MdC_aJ.png\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_sensors_DDR4_RH_WIN512_KF3_PCT10_MdC_aJ.png\n",
      "\n",
      "[RUN] SHAP explainability (BEST PLATFORM) → DDR5/DROOP  WIN=1024  K=10  PCT=10  METHOD=dC_aJ\n",
      "[EXPL] full → /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_DDR5_DROOP_WIN1024_KF10_PCT10_MdC_aJ.csv\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_compute_DDR5_DROOP_WIN1024_KF10_PCT10_MdC_aJ.png\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_memory_DDR5_DROOP_WIN1024_KF10_PCT10_MdC_aJ.png\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_sensors_DDR5_DROOP_WIN1024_KF10_PCT10_MdC_aJ.png\n",
      "\n",
      "[RUN] SHAP explainability (BEST PLATFORM) → DDR5/SPECTRE  WIN=1024  K=10  PCT=10  METHOD=dC_aJ\n",
      "[EXPL] full → /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_DDR5_SPECTRE_WIN1024_KF10_PCT10_MdC_aJ.csv\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_compute_DDR5_SPECTRE_WIN1024_KF10_PCT10_MdC_aJ.png\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_memory_DDR5_SPECTRE_WIN1024_KF10_PCT10_MdC_aJ.png\n",
      "[FIG] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/BAR_BESTPLAT_sensors_DDR5_SPECTRE_WIN1024_KF10_PCT10_MdC_aJ.png\n",
      "\n",
      "[OK] Master table → /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_master_all.csv\n"
     ]
    }
   ],
   "source": [
    "# === SHAP explainability for BEST cases only (PER PLATFORM) ==========================\n",
    "# Platform = DDR4, DDR5\n",
    "#\n",
    "# Reads winners from:\n",
    "#   Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#   (generated by your per-platform selector; contains one row per platform×anomaly,\n",
    "#    with chosen WIN/K plus anomaly-specific best METHOD and best PCT)\n",
    "#\n",
    "# Uses your pipeline helpers if present; otherwise uses local rank-reader + simple selection.\n",
    "#\n",
    "# Outputs:\n",
    "#   Results/Explainability_SHAP_BestPlatforms/\n",
    "#       SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN..._KF..._PCT..._METHOD....csv\n",
    "#       SHAP_BESTPLAT_<subspace>_...csv\n",
    "#       figs/BAR_BESTPLAT_<subspace>_...png\n",
    "#       figs/BEE_BESTPLAT_...png\n",
    "#       SHAP_BESTPLAT_master_all.csv\n",
    "# =====================================================================================\n",
    "\n",
    "import os, gc, re, unicodedata\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Paths ----\n",
    "ROOT        = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES_DIR     = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "DATA_DIR    = Path(globals().get(\"DATA_DIR\", \"/Users/hsiaopingni/Desktop/SLM_RAS-main/HW_TELEMETRY_DATA_COLLECTION/TELEMETRY_DATA\"))\n",
    "\n",
    "DETAILS_CSV = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "OUT_DIR     = RES_DIR / \"Explainability_SHAP_BestPlatforms\"\n",
    "FIG_DIR     = OUT_DIR / \"figs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUBSPACES = (\"compute\",\"memory\",\"sensors\")\n",
    "SEED      = int(globals().get(\"SEED\", 42))\n",
    "\n",
    "# ---- Require helpers from your pipeline (recommended) ----\n",
    "_required = [\n",
    "    \"collect_raw_pairs_by_setup\",\"build_windowed_raw_means\",\"telemetry_cols\",\n",
    "    \"robust_scale_train\",\"apply_robust_scale\",\n",
    "    \"slice_by_percent\",\"intersect_selection_with_columns_robust\",\n",
    "]\n",
    "_missing = [r for r in _required if r not in globals()]\n",
    "if _missing:\n",
    "    raise RuntimeError(\n",
    "        f\"Missing required helpers from your pipeline: {_missing}\\n\"\n",
    "        f\"Run the pipeline script first (or import it) so these functions exist in globals().\"\n",
    "    )\n",
    "\n",
    "# ---- Rank roots (use your existing RANK_DIRS if present) ----\n",
    "if \"RANK_DIRS\" not in globals():\n",
    "    RANK_DIRS = [\n",
    "        ROOT / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"octaneX\" / \"FeatureRankOUT\",\n",
    "    ]\n",
    "\n",
    "def _read_rank_list(rank_dirs, setup, win, kfold, sub):\n",
    "    \"\"\"Return ordered list of features from MMI FeatureRankOUT (first hit wins).\"\"\"\n",
    "    fname = f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "    for d in rank_dirs:\n",
    "        p = Path(d) / fname\n",
    "        if p.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(p)\n",
    "                col_f = \"feature\" if \"feature\" in df.columns else df.columns[0]\n",
    "                return df[col_f].dropna().astype(str).tolist()\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] cannot read {p}: {e}\")\n",
    "    return []\n",
    "\n",
    "def _train_xgb_simple(X_tr, y_tr, seed=SEED):\n",
    "    from xgboost import XGBClassifier\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=250, max_depth=4, learning_rate=0.06,\n",
    "        subsample=0.9, colsample_bytree=0.9,\n",
    "        random_state=seed, tree_method=\"hist\", n_jobs=os.cpu_count()\n",
    "    )\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    return clf\n",
    "\n",
    "def _compute_shap(clf, X_ref):\n",
    "    \"\"\"Return mean|SHAP| per feature (Series). Fall back to XGB gain if shap not installed.\"\"\"\n",
    "    try:\n",
    "        import shap\n",
    "        explainer = shap.TreeExplainer(clf, feature_perturbation=\"interventional\")\n",
    "        sv = explainer.shap_values(X_ref)\n",
    "        if isinstance(sv, list):  # binary -> [neg,pos]\n",
    "            vals = np.abs(sv[1]).mean(axis=0)\n",
    "        else:\n",
    "            vals = np.abs(sv).mean(axis=0)\n",
    "        return pd.Series(vals, index=X_ref.columns, name=\"shap_mean_abs\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            booster = clf.get_booster()\n",
    "            gain = booster.get_score(importance_type=\"gain\")\n",
    "            s = pd.Series(gain, dtype=float)\n",
    "            s = s.reindex(X_ref.columns).fillna(0.0)\n",
    "            s.name = \"shap_mean_abs\"\n",
    "            print(\"[WARN] shap unavailable; fell back to XGB gain.\")\n",
    "            return s\n",
    "        except Exception:\n",
    "            return pd.Series(0.0, index=X_ref.columns, name=\"shap_mean_abs\")\n",
    "\n",
    "def _beeswarm_plot(clf, X, title, out_png, max_display=40):\n",
    "    try:\n",
    "        import shap\n",
    "        explainer = shap.TreeExplainer(clf, feature_perturbation=\"interventional\")\n",
    "        sv = explainer.shap_values(X)\n",
    "        if isinstance(sv, list): sv = sv[1]\n",
    "        plt.figure(figsize=(10,6))\n",
    "        shap.summary_plot(sv, X, show=False, max_display=max_display)\n",
    "        plt.title(title); plt.tight_layout()\n",
    "        plt.savefig(out_png, dpi=220); plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] SHAP beeswarm skipped: {e}\")\n",
    "\n",
    "def _barplot_subspace(df_sub, title, out_png, topk=30):\n",
    "    d = df_sub.sort_values(\"shap_mean_abs\", ascending=False)\n",
    "    if topk is not None and topk > 0:\n",
    "        d = d.head(topk)\n",
    "    plt.figure(figsize=(10, max(3.5, 0.25*len(d))))\n",
    "    plt.barh(d[\"feature\"], d[\"shap_mean_abs\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"mean |SHAP| (validation set)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "# ---- Load per-platform details (winners) ----\n",
    "if not DETAILS_CSV.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing platform details CSV: {DETAILS_CSV}\\n\"\n",
    "        f\"Generate it first using your per-platform winners code:\\n\"\n",
    "        f\"  - BEST_in_DesignSpace_Post_per_platform.csv\\n\"\n",
    "        f\"  - BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "    )\n",
    "\n",
    "winners = pd.read_csv(DETAILS_CSV).copy()\n",
    "\n",
    "# Normalize/validate columns\n",
    "# expected columns (from your generator):\n",
    "# setup, anomaly, win, kfold, best_pct_by_median, method, auc_pr_median, roc_auc_median, ...\n",
    "colmap = {}\n",
    "if \"best_method\" in winners.columns and \"method\" not in winners.columns:\n",
    "    colmap[\"best_method\"] = \"method\"\n",
    "winners = winners.rename(columns=colmap)\n",
    "\n",
    "required_cols = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"]\n",
    "missing = [c for c in required_cols if c not in winners.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Platform details CSV missing required columns: {missing}. Have: {list(winners.columns)}\")\n",
    "\n",
    "# Keep only defined anomalies per platform (defensive)\n",
    "ANOMALIES_BY_SETUP = {\"DDR4\": [\"DROOP\",\"RH\"], \"DDR5\": [\"DROOP\",\"SPECTRE\"]}\n",
    "def _keep(row):\n",
    "    s = str(row[\"setup\"]).upper()\n",
    "    a = str(row[\"anomaly\"]).upper()\n",
    "    return s in ANOMALIES_BY_SETUP and a in [x.upper() for x in ANOMALIES_BY_SETUP[s]]\n",
    "winners = winners[winners.apply(_keep, axis=1)].copy()\n",
    "winners = winners.reset_index(drop=True)\n",
    "\n",
    "print(f\"[OK] Loaded platform winners (details) rows: {len(winners)}\")\n",
    "print(winners[[\"setup\",\"anomaly\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"]].to_string(index=False))\n",
    "\n",
    "# ---- Run explainability per platform×anomaly best config ----\n",
    "all_rows = []\n",
    "\n",
    "for _, row in winners.iterrows():\n",
    "    setup   = str(row[\"setup\"])\n",
    "    anomaly = str(row[\"anomaly\"])\n",
    "    win     = int(row[\"win\"])\n",
    "    kfold   = int(row[\"kfold\"])\n",
    "    pct     = int(row[\"best_pct_by_median\"])\n",
    "    method  = str(row[\"method\"])\n",
    "\n",
    "    print(f\"\\n[RUN] SHAP explainability (BEST PLATFORM) → {setup}/{anomaly}  WIN={win}  K={kfold}  PCT={pct}  METHOD={method}\")\n",
    "\n",
    "    # 1) Build BENIGN/ANOM windows\n",
    "    ben_pairs = collect_raw_pairs_by_setup(DATA_DIR, which=\"benign\").get(setup, [])\n",
    "    an_pairs  = collect_raw_pairs_by_setup(DATA_DIR, which=\"anomaly\", anomaly=anomaly).get(setup, [])\n",
    "    if not ben_pairs or not an_pairs:\n",
    "        print(f\"[SKIP] Missing RAW files for {setup}/{anomaly}\")\n",
    "        continue\n",
    "\n",
    "    # NOTE: build_windowed_raw_means signature includes overlap_ratio in your pipeline.\n",
    "    # If your helper requires it, it will be in globals() and this call will work.\n",
    "    # If not, you can add overlap_ratio=... as needed.\n",
    "    try:\n",
    "        df_b = build_windowed_raw_means(ben_pairs, setup=setup, win=win, label=\"BENIGN\", overlap_ratio=0.50)\n",
    "        df_a = build_windowed_raw_means(an_pairs,  setup=setup, win=win, label=anomaly, overlap_ratio=(0.80 if anomaly.upper()==\"DROOP\" else 0.50))\n",
    "    except TypeError:\n",
    "        # fallback if your helper doesn't have overlap_ratio\n",
    "        df_b = build_windowed_raw_means(ben_pairs, setup=setup, win=win, label=\"BENIGN\")\n",
    "        df_a = build_windowed_raw_means(an_pairs,  setup=setup, win=win, label=anomaly)\n",
    "\n",
    "    if df_b.empty or df_a.empty:\n",
    "        print(f\"[SKIP] Empty windows for {setup}/{anomaly} WIN={win}\")\n",
    "        continue\n",
    "\n",
    "    # 2) Robust scale on BENIGN; apply to all\n",
    "    xb_cols = telemetry_cols(df_b)\n",
    "    if not xb_cols:\n",
    "        print(f\"[SKIP] No telemetry cols for {setup}/{anomaly}\")\n",
    "        continue\n",
    "\n",
    "    Xb = df_b[xb_cols].astype(float)\n",
    "    mu, sd, q1, q2 = robust_scale_train(Xb.values, winsor=(2.0, 98.0))\n",
    "    Xb_z = apply_robust_scale(Xb, mu, sd, q1, q2)\n",
    "\n",
    "    Xa = df_a[[c for c in xb_cols if c in df_a.columns]].astype(float)\n",
    "    Xa_z = apply_robust_scale(Xa, mu, sd, q1, q2)\n",
    "\n",
    "    # Align columns (just in case)\n",
    "    common_cols = [c for c in Xb_z.columns if c in Xa_z.columns]\n",
    "    if not common_cols:\n",
    "        print(f\"[SKIP] No common scaled columns for {setup}/{anomaly}\")\n",
    "        continue\n",
    "    Xb_z = Xb_z[common_cols]\n",
    "    Xa_z = Xa_z[common_cols]\n",
    "\n",
    "    X_all = pd.concat([Xb_z, Xa_z], ignore_index=True)\n",
    "    y_all = np.concatenate([np.zeros(len(Xb_z), dtype=int), np.ones(len(Xa_z), dtype=int)])\n",
    "\n",
    "    # 3) One validation mask per case\n",
    "    rng = np.random.RandomState(SEED)\n",
    "    idx = np.arange(len(y_all))\n",
    "    val_mask = np.zeros_like(y_all, dtype=bool)\n",
    "    val_mask[rng.choice(idx, size=max(1, int(0.30*len(y_all))), replace=False)] = True\n",
    "\n",
    "    # 4) MMI selection at best pct (robust mapping against columns)\n",
    "    full_lists = {sub: _read_rank_list(RANK_DIRS, setup, win, kfold, sub) for sub in SUBSPACES}\n",
    "\n",
    "    # Your pipeline's slice_by_percent returns sel dict (not tuple). Handle both.\n",
    "    sel_raw = slice_by_percent(full_lists, pct)\n",
    "    if isinstance(sel_raw, tuple):\n",
    "        sel_raw = sel_raw[0]\n",
    "\n",
    "    sel_map = intersect_selection_with_columns_robust(sel_raw, set(X_all.columns))\n",
    "    if isinstance(sel_map, tuple):\n",
    "        sel_map = sel_map[0]\n",
    "\n",
    "    chosen, feat_sub = [], {}\n",
    "    for sub in SUBSPACES:\n",
    "        feats = list(sel_map.get(sub, []) or [])\n",
    "        for f in feats:\n",
    "            feat_sub[f] = sub\n",
    "        chosen.extend(feats)\n",
    "\n",
    "    chosen = [c for c in dict.fromkeys(chosen) if c in X_all.columns]\n",
    "    if not chosen:\n",
    "        print(f\"[SKIP] No features after mapping for {setup}/{anomaly} at WIN={win},K={kfold},PCT={pct}\")\n",
    "        continue\n",
    "\n",
    "    X = X_all[chosen]\n",
    "    X_tr, y_tr = X.loc[~val_mask], y_all[~val_mask]\n",
    "    X_va, y_va = X.loc[val_mask],  y_all[val_mask]\n",
    "\n",
    "    # 5) Train and compute SHAP\n",
    "    clf     = _train_xgb_simple(X_tr, y_tr, seed=SEED)\n",
    "    shap_s  = _compute_shap(clf, X_va)\n",
    "\n",
    "    # z²-lift on validation (anomaly minus benign)\n",
    "    ben_mask = (y_va == 0)\n",
    "    an_mask  = (y_va == 1)\n",
    "    if ben_mask.sum() == 0 or an_mask.sum() == 0:\n",
    "        z2_lift = np.zeros(X_va.shape[1], dtype=float)\n",
    "    else:\n",
    "        z2_lift = ((X_va.values[an_mask]**2).mean(axis=0) - (X_va.values[ben_mask]**2).mean(axis=0))\n",
    "    z2_s = pd.Series(z2_lift, index=X.columns, name=\"z2_lift\")\n",
    "\n",
    "    df_all = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": X.columns,\n",
    "            \"subspace\": [feat_sub.get(c, \"compute\") for c in X.columns],\n",
    "            \"shap_mean_abs\": shap_s.reindex(X.columns).fillna(0.0).values,\n",
    "            \"z2_lift\": z2_s.reindex(X.columns).fillna(0.0).values,\n",
    "            \"setup\": setup, \"anomaly\": anomaly, \"win\": win, \"kfold\": kfold, \"pct\": pct, \"method\": method\n",
    "        })\n",
    "        .sort_values(\"shap_mean_abs\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # 6) Save outputs (full + per-subspace + plots)\n",
    "    base = f\"{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT{pct}_M{method}\"\n",
    "    out_full = OUT_DIR / f\"SHAP_BESTPLAT_full_{base}.csv\"\n",
    "    df_all.to_csv(out_full, index=False)\n",
    "    print(f\"[EXPL] full → {out_full}\")\n",
    "\n",
    "    for sub in SUBSPACES:\n",
    "        sub_df = df_all[df_all[\"subspace\"]==sub].copy()\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "        out_sub = OUT_DIR / f\"SHAP_BESTPLAT_{sub}_{base}.csv\"\n",
    "        sub_df.to_csv(out_sub, index=False)\n",
    "\n",
    "        png = FIG_DIR / f\"BAR_BESTPLAT_{sub}_{base}.png\"\n",
    "        _barplot_subspace(\n",
    "            sub_df,\n",
    "            title=f\"{setup}/{anomaly} • {sub} • WIN={win} K={kfold} PCT={pct} • {method}\",\n",
    "            out_png=png,\n",
    "            topk=30\n",
    "        )\n",
    "        print(f\"[FIG] {png}\")\n",
    "\n",
    "    bees = FIG_DIR / f\"BEE_BESTPLAT_{base}.png\"\n",
    "    _beeswarm_plot(\n",
    "        clf, X_va,\n",
    "        title=f\"{setup}/{anomaly} • WIN={win} K={kfold} PCT={pct} • {method} (validation)\",\n",
    "        out_png=bees,\n",
    "        max_display=40\n",
    "    )\n",
    "\n",
    "    all_rows.append(df_all)\n",
    "    gc.collect()\n",
    "\n",
    "# 7) Master table\n",
    "if all_rows:\n",
    "    master = pd.concat(all_rows, ignore_index=True)\n",
    "    master_csv = OUT_DIR / \"SHAP_BESTPLAT_master_all.csv\"\n",
    "    master.to_csv(master_csv, index=False)\n",
    "    print(f\"\\n[OK] Master table → {master_csv}\")\n",
    "else:\n",
    "    print(\"\\n[WARN] No SHAP outputs were produced (check platform winners/ranks/data).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f060f89-26a4-4f14-a5b5-547ea4685680",
   "metadata": {},
   "source": [
    "## *SHAP explainability for BEST cases only (per platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27c789cb-4b2c-49a1-a3fc-a683ab97740d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/FIG_Top10_SHAP_LEGEND.png\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_PLATFORM_AGG_DDR4.csv\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/FIG_Top10_SHAP_PLATFORM_DDR4_PLOT.png\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_PLATFORM_AGG_DDR5.csv\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/figs/FIG_Top10_SHAP_PLATFORM_DDR5_PLOT.png\n"
     ]
    }
   ],
   "source": [
    "# === PLATFORM-LEVEL SHAP Top-10 plot-only PNGs (ONE per platform) + FULL BLACK BOX + legend ===\n",
    "# This aggregates SHAP across the platform's anomalies (using the chosen best configs in\n",
    "# BEST_in_DesignSpace_Post_per_platform_details.csv) and produces ONE Top-10 bar plot per platform.\n",
    "#\n",
    "# Inputs:\n",
    "#   1) Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#   2) Results/Explainability_SHAP_BestPlatforms/\n",
    "#        SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN<w>_KF<k>_PCT<p>_M<method>.csv\n",
    "#\n",
    "# Outputs (3 PNGs total):\n",
    "#   1) figs/FIG_Top10_SHAP_LEGEND.png\n",
    "#   2) figs/FIG_Top10_SHAP_PLATFORM_DDR4_PLOT.png\n",
    "#   3) figs/FIG_Top10_SHAP_PLATFORM_DDR5_PLOT.png\n",
    "#\n",
    "# Aggregation rule:\n",
    "#   - For each platform, load the anomaly-specific SHAP tables (already at each anomaly's best config)\n",
    "#   - Compute mean SHAP (mean |value|) per feature ACROSS anomalies for that platform:\n",
    "#       shap_platform(feature) = mean_over_anomalies(shap_mean_abs_anomaly(feature))\n",
    "#     Missing features in an anomaly contribute 0 for that anomaly.\n",
    "#   - Subspace label per feature: majority vote across anomalies (tie -> subspace of highest mean SHAP).\n",
    "#\n",
    "# Styling:\n",
    "#   - Full black rectangle frame (all four spines visible & black), like your paper figure\n",
    "#   - Separate legend PNG\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------ Paths ------------------\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES_DIR = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "\n",
    "DETAILS = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "if not DETAILS.exists():\n",
    "    raise FileNotFoundError(f\"Missing platform details CSV: {DETAILS}\")\n",
    "\n",
    "PLAT_DIR = RES_DIR / \"Explainability_SHAP_BestPlatforms\"\n",
    "FIG_DIR  = PLAT_DIR / \"figs\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------ Colors match paper style ------------------\n",
    "DOMAIN_COLORS = {\"compute\": \"#1f77b4\", \"memory\": \"#ff7f0e\", \"sensors\": \"#2ca02c\"}  # blue, orange, green\n",
    "DOMAIN_LABELS = {\"compute\": \"Compute\", \"memory\": \"Memory\", \"sensors\": \"Sensors\"}\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 13,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.25,\n",
    "    \"grid.linestyle\": \"-\",\n",
    "})\n",
    "\n",
    "def _apply_full_black_box(ax, lw=1.1):\n",
    "    \"\"\"Make a full black rectangle frame like the paper (all 4 spines).\"\"\"\n",
    "    for side in [\"left\", \"right\", \"top\", \"bottom\"]:\n",
    "        ax.spines[side].set_visible(True)\n",
    "        ax.spines[side].set_color(\"black\")\n",
    "        ax.spines[side].set_linewidth(lw)\n",
    "\n",
    "def save_legend_only(out_png: Path):\n",
    "    fig, ax = plt.subplots(figsize=(3.0, 1.4), dpi=220)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    handles, labels = [], []\n",
    "    for key in [\"compute\", \"memory\", \"sensors\"]:\n",
    "        handles.append(plt.Rectangle((0, 0), 1, 1, color=DOMAIN_COLORS[key]))\n",
    "        labels.append(DOMAIN_LABELS[key])\n",
    "\n",
    "    ax.legend(handles, labels, loc=\"center\", frameon=True, fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_png, dpi=300, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    plt.close(fig)\n",
    "    print(\"[WROTE]\", out_png)\n",
    "\n",
    "def _load_bestplat_csv(setup: str, anomaly: str, win: int, kfold: int, pct: int, method: str) -> pd.DataFrame:\n",
    "    p = PLAT_DIR / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT{pct}_M{method}.csv\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing best-platform SHAP CSV: {p}\")\n",
    "    df = pd.read_csv(p)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    need = {\"feature\", \"subspace\", \"shap_mean_abs\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        raise KeyError(f\"{p} missing columns {need - set(df.columns)}. Have: {list(df.columns)}\")\n",
    "    df[\"feature\"] = df[\"feature\"].astype(str)\n",
    "    df[\"subspace\"] = df[\"subspace\"].astype(str).str.lower()\n",
    "    df[\"shap_mean_abs\"] = pd.to_numeric(df[\"shap_mean_abs\"], errors=\"coerce\").fillna(0.0)\n",
    "    return df[[\"feature\",\"subspace\",\"shap_mean_abs\"]].copy()\n",
    "\n",
    "def _aggregate_platform_shap(cfg_rows: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    cfg_rows: rows for one setup (platform), each row includes anomaly/win/kfold/pct/method.\n",
    "    Returns: aggregated per-feature table with columns:\n",
    "      feature, subspace, shap_mean_abs_platform\n",
    "    \"\"\"\n",
    "    if cfg_rows.empty:\n",
    "        return pd.DataFrame(columns=[\"feature\",\"subspace\",\"shap_mean_abs_platform\"])\n",
    "\n",
    "    dfs = []\n",
    "    for _, r in cfg_rows.iterrows():\n",
    "        setup  = str(r[\"setup\"])\n",
    "        anomaly= str(r[\"anomaly\"])\n",
    "        win    = int(r[\"win\"])\n",
    "        kfold  = int(r[\"kfold\"])\n",
    "        pct    = int(r[\"best_pct_by_median\"])\n",
    "        method = str(r[\"method\"])\n",
    "        d = _load_bestplat_csv(setup, anomaly, win, kfold, pct, method)\n",
    "        d = d.rename(columns={\"shap_mean_abs\": f\"shap_{anomaly}\"})\n",
    "        dfs.append(d)\n",
    "\n",
    "    # Outer-merge across anomalies on feature; keep subspace columns for voting\n",
    "    # We merge in a way that preserves all features ever seen across anomalies.\n",
    "    merged = None\n",
    "    for d in dfs:\n",
    "        if merged is None:\n",
    "            merged = d.copy()\n",
    "        else:\n",
    "            # when merging, keep both subspace columns (we'll vote later)\n",
    "            merged = merged.merge(d, on=\"feature\", how=\"outer\", suffixes=(\"\", \"_r\"))\n",
    "\n",
    "            # consolidate/rename subspace columns into a list-friendly set\n",
    "            # after merge, we may have 'subspace' and 'subspace_r'\n",
    "            if \"subspace_r\" in merged.columns:\n",
    "                # keep both; rename to unique col name\n",
    "                # we’ll just leave them and handle later\n",
    "                pass\n",
    "\n",
    "    if merged is None or merged.empty:\n",
    "        return pd.DataFrame(columns=[\"feature\",\"subspace\",\"shap_mean_abs_platform\"])\n",
    "\n",
    "    # Identify all shap columns (one per anomaly)\n",
    "    shap_cols = [c for c in merged.columns if c.startswith(\"shap_\")]\n",
    "    if not shap_cols:\n",
    "        return pd.DataFrame(columns=[\"feature\",\"subspace\",\"shap_mean_abs_platform\"])\n",
    "\n",
    "    # Fill missing SHAP with 0 (means feature absent in that anomaly)\n",
    "    merged[shap_cols] = merged[shap_cols].fillna(0.0)\n",
    "\n",
    "    # Platform aggregate = mean over anomalies (equal weight)\n",
    "    merged[\"shap_mean_abs_platform\"] = merged[shap_cols].mean(axis=1)\n",
    "\n",
    "    # Determine subspace via majority vote across available subspace columns\n",
    "    sub_cols = [c for c in merged.columns if c.startswith(\"subspace\")]\n",
    "    def vote_subspace(row):\n",
    "        vals = [str(row[c]).lower() for c in sub_cols if pd.notna(row[c]) and str(row[c]).strip() != \"\"]\n",
    "        vals = [v for v in vals if v in (\"compute\",\"memory\",\"sensors\")]\n",
    "        if not vals:\n",
    "            return \"compute\"\n",
    "        # majority vote\n",
    "        counts = {k: vals.count(k) for k in (\"compute\",\"memory\",\"sensors\")}\n",
    "        best = max(counts, key=lambda k: counts[k])\n",
    "        # tie-break: choose the subspace corresponding to the anomaly where this feature has max SHAP\n",
    "        top_cnt = counts[best]\n",
    "        ties = [k for k,v in counts.items() if v == top_cnt]\n",
    "        if len(ties) == 1:\n",
    "            return best\n",
    "        # tie: pick subspace of the anomaly with largest shap value among tie subspaces\n",
    "        # (use first non-empty subspace aligned with max shap col)\n",
    "        max_col = shap_cols[int(np.argmax([row[c] for c in shap_cols]))]\n",
    "        # find a subspace column that likely came from same anomaly merge stage:\n",
    "        # simplest tie-break: pick first subspace that is in ties\n",
    "        for v in vals:\n",
    "            if v in ties:\n",
    "                return v\n",
    "        return ties[0]\n",
    "\n",
    "    merged[\"subspace\"] = merged.apply(vote_subspace, axis=1)\n",
    "\n",
    "    out = merged[[\"feature\",\"subspace\",\"shap_mean_abs_platform\"]].copy()\n",
    "    out[\"shap_mean_abs_platform\"] = pd.to_numeric(out[\"shap_mean_abs_platform\"], errors=\"coerce\").fillna(0.0)\n",
    "    return out\n",
    "\n",
    "def save_platform_plot_only(df_platform: pd.DataFrame, setup: str, out_png: Path, topk: int = 10):\n",
    "    d = df_platform.sort_values(\"shap_mean_abs_platform\", ascending=False).head(topk).copy()\n",
    "    if d.empty:\n",
    "        print(\"[SKIP] empty platform df for\", setup)\n",
    "        return\n",
    "\n",
    "    colors = [DOMAIN_COLORS.get(s, \"#777777\") for s in d[\"subspace\"]]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.2, 4.4), dpi=220)\n",
    "    ax.barh(d[\"feature\"], d[\"shap_mean_abs_platform\"], color=colors, edgecolor=\"none\")\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    ax.set_xlabel(\"SHAP (mean |value|) • platform-avg\", fontweight=\"bold\")\n",
    "    ax.grid(True, axis=\"both\", alpha=0.25)\n",
    "\n",
    "    _apply_full_black_box(ax, lw=1.1)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_png, dpi=300, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    plt.close(fig)\n",
    "    print(\"[WROTE]\", out_png)\n",
    "\n",
    "# ------------------ Load per-platform details and build configs ------------------\n",
    "w = pd.read_csv(DETAILS).copy()\n",
    "if \"best_method\" in w.columns and \"method\" not in w.columns:\n",
    "    w = w.rename(columns={\"best_method\":\"method\"})\n",
    "if \"best_pct_by_median\" not in w.columns and \"pct\" in w.columns:\n",
    "    w = w.rename(columns={\"pct\":\"best_pct_by_median\"})\n",
    "\n",
    "need = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"}\n",
    "missing = need - set(w.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"{DETAILS} missing columns {sorted(missing)}. Have: {list(w.columns)}\")\n",
    "\n",
    "# ------------------ 1) Legend PNG (shared) ------------------\n",
    "legend_png = FIG_DIR / \"FIG_Top10_SHAP_LEGEND.png\"\n",
    "save_legend_only(legend_png)\n",
    "\n",
    "# ------------------ 2) One plot per platform ------------------\n",
    "for setup in [\"DDR4\",\"DDR5\"]:\n",
    "    cfg = w[w[\"setup\"].astype(str).str.upper() == setup].copy()\n",
    "    if cfg.empty:\n",
    "        print(\"[WARN] No config rows for\", setup)\n",
    "        continue\n",
    "\n",
    "    # Build platform-level aggregated SHAP\n",
    "    plat = _aggregate_platform_shap(cfg)\n",
    "\n",
    "    # Save the aggregated table too (useful for checking)\n",
    "    agg_csv = PLAT_DIR / f\"SHAP_PLATFORM_AGG_{setup}.csv\"\n",
    "    plat.sort_values(\"shap_mean_abs_platform\", ascending=False).to_csv(agg_csv, index=False)\n",
    "    print(\"[WROTE]\", agg_csv)\n",
    "\n",
    "    out_plot = FIG_DIR / f\"FIG_Top10_SHAP_PLATFORM_{setup}_PLOT.png\"\n",
    "    save_platform_plot_only(plat, setup=setup, out_png=out_plot, topk=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5464ef9a-ba2b-4ed4-b2fe-7e74f02edd60",
   "metadata": {},
   "source": [
    "# **Compare Explainability: SHAP vs CP-MI (BEST cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e325a34-1517-4868-849d-75f9545c3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PLATFORM] DDR4 using WIN=512 K=3\n",
      "\n",
      "[PLATFORM] DDR5 using WIN=1024 K=5\n",
      "\n",
      "[OK] Summary → /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_vs_CPMI_summary_PLATFORM.csv\n",
      "[OK] Plots   → /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/comparison_plots_platform\n"
     ]
    }
   ],
   "source": [
    "# === SHAP vs CP-MI (PER PLATFORM) — stability + concordance plots ============================\n",
    "# This is a PER-PLATFORM revision of your original \"BestCases\" script.\n",
    "#\n",
    "# What changes:\n",
    "#   - Uses per-platform winners from:\n",
    "#       Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#   - Uses SHAP tables generated by your per-platform SHAP run:\n",
    "#       Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN..._KF..._PCT..._M<method>.csv\n",
    "#   - For each PLATFORM (DDR4, DDR5) and each SUBSPACE (compute/memory/sensors), it:\n",
    "#       1) Loads CP-MI ranks (from FeatureRankOUT) for that platform's chosen WIN/K\n",
    "#       2) Aggregates SHAP across the platform's anomalies (equal-weight mean, missing=0)\n",
    "#       3) Computes Spearman (rank/percentile/score) and top-% Jaccard/overlap\n",
    "#       4) Produces platform-level plots (scatter + jaccard curve + overlap bars)\n",
    "#\n",
    "# Outputs:\n",
    "#   Results/Explainability_SHAP_BestPlatforms/\n",
    "#     - SHAP_vs_CPMI_summary_PLATFORM.csv\n",
    "#     - comparison_plots_platform/\n",
    "#         DDR4_compute_scatter_cpmi_vs_shap.png\n",
    "#         DDR4_compute_scatter_percentile_ranks.png\n",
    "#         DDR4_compute_jaccard_vs_pct.png\n",
    "#         DDR4_compute_overlap_bar_pct.png\n",
    "#         ... (memory/sensors, DDR5)\n",
    "# ============================================================================================\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ---------- Paths (ROBUST) ----------\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "BASE_RES = ROOT / \"Results\"\n",
    "\n",
    "# Use the PER-PLATFORM SHAP directory\n",
    "OUT_DIR = BASE_RES / \"Explainability_SHAP_BestPlatforms\"\n",
    "FIG_DIR = OUT_DIR / \"comparison_plots_platform\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DETAILS_CSV = BASE_RES / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "if not DETAILS_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Missing per-platform details CSV: {DETAILS_CSV}\")\n",
    "\n",
    "SUBSPACES = (\"compute\", \"memory\", \"sensors\")\n",
    "TOP_PCTS  = list(range(10, 101, 10))   # for overlap/jaccard sweep\n",
    "TAKE_PCTS = list(range(10, 101, 10))   # which points to record\n",
    "\n",
    "NORMALIZE_BY_PERCENTILE = True\n",
    "\n",
    "# Where CP-MI ranks might be (first hit wins)\n",
    "if \"RANK_DIRS\" not in globals():\n",
    "    RANK_DIRS = [\n",
    "        ROOT / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"octaneX\" / \"FeatureRankOUT\",\n",
    "        Path.home() / \"Desktop\" / \"octaneX\" / \"FeatureRankOUT\",\n",
    "    ]\n",
    "\n",
    "def _read_csv_safe(p: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------------------ CP-MI loader ------------------------\n",
    "def _cpmi_rank_path(setup: str, win: int, kfold: int, sub: str) -> Optional[Path]:\n",
    "    fname = f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "    for d in RANK_DIRS:\n",
    "        p = Path(d) / fname\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # fallback search (rare)\n",
    "    for d in RANK_DIRS:\n",
    "        d = Path(d)\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        hits = list(d.rglob(f\"*{setup}*{sub}*CPMI*.csv\")) + list(d.rglob(f\"*{setup}*{sub}*CP-MI*.csv\"))\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "def _load_cpmi(setup: str, win: int, kfold: int) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns dict[subspace] -> DataFrame(feature, cpmi_score, cpmi_rank), sorted by score desc.\n",
    "    \"\"\"\n",
    "    out: Dict[str, pd.DataFrame] = {}\n",
    "    for sub in SUBSPACES:\n",
    "        rp = _cpmi_rank_path(setup, win, kfold, sub)\n",
    "        if not rp:\n",
    "            out[sub] = pd.DataFrame()\n",
    "            continue\n",
    "        df = _read_csv_safe(rp)\n",
    "        if df.empty:\n",
    "            out[sub] = df\n",
    "            continue\n",
    "\n",
    "        feat_col = None\n",
    "        for c in df.columns:\n",
    "            if c.lower() in (\"feature\", \"features\", \"name\", \"signal\", \"column\"):\n",
    "                feat_col = c\n",
    "                break\n",
    "        if feat_col is None:\n",
    "            nonnum = df.select_dtypes(exclude=[np.number]).columns\n",
    "            feat_col = nonnum[0] if len(nonnum) else df.columns[0]\n",
    "\n",
    "        score_col = None\n",
    "        for c in df.columns:\n",
    "            if c.lower() in (\"cpmi\", \"cp-mi\", \"score\", \"mi\", \"mi_score\", \"rank_score\"):\n",
    "                score_col = c\n",
    "                break\n",
    "        if score_col is None:\n",
    "            nums = df.select_dtypes(include=[np.number]).columns\n",
    "            score_col = nums[0] if len(nums) else df.columns[-1]\n",
    "\n",
    "        d = (\n",
    "            df[[feat_col, score_col]]\n",
    "            .rename(columns={feat_col: \"feature\", score_col: \"cpmi_score\"})\n",
    "            .assign(feature=lambda x: x[\"feature\"].astype(str).str.strip())\n",
    "            .dropna(subset=[\"feature\"])\n",
    "            .drop_duplicates(subset=[\"feature\"])\n",
    "            .sort_values(\"cpmi_score\", ascending=False)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        d[\"cpmi_rank\"] = np.arange(1, len(d) + 1)\n",
    "        out[sub] = d\n",
    "    return out\n",
    "\n",
    "# ------------------------ SHAP loader (per-platform best) ------------------------\n",
    "def _shap_bestplat_full_path(setup: str, anomaly: str, win: int, kfold: int, pct: int, method: str) -> Path:\n",
    "    return OUT_DIR / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT{pct}_M{method}.csv\"\n",
    "\n",
    "def _load_shap_bestplat_full(setup: str, anomaly: str, win: int, kfold: int, pct: int, method: str) -> pd.DataFrame:\n",
    "    p = _shap_bestplat_full_path(setup, anomaly, win, kfold, pct, method)\n",
    "    df = _read_csv_safe(p)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    # allow alt naming\n",
    "    if \"shap_mean_abs\" not in df.columns and \"importance\" in df.columns:\n",
    "        df = df.rename(columns={\"importance\": \"shap_mean_abs\"})\n",
    "    # keep minimum set\n",
    "    need = {\"feature\", \"subspace\", \"shap_mean_abs\"}\n",
    "    if not need.issubset(df.columns):\n",
    "        return pd.DataFrame()\n",
    "    df[\"feature\"] = df[\"feature\"].astype(str).str.strip()\n",
    "    df[\"subspace\"] = df[\"subspace\"].astype(str).str.lower()\n",
    "    df[\"shap_mean_abs\"] = pd.to_numeric(df[\"shap_mean_abs\"], errors=\"coerce\").fillna(0.0)\n",
    "    return df[[\"feature\", \"subspace\", \"shap_mean_abs\"]].copy()\n",
    "\n",
    "# ------------------------ Aggregation helpers ------------------------\n",
    "def _percentile_rank(series: pd.Series) -> pd.Series:\n",
    "    n = len(series)\n",
    "    if n <= 1:\n",
    "        return pd.Series(np.zeros(n), index=series.index, dtype=float)\n",
    "    r = series.rank(method=\"average\", ascending=True)\n",
    "    return (r - 1) / (n - 1)\n",
    "\n",
    "def _aggregate_shap_platform(details_rows: pd.DataFrame, setup: str, subspace: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    details_rows: rows from BEST_in_DesignSpace_Post_per_platform_details.csv for this setup\n",
    "    Returns: DataFrame(feature, shap_mean_abs_platform) for one subspace\n",
    "    \"\"\"\n",
    "    parts: List[pd.DataFrame] = []\n",
    "    for _, r in details_rows.iterrows():\n",
    "        anomaly = str(r[\"anomaly\"])\n",
    "        win     = int(r[\"win\"])\n",
    "        kfold   = int(r[\"kfold\"])\n",
    "        pct     = int(r[\"best_pct_by_median\"])\n",
    "        method  = str(r[\"method\"])\n",
    "\n",
    "        df_shap = _load_shap_bestplat_full(setup, anomaly, win, kfold, pct, method)\n",
    "        if df_shap.empty:\n",
    "            continue\n",
    "\n",
    "        sub_df = df_shap[df_shap[\"subspace\"] == subspace][[\"feature\", \"shap_mean_abs\"]].copy()\n",
    "        sub_df = sub_df.rename(columns={\"shap_mean_abs\": f\"shap_{anomaly}\"})\n",
    "        parts.append(sub_df)\n",
    "\n",
    "    if not parts:\n",
    "        return pd.DataFrame(columns=[\"feature\", \"shap_mean_abs_platform\"])\n",
    "\n",
    "    merged = None\n",
    "    for d in parts:\n",
    "        if merged is None:\n",
    "            merged = d.copy()\n",
    "        else:\n",
    "            merged = merged.merge(d, on=\"feature\", how=\"outer\")\n",
    "\n",
    "    shap_cols = [c for c in merged.columns if c.startswith(\"shap_\")]\n",
    "    merged[shap_cols] = merged[shap_cols].fillna(0.0)\n",
    "\n",
    "    # equal-weight mean across anomalies\n",
    "    merged[\"shap_mean_abs_platform\"] = merged[shap_cols].mean(axis=1)\n",
    "\n",
    "    out = merged[[\"feature\", \"shap_mean_abs_platform\"]].copy()\n",
    "    out[\"feature\"] = out[\"feature\"].astype(str)\n",
    "    out[\"shap_mean_abs_platform\"] = pd.to_numeric(out[\"shap_mean_abs_platform\"], errors=\"coerce\").fillna(0.0)\n",
    "    return out\n",
    "\n",
    "# ------------------------ Comparison ------------------------\n",
    "def _compare_subspace(cpmi_df: pd.DataFrame, shap_df: pd.DataFrame) -> Optional[dict]:\n",
    "    \"\"\"\n",
    "    cpmi_df: feature, cpmi_score, cpmi_rank\n",
    "    shap_df: feature, shap_mean_abs_platform\n",
    "    \"\"\"\n",
    "    if cpmi_df.empty or shap_df.empty:\n",
    "        return None\n",
    "\n",
    "    m = pd.merge(\n",
    "        cpmi_df[[\"feature\", \"cpmi_score\", \"cpmi_rank\"]],\n",
    "        shap_df[[\"feature\", \"shap_mean_abs_platform\"]],\n",
    "        on=\"feature\", how=\"inner\"\n",
    "    )\n",
    "    if m.empty:\n",
    "        return None\n",
    "\n",
    "    m = m.copy()\n",
    "    m[\"shap_rank\"] = m[\"shap_mean_abs_platform\"].rank(ascending=False, method=\"average\").astype(float)\n",
    "\n",
    "    if NORMALIZE_BY_PERCENTILE:\n",
    "        m[\"cpmi_prank\"] = _percentile_rank(-m[\"cpmi_rank\"])\n",
    "        m[\"shap_prank\"] = _percentile_rank(-m[\"shap_rank\"])\n",
    "    else:\n",
    "        m[\"cpmi_prank\"] = np.nan\n",
    "        m[\"shap_prank\"] = np.nan\n",
    "\n",
    "    rho_rank,  p_rank  = spearmanr(-m[\"cpmi_rank\"], -m[\"shap_rank\"])\n",
    "    rho_prank, p_prank = (np.nan, np.nan)\n",
    "    if NORMALIZE_BY_PERCENTILE:\n",
    "        rho_prank, p_prank = spearmanr(m[\"cpmi_prank\"], m[\"shap_prank\"])\n",
    "    rho_score, p_score = spearmanr(m[\"cpmi_score\"], m[\"shap_mean_abs_platform\"])\n",
    "\n",
    "    rows = []\n",
    "    n = len(m)\n",
    "    for pct in TOP_PCTS:\n",
    "        k = max(1, math.floor(pct * n / 100))\n",
    "        top_c = set(m.sort_values(\"cpmi_rank\").head(k)[\"feature\"])\n",
    "        top_s = set(m.sort_values(\"shap_rank\").head(k)[\"feature\"])\n",
    "        inter = len(top_c & top_s)\n",
    "        union = len(top_c | top_s) if (top_c or top_s) else 1\n",
    "        rows.append({\"k\": f\"top{pct}%\", \"overlap\": inter, \"jaccard\": inter / union, \"n_aligned\": n})\n",
    "\n",
    "    return {\n",
    "        \"aligned\": m,\n",
    "        \"rho_rank\": rho_rank, \"p_rank\": p_rank,\n",
    "        \"rho_prank\": rho_prank, \"p_prank\": p_prank,\n",
    "        \"rho_score\": rho_score, \"p_score\": p_score,\n",
    "        \"summary\": pd.DataFrame(rows)\n",
    "    }\n",
    "\n",
    "# ------------------------ Plotting ------------------------\n",
    "def _plot_platform(setup: str, subspace: str, comp: dict):\n",
    "    aligned = comp[\"aligned\"]\n",
    "    case_tag = f\"{setup}_{subspace}\"\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(aligned[\"cpmi_score\"], aligned[\"shap_mean_abs_platform\"], s=12)\n",
    "    plt.xlabel(\"CP-MI score\")\n",
    "    plt.ylabel(\"SHAP importance (mean |SHAP|) • platform-avg\")\n",
    "    plt.title(f\"{setup} • {subspace} • CP-MI vs SHAP (ρ={comp['rho_score']:.2f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{case_tag}_scatter_cpmi_vs_shap.png\", dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "    if \"cpmi_prank\" in aligned.columns and \"shap_prank\" in aligned.columns:\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.scatter(aligned[\"cpmi_prank\"], aligned[\"shap_prank\"], s=12)\n",
    "        plt.xlabel(\"CP-MI percentile rank (higher = more important)\")\n",
    "        plt.ylabel(\"SHAP percentile rank (higher = more important)\")\n",
    "        plt.title(f\"{setup} • {subspace} • Percentile Concordance (ρ={comp['rho_prank']:.2f})\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIG_DIR / f\"{case_tag}_scatter_percentile_ranks.png\", dpi=220)\n",
    "        plt.close()\n",
    "\n",
    "    s_pct = comp[\"summary\"].copy()\n",
    "    s_pct[\"pct\"] = s_pct[\"k\"].str.extract(r\"top(\\d+)%\").astype(int)\n",
    "    s_pct = s_pct.sort_values(\"pct\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(s_pct[\"pct\"], s_pct[\"jaccard\"], marker=\"o\")\n",
    "    plt.xlabel(\"Top-% of features\")\n",
    "    plt.ylabel(\"Jaccard (CP-MI vs SHAP)\")\n",
    "    plt.title(f\"{setup} • {subspace} • Agreement vs Percentage\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{case_tag}_jaccard_vs_pct.png\", dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(s_pct[\"pct\"].astype(str), s_pct[\"overlap\"])\n",
    "    plt.xlabel(\"Top-% of features\")\n",
    "    plt.ylabel(\"Overlap count\")\n",
    "    plt.title(f\"{setup} • {subspace} • Overlap (CP-MI ∩ SHAP) vs %\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{case_tag}_overlap_bar_pct.png\", dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "# ====================== Main ======================\n",
    "details = pd.read_csv(DETAILS_CSV).copy()\n",
    "\n",
    "# Normalize column naming\n",
    "if \"best_method\" in details.columns and \"method\" not in details.columns:\n",
    "    details = details.rename(columns={\"best_method\":\"method\"})\n",
    "if \"best_pct_by_median\" not in details.columns and \"pct\" in details.columns:\n",
    "    details = details.rename(columns={\"pct\":\"best_pct_by_median\"})\n",
    "\n",
    "need_det = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"}\n",
    "missing = need_det - set(details.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"{DETAILS_CSV} missing columns {sorted(missing)}. Have: {list(details.columns)}\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for setup in [\"DDR4\",\"DDR5\"]:\n",
    "    dsetup = details[details[\"setup\"].astype(str).str.upper() == setup].copy()\n",
    "    if dsetup.empty:\n",
    "        print(f\"[WARN] No details rows for {setup}\")\n",
    "        continue\n",
    "\n",
    "    # Use platform's WIN/K from the first row (they should be identical across anomalies for that setup)\n",
    "    win_best = int(dsetup[\"win\"].iloc[0])\n",
    "    kf_best  = int(dsetup[\"kfold\"].iloc[0])\n",
    "\n",
    "    print(f\"\\n[PLATFORM] {setup} using WIN={win_best} K={kf_best}\")\n",
    "\n",
    "    cpmi = _load_cpmi(setup, win_best, kf_best)\n",
    "\n",
    "    for sub in SUBSPACES:\n",
    "        shap_plat = _aggregate_shap_platform(dsetup, setup=setup, subspace=sub)\n",
    "        comp = _compare_subspace(cpmi.get(sub, pd.DataFrame()), shap_plat)\n",
    "\n",
    "        if comp is None:\n",
    "            rows.append({\n",
    "                \"setup\": setup, \"win\": win_best, \"kfold\": kf_best,\n",
    "                \"subspace\": sub, \"k\": \"NA\",\n",
    "                \"jaccard\": np.nan, \"overlap\": np.nan,\n",
    "                \"rho_rank\": np.nan, \"rho_prank\": np.nan, \"rho_score\": np.nan,\n",
    "                \"aligned_features\": 0\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        _plot_platform(setup, sub, comp)\n",
    "\n",
    "        s = comp[\"summary\"].copy()\n",
    "        s[\"pct\"] = s[\"k\"].str.extract(r\"top(\\d+)%\").astype(int)\n",
    "        s[\"k_eff\"] = (np.floor(s[\"pct\"] * s[\"n_aligned\"] / 100)).astype(int).clip(lower=1)\n",
    "        s = s.sort_values(\"pct\").drop_duplicates(subset=[\"k_eff\"], keep=\"first\")\n",
    "        take = s[s[\"pct\"].isin(TAKE_PCTS)].copy()\n",
    "\n",
    "        for _, rr in take.iterrows():\n",
    "            rows.append({\n",
    "                \"setup\": setup, \"win\": win_best, \"kfold\": kf_best,\n",
    "                \"subspace\": sub, \"k\": rr[\"k\"],\n",
    "                \"jaccard\": rr[\"jaccard\"], \"overlap\": rr[\"overlap\"],\n",
    "                \"rho_rank\": comp[\"rho_rank\"], \"rho_prank\": comp[\"rho_prank\"], \"rho_score\": comp[\"rho_score\"],\n",
    "                \"aligned_features\": int(comp[\"aligned\"].shape[0])\n",
    "            })\n",
    "\n",
    "out_csv = OUT_DIR / \"SHAP_vs_CPMI_summary_PLATFORM.csv\"\n",
    "pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "print(f\"\\n[OK] Summary → {out_csv}\")\n",
    "print(f\"[OK] Plots   → {FIG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "104a5cf3-b5b1-4289-8268-c1c6b9514158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Using summary: /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_vs_CPMI_summary_PLATFORM.csv\n",
      "[OK] Aggregates → /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_vs_CPMI_aggregate_corr_by_subspace_PLATFORM.csv , /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_vs_CPMI_aggregate_corr_by_platform_subspace.csv , /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_vs_CPMI_aggregate_jaccard_by_subspace_k_PLATFORM.csv , /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_vs_CPMI_aggregate_jaccard_by_subspace_pivot_PLATFORM.csv , /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_vs_CPMI_aggregate_jaccard_by_platform_subspace_k.csv , /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_vs_CPMI_aggregate_jaccard_by_platform_subspace_pivot.csv\n"
     ]
    }
   ],
   "source": [
    "# === Aggregates: subspace-level & PLATFORM×subspace (percentages-only run) ===\n",
    "# UPDATED for PER-PLATFORM workflow.\n",
    "#\n",
    "# Reads:\n",
    "#   Results/Explainability_SHAP_BestPlatforms/SHAP_vs_CPMI_summary_PLATFORM.csv\n",
    "#\n",
    "# Writes:\n",
    "#   Results/Explainability_SHAP_BestPlatforms/\n",
    "#     - SHAP_vs_CPMI_aggregate_corr_by_subspace_PLATFORM.csv\n",
    "#     - SHAP_vs_CPMI_aggregate_corr_by_platform_subspace.csv\n",
    "#     - SHAP_vs_CPMI_aggregate_jaccard_by_subspace_k_PLATFORM.csv\n",
    "#     - SHAP_vs_CPMI_aggregate_jaccard_by_subspace_pivot_PLATFORM.csv\n",
    "#     - SHAP_vs_CPMI_aggregate_jaccard_by_platform_subspace_k.csv\n",
    "#     - SHAP_vs_CPMI_aggregate_jaccard_by_platform_subspace_pivot.csv\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "\n",
    "BASE_RES = ROOT / \"Results\"\n",
    "OUT_DIR  = BASE_RES / \"Explainability_SHAP_BestPlatforms\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _find_summary_csv() -> Path:\n",
    "    fname = \"SHAP_vs_CPMI_summary_PLATFORM.csv\"\n",
    "    candidates = [\n",
    "        OUT_DIR / fname,\n",
    "        BASE_RES / fname,\n",
    "        # if someone nested folders accidentally\n",
    "        OUT_DIR / \"Explainability_SHAP_BestPlatforms\" / fname,\n",
    "        BASE_RES / \"Explainability_SHAP_BestPlatforms\" / fname,\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    hits = list(BASE_RES.rglob(fname))\n",
    "    if hits:\n",
    "        return hits[0]\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find {fname}. Tried:\\n  - \" +\n",
    "        \"\\n  - \".join(str(x) for x in candidates) +\n",
    "        f\"\\nAlso searched under: {BASE_RES}\"\n",
    "    )\n",
    "\n",
    "summary_path = _find_summary_csv()\n",
    "print(\"[OK] Using summary:\", summary_path)\n",
    "\n",
    "df = pd.read_csv(summary_path)\n",
    "\n",
    "# Expected per-platform summary schema:\n",
    "#   setup, win, kfold, subspace, k, jaccard, overlap, rho_rank, rho_prank, rho_score, aligned_features\n",
    "required = {\"setup\",\"win\",\"kfold\",\"subspace\",\"k\",\"jaccard\",\"overlap\",\"rho_rank\",\"rho_prank\",\"rho_score\",\"aligned_features\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"Summary missing required columns: {sorted(missing)}. Have: {list(df.columns)}\")\n",
    "\n",
    "# Keys for a unique \"platform case\"\n",
    "case_keys = [\"setup\",\"win\",\"kfold\",\"subspace\"]\n",
    "\n",
    "# --- Correlations aggregated per subspace (one row per platform/subspace) ---\n",
    "corr_cols = [\"rho_rank\",\"rho_prank\",\"rho_score\",\"aligned_features\"]\n",
    "\n",
    "corr_per_case = df[case_keys + corr_cols].drop_duplicates(case_keys).reset_index(drop=True)\n",
    "\n",
    "# n_cases per subspace (across platforms; typically 2 platforms => n_cases=2)\n",
    "n_cases = (\n",
    "    corr_per_case.groupby(\"subspace\")\n",
    "    .size()\n",
    "    .rename(\"n_cases\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg_corr_sub = (\n",
    "    corr_per_case.groupby(\"subspace\")[corr_cols]\n",
    "    .median(numeric_only=True)\n",
    "    .reset_index()\n",
    ")\n",
    "agg_corr_sub = agg_corr_sub.merge(n_cases, on=\"subspace\", how=\"left\")\n",
    "agg_corr_sub.to_csv(OUT_DIR / \"SHAP_vs_CPMI_aggregate_corr_by_subspace_PLATFORM.csv\", index=False)\n",
    "\n",
    "# Correlations per PLATFORM × subspace\n",
    "agg_corr_plat_sub = (\n",
    "    corr_per_case.groupby([\"setup\",\"subspace\"])[corr_cols]\n",
    "    .median(numeric_only=True)\n",
    "    .reset_index()\n",
    ")\n",
    "agg_corr_plat_sub.to_csv(OUT_DIR / \"SHAP_vs_CPMI_aggregate_corr_by_platform_subspace.csv\", index=False)\n",
    "\n",
    "# --- Jaccard aggregated per subspace at each percentage threshold ---\n",
    "jacc_sub_k = (\n",
    "    df.groupby([\"subspace\",\"k\"])[\"jaccard\"]\n",
    "      .median()\n",
    "      .reset_index()\n",
    "      .sort_values([\"subspace\",\"k\"])\n",
    ")\n",
    "jacc_sub_k.to_csv(OUT_DIR / \"SHAP_vs_CPMI_aggregate_jaccard_by_subspace_k_PLATFORM.csv\", index=False)\n",
    "\n",
    "# Pivot for a compact table: subspace × selected thresholds\n",
    "take_pcts = [\"top10%\",\"top25%\",\"top50%\",\"top100%\"]\n",
    "available = [c for c in take_pcts if c in set(jacc_sub_k[\"k\"].astype(str).unique())]\n",
    "\n",
    "jacc_pivot = (\n",
    "    jacc_sub_k.pivot(index=\"subspace\", columns=\"k\", values=\"jaccard\")\n",
    "      .reindex(columns=available)\n",
    "      .reset_index()\n",
    ")\n",
    "jacc_pivot.to_csv(OUT_DIR / \"SHAP_vs_CPMI_aggregate_jaccard_by_subspace_pivot_PLATFORM.csv\", index=False)\n",
    "\n",
    "# --- Jaccard per PLATFORM × subspace at each percentage threshold ---\n",
    "jacc_plat_sub_k = (\n",
    "    df.groupby([\"setup\",\"subspace\",\"k\"])[\"jaccard\"]\n",
    "      .median()\n",
    "      .reset_index()\n",
    "      .sort_values([\"setup\",\"subspace\",\"k\"])\n",
    ")\n",
    "jacc_plat_sub_k.to_csv(OUT_DIR / \"SHAP_vs_CPMI_aggregate_jaccard_by_platform_subspace_k.csv\", index=False)\n",
    "\n",
    "available2 = [c for c in take_pcts if c in set(jacc_plat_sub_k[\"k\"].astype(str).unique())]\n",
    "jacc_plat_sub_pivot = (\n",
    "    jacc_plat_sub_k.pivot(index=[\"setup\",\"subspace\"], columns=\"k\", values=\"jaccard\")\n",
    "      .reindex(columns=available2)\n",
    "      .reset_index()\n",
    ")\n",
    "jacc_plat_sub_pivot.to_csv(OUT_DIR / \"SHAP_vs_CPMI_aggregate_jaccard_by_platform_subspace_pivot.csv\", index=False)\n",
    "\n",
    "print(\"[OK] Aggregates →\",\n",
    "      OUT_DIR / \"SHAP_vs_CPMI_aggregate_corr_by_subspace_PLATFORM.csv\", \",\",\n",
    "      OUT_DIR / \"SHAP_vs_CPMI_aggregate_corr_by_platform_subspace.csv\", \",\",\n",
    "      OUT_DIR / \"SHAP_vs_CPMI_aggregate_jaccard_by_subspace_k_PLATFORM.csv\", \",\",\n",
    "      OUT_DIR / \"SHAP_vs_CPMI_aggregate_jaccard_by_subspace_pivot_PLATFORM.csv\", \",\",\n",
    "      OUT_DIR / \"SHAP_vs_CPMI_aggregate_jaccard_by_platform_subspace_k.csv\", \",\",\n",
    "      OUT_DIR / \"SHAP_vs_CPMI_aggregate_jaccard_by_platform_subspace_pivot.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35065237-b189-49be-a324-4c31f7184afe",
   "metadata": {},
   "source": [
    "## Consistency bars (CP-MI selection supported by SHAP, BEST cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52eba978-4ece-498e-bd1f-bf1f095f9aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Plot saved: /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/consistency_v2_platform/CONSISTENCY_BARS_PLATFORM_DDR4_WIN512_KF3_PLOT.png\n",
      "\n",
      "[PLATFORM CHECK] DDR4  WIN=512  K=3\n",
      "  anomalies: DROOP, RH\n",
      "  compute: SHAP-low=16  SHAP-high=10  total=26\n",
      "  memory: SHAP-low=2  SHAP-high=6  total=8\n",
      "  sensors: SHAP-low=0  SHAP-high=4  total=4\n",
      "[OK] Plot saved: /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/consistency_v2_platform/CONSISTENCY_BARS_PLATFORM_DDR5_WIN1024_KF5_PLOT.png\n",
      "\n",
      "[PLATFORM CHECK] DDR5  WIN=1024  K=5\n",
      "  anomalies: DROOP, SPECTRE\n",
      "  compute: SHAP-low=52  SHAP-high=44  total=96\n",
      "  memory: SHAP-low=12  SHAP-high=21  total=33\n",
      "  sensors: SHAP-low=5  SHAP-high=6  total=11\n",
      "\n",
      "[OK] Counts CSV → /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/consistency_v2_platform/CONSISTENCY_BARS_PLATFORM_counts.csv\n"
     ]
    }
   ],
   "source": [
    "# ==== Cell: Consistency bars (CP-MI selection supported by SHAP) — PER PLATFORM ====\n",
    "# PER-PLATFORM revision of your \"BEST cases\" script.\n",
    "#\n",
    "# Reads platform winners from:\n",
    "#   Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#\n",
    "# Uses SHAP outputs from:\n",
    "#   Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN<w>_KF<k>_PCT<p>_M<method>.csv\n",
    "#\n",
    "# CP-MI rank files under FeatureRankOUT:\n",
    "#   <setup>_<win>_<kfold>_0_<subspace>.csv  (preferred)\n",
    "#\n",
    "# Produces:\n",
    "#   - ONE plot per PLATFORM (DDR4, DDR5):\n",
    "#       Results/Explainability_SHAP_BestPlatforms/consistency_v2_platform/\n",
    "#         CONSISTENCY_BARS_PLATFORM_<setup>_WIN<w>_KF<k>_PLOT.png\n",
    "#   - Also writes platform-level counts CSV:\n",
    "#       CONSISTENCY_BARS_PLATFORM_counts.csv\n",
    "#\n",
    "# How it works (platform-level):\n",
    "#   1) For a platform, choose (WIN,KF) from the details file (consistent across anomalies).\n",
    "#   2) CP-MI selection:\n",
    "#        - For each anomaly, select top-<pct>% CP-MI features per subspace (from CP-MI rank lists)\n",
    "#        - Union these anomaly selections (so CP-MI selection reflects the platform's anomaly mix)\n",
    "#   3) SHAP support:\n",
    "#        - Load SHAP tables for each anomaly at its best config (method+pct+WIN+KF)\n",
    "#        - For each anomaly, split features into SHAP-low vs SHAP-high by median SHAP\n",
    "#        - Count how many CP-MI-selected features fall into SHAP-high vs SHAP-low, per subspace\n",
    "#        - Sum counts across anomalies (platform totals)\n",
    "#\n",
    "# Note:\n",
    "#   - This is a COUNT-based view (like your original). It does NOT weight by SHAP magnitude.\n",
    "# -------------------------------------------------------------------------------\n",
    "\n",
    "import os, re, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Paths (reuse if already defined in your notebook) -----------------------\n",
    "ROOT     = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES_DIR  = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "\n",
    "DETAILS_CSV = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "EXPL_DIR    = RES_DIR / \"Explainability_SHAP_BestPlatforms\"  # SHAP_BESTPLAT_full_*.csv live here\n",
    "CONS_DIR    = EXPL_DIR / \"consistency_v2_platform\"\n",
    "CONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CP-MI rank roots (use global RANK_DIRS if already set)\n",
    "if \"RANK_DIRS\" in globals():\n",
    "    RANK_DIRS = list(globals()[\"RANK_DIRS\"])\n",
    "else:\n",
    "    RANK_DIRS = [\n",
    "        ROOT / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"octaneX\" / \"FeatureRankOUT\",\n",
    "        Path.home() / \"Desktop\" / \"octaneX\" / \"FeatureRankOUT\",\n",
    "    ]\n",
    "\n",
    "SUBSPACES = (\"compute\",\"memory\",\"sensors\")\n",
    "\n",
    "# --- Helpers -----------------------------------------------------------------\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"Normalize feature names to increase intersection chances.\"\"\"\n",
    "    s = re.sub(r\"\\s+\", \"_\", str(s)).lower()\n",
    "    s = s.replace(\"%\", \"pct\")\n",
    "    s = re.sub(r\"[^a-z0-9_]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def _read_csv_safe(p: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def _read_cpmi_list(setup: str, win: int, kfold: int, sub: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Load CP-MI FeatureRankOUT list for one subspace. Returns ordered original names.\n",
    "    Expected filename: <setup>_<win>_<kfold>_0_<sub>.csv\n",
    "    Falls back to *CPMI*/*CP-MI* patterns.\n",
    "    \"\"\"\n",
    "    fname = f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "    # 1) direct hits\n",
    "    for rd in RANK_DIRS:\n",
    "        p = Path(rd) / fname\n",
    "        if p.exists():\n",
    "            df = _read_csv_safe(p)\n",
    "            if df.empty:\n",
    "                continue\n",
    "            col = \"feature\" if \"feature\" in df.columns else df.columns[0]\n",
    "            return df[col].dropna().astype(str).tolist()\n",
    "    # 2) fallback search\n",
    "    for rd in RANK_DIRS:\n",
    "        rd = Path(rd)\n",
    "        if not rd.exists():\n",
    "            continue\n",
    "        hits = list(rd.rglob(f\"*{setup}*{sub}*CPMI*.csv\")) + list(rd.rglob(f\"*{setup}*{sub}*CP-MI*.csv\"))\n",
    "        if hits:\n",
    "            df = _read_csv_safe(hits[0])\n",
    "            if df.empty:\n",
    "                continue\n",
    "            col = \"feature\" if \"feature\" in df.columns else df.columns[0]\n",
    "            return df[col].dropna().astype(str).tolist()\n",
    "    return []\n",
    "\n",
    "def _cpmi_selection_at_pct(setup: str, win: int, kfold: int, pct: int) -> dict:\n",
    "    \"\"\"\n",
    "    Return dict{subspace: [features]}: top-<pct>% per subspace (ceil, min 1 if pct>0).\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for sub in SUBSPACES:\n",
    "        feats = _read_cpmi_list(setup, win, kfold, sub)\n",
    "        if feats:\n",
    "            k = int(np.ceil(len(feats) * (pct / 100.0)))\n",
    "            if pct > 0 and k == 0:\n",
    "                k = 1\n",
    "            out[sub] = feats[:k]\n",
    "        else:\n",
    "            out[sub] = []\n",
    "    return out\n",
    "\n",
    "def _find_shap_bestplat_full(setup: str, anomaly: str, win: int, kfold: int, pct: int, method: str) -> Path | None:\n",
    "    \"\"\"\n",
    "    Prefer exact BEST file:\n",
    "      SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN<w>_KF<k>_PCT<p>_M<method>.csv\n",
    "    Fallback: any PCT for the same (setup, anomaly, win, kfold, method).\n",
    "    \"\"\"\n",
    "    exact = EXPL_DIR / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT{pct}_M{method}.csv\"\n",
    "    if exact.exists():\n",
    "        return exact\n",
    "    pat = str(EXPL_DIR / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT*_M{method}.csv\")\n",
    "    alts = sorted(glob.glob(pat))\n",
    "    return Path(alts[0]) if alts else None\n",
    "\n",
    "def _split_high_low(shap_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Binary split by median SHAP so both sides are non-empty when >1 feature.\n",
    "    Returns (low_features, high_features) as pd.Series of feature strings.\n",
    "    \"\"\"\n",
    "    if shap_df.empty:\n",
    "        return pd.Series([], dtype=str), pd.Series([], dtype=str)\n",
    "    med = shap_df[\"shap_mean_abs\"].median()\n",
    "    high_idx = shap_df[shap_df[\"shap_mean_abs\"] >= med][\"feature\"].astype(str)\n",
    "    low_idx  = shap_df[shap_df[\"shap_mean_abs\"]  < med][\"feature\"].astype(str)\n",
    "    return low_idx, high_idx\n",
    "\n",
    "def _debug_platform(setup, win, kfold, detail_rows, by_sub_counts):\n",
    "    print(f\"\\n[PLATFORM CHECK] {setup}  WIN={win}  K={kfold}\")\n",
    "    print(\"  anomalies:\", \", \".join(sorted(detail_rows[\"anomaly\"].astype(str).unique().tolist())))\n",
    "    for sub in SUBSPACES:\n",
    "        lo = by_sub_counts[sub][\"low\"]\n",
    "        hi = by_sub_counts[sub][\"high\"]\n",
    "        tot = lo + hi\n",
    "        print(f\"  {sub}: SHAP-low={lo}  SHAP-high={hi}  total={tot}\")\n",
    "\n",
    "# --- Main --------------------------------------------------------------------\n",
    "def plot_cpmi_supported_by_shap_per_platform():\n",
    "    if not DETAILS_CSV.exists():\n",
    "        raise FileNotFoundError(f\"Missing platform details CSV: {DETAILS_CSV}\")\n",
    "\n",
    "    details = pd.read_csv(DETAILS_CSV).copy()\n",
    "    details.columns = [c.lower() for c in details.columns]\n",
    "\n",
    "    # normalize column names\n",
    "    if \"best_method\" in details.columns and \"method\" not in details.columns:\n",
    "        details = details.rename(columns={\"best_method\":\"method\"})\n",
    "    if \"best_pct_by_median\" not in details.columns and \"pct\" in details.columns:\n",
    "        details = details.rename(columns={\"pct\":\"best_pct_by_median\"})\n",
    "\n",
    "    need = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"}\n",
    "    missing = need - set(details.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Platform details CSV missing columns: {sorted(missing)}. Have: {list(details.columns)}\")\n",
    "\n",
    "    out_rows = []\n",
    "\n",
    "    for setup in [\"DDR4\",\"DDR5\"]:\n",
    "        dplat = details[details[\"setup\"].astype(str).str.upper() == setup].copy()\n",
    "        if dplat.empty:\n",
    "            print(f\"[WARN] No detail rows for platform={setup}\")\n",
    "            continue\n",
    "\n",
    "        # WIN/K should be consistent across anomalies for the platform\n",
    "        win = int(pd.to_numeric(dplat[\"win\"], errors=\"coerce\").dropna().iloc[0])\n",
    "        kfold = int(pd.to_numeric(dplat[\"kfold\"], errors=\"coerce\").dropna().iloc[0])\n",
    "\n",
    "        # Platform aggregate counts\n",
    "        by_sub = {sub: {\"low\": 0, \"high\": 0} for sub in SUBSPACES}\n",
    "\n",
    "        # --- Loop anomalies in this platform ---\n",
    "        for _, r in dplat.iterrows():\n",
    "            anomaly = str(r[\"anomaly\"]).strip()\n",
    "            pct     = int(pd.to_numeric(r[\"best_pct_by_median\"], errors=\"coerce\"))\n",
    "            method  = str(r[\"method\"]).strip()\n",
    "\n",
    "            # 1) CP-MI selection for THIS anomaly at its best pct (based on CP-MI lists at platform WIN/K)\n",
    "            sel = _cpmi_selection_at_pct(setup, win, kfold, pct)\n",
    "\n",
    "            # Build per-subspace normalized selection sets (for quick membership)\n",
    "            sel_norm = {sub: set(_norm(f) for f in sel[sub]) for sub in SUBSPACES}\n",
    "\n",
    "            # 2) Load SHAP best full for THIS anomaly\n",
    "            shap_csv = _find_shap_bestplat_full(setup, anomaly, win, kfold, pct, method)\n",
    "            shap_df = pd.DataFrame()\n",
    "            if shap_csv is not None and shap_csv.exists():\n",
    "                shap_df = _read_csv_safe(shap_csv)\n",
    "                if not shap_df.empty:\n",
    "                    shap_df.columns = [c.lower() for c in shap_df.columns]\n",
    "                    if \"feature\" not in shap_df.columns:\n",
    "                        shap_df = shap_df.rename(columns={shap_df.columns[0]: \"feature\"})\n",
    "                    if \"shap_mean_abs\" not in shap_df.columns and \"importance\" in shap_df.columns:\n",
    "                        shap_df = shap_df.rename(columns={\"importance\": \"shap_mean_abs\"})\n",
    "                    if \"subspace\" not in shap_df.columns:\n",
    "                        # if missing, we can't do subspace-wise counts reliably\n",
    "                        shap_df[\"subspace\"] = \"compute\"\n",
    "                    shap_df[\"feature\"] = shap_df[\"feature\"].astype(str)\n",
    "                    shap_df[\"subspace\"] = shap_df[\"subspace\"].astype(str).str.lower()\n",
    "                    shap_df[\"shap_mean_abs\"] = pd.to_numeric(shap_df[\"shap_mean_abs\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "            # If missing SHAP, skip this anomaly gracefully\n",
    "            if shap_df.empty:\n",
    "                print(f\"[WARN] Missing/empty SHAP for {setup}/{anomaly} (WIN={win} K={kfold} PCT={pct} M={method})\")\n",
    "                continue\n",
    "\n",
    "            # 3) SHAP split (low/high by median)\n",
    "            low_f, high_f = _split_high_low(shap_df)\n",
    "            low_norm  = set(low_f.map(_norm))\n",
    "            high_norm = set(high_f.map(_norm))\n",
    "\n",
    "            # 4) Count CP-MI-selected features that fall into SHAP-low vs SHAP-high, per subspace\n",
    "            #    We do membership by normalized name (robust)\n",
    "            for sub in SUBSPACES:\n",
    "                if not sel_norm[sub]:\n",
    "                    continue\n",
    "                # A feature is \"supported by SHAP-high\" if it appears in high_norm\n",
    "                hi = len(sel_norm[sub] & high_norm)\n",
    "                # Count \"low\" as those selected but not in high; we can intersect with low_norm explicitly\n",
    "                lo = len(sel_norm[sub] & low_norm)\n",
    "                # If some selected features are in neither (e.g., not present in SHAP file), ignore them\n",
    "                by_sub[sub][\"high\"] += hi\n",
    "                by_sub[sub][\"low\"]  += lo\n",
    "\n",
    "        # --- Plot ONE grouped-bar figure per platform ---\n",
    "        cats  = list(SUBSPACES)\n",
    "        lows  = [by_sub[s][\"low\"]  for s in cats]\n",
    "        highs = [by_sub[s][\"high\"] for s in cats]\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        x = np.arange(len(cats)); w = 0.35\n",
    "        plt.bar(x - w/2, lows,  width=w, label=\"SHAP-low\")\n",
    "        plt.bar(x + w/2, highs, width=w, label=\"SHAP-high\")\n",
    "        plt.xticks(x, cats)\n",
    "        plt.ylabel(\"Count of CP-MI-selected features\")\n",
    "        plt.title(f\"{setup} • WIN={win} K={kfold}\\nCP-MI selection supported by SHAP (summed across anomalies)\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        outp = CONS_DIR / f\"CONSISTENCY_BARS_PLATFORM_{setup}_WIN{win}_KF{kfold}_PLOT.png\"\n",
    "        fig.savefig(outp, dpi=220)\n",
    "        plt.close(fig)\n",
    "        print(f\"[OK] Plot saved: {outp}\")\n",
    "\n",
    "        # record counts for CSV\n",
    "        for sub in SUBSPACES:\n",
    "            out_rows.append({\n",
    "                \"setup\": setup, \"win\": win, \"kfold\": kfold,\n",
    "                \"subspace\": sub,\n",
    "                \"count_shap_low\": int(by_sub[sub][\"low\"]),\n",
    "                \"count_shap_high\": int(by_sub[sub][\"high\"]),\n",
    "                \"count_total_counted\": int(by_sub[sub][\"low\"] + by_sub[sub][\"high\"]),\n",
    "                \"note\": \"counts are summed across platform anomalies; only features present in SHAP file contribute\",\n",
    "            })\n",
    "\n",
    "        _debug_platform(setup, win, kfold, dplat, by_sub)\n",
    "\n",
    "    # Save platform-level counts\n",
    "    if out_rows:\n",
    "        out_df = pd.DataFrame(out_rows)\n",
    "        out_csv = CONS_DIR / \"CONSISTENCY_BARS_PLATFORM_counts.csv\"\n",
    "        out_df.to_csv(out_csv, index=False)\n",
    "        print(f\"\\n[OK] Counts CSV → {out_csv}\")\n",
    "    else:\n",
    "        print(\"\\n[WARN] No platform consistency counts produced (check SHAP + details CSV).\")\n",
    "\n",
    "# Run\n",
    "plot_cpmi_supported_by_shap_per_platform()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303fb726-4755-48d0-a2ba-de7ca591aeb4",
   "metadata": {},
   "source": [
    "# OPTIONAL hybrid promoter (CP-MI baseline + SHAP boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13e1aa0f-1dc9-4cd4-9b1f-d24ce8b343f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PLATFORM] DDR4: discovering CP-MI grid ...\n",
      "[INFO] DDR4: found 24 (WIN,KF) combos from CP-MI ranks.\n",
      "[OK] Joined saved → PLATFORM_DDR4_WIN512_KF3__SHAP_vs_CPMI_joined.csv\n",
      "[OK] Hybrid written → /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT_HYBRID/DDR4_512_3_0_compute.csv\n",
      "[OK] Hybrid written → /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT_HYBRID/DDR4_512_3_0_memory.csv\n",
      "[OK] Hybrid written → /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT_HYBRID/DDR4_512_3_0_sensors.csv\n",
      "\n",
      "[PLATFORM] DDR5: discovering CP-MI grid ...\n",
      "[INFO] DDR5: found 24 (WIN,KF) combos from CP-MI ranks.\n",
      "[OK] Joined saved → PLATFORM_DDR5_WIN1024_KF5__SHAP_vs_CPMI_joined.csv\n",
      "[OK] Hybrid written → /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT_HYBRID/DDR5_1024_5_0_compute.csv\n",
      "[OK] Hybrid written → /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT_HYBRID/DDR5_1024_5_0_memory.csv\n",
      "[OK] Hybrid written → /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT_HYBRID/DDR5_1024_5_0_sensors.csv\n",
      "\n",
      "[DONE] Hybrid rankouts built for 2 (platform,WIN,KF) combos. Skipped=46.\n",
      "       Output folder: /Users/hsiaopingni/octaneX_v7_4functions/FeatureRankOUT_HYBRID\n"
     ]
    }
   ],
   "source": [
    "# === JUPYTER: Build FeatureRankOUT_HYBRID PER PLATFORM — ACROSS ALL (WIN,KF) GRID ===\n",
    "# UPDATED per your request:\n",
    "#   ✅ \"building hybrid ranking across WIN and F like CP-MI ranking\"\n",
    "#\n",
    "# What this does:\n",
    "#   - Instead of hard-coding (WIN,KF) per platform, it scans ALL (WINS × KFOLDS) available,\n",
    "#     exactly like your CP-MI rank grid exists.\n",
    "#   - For each platform (DDR4, DDR5) and for each (win,kfold):\n",
    "#       1) Load CP-MI ranks from FeatureRankOUT/<setup>_<win>_<kfold>_0_{sub}.csv\n",
    "#       2) Pool SHAP across that platform's anomalies at that SAME (win,kfold),\n",
    "#          using the best pct + method per anomaly from:\n",
    "#             Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#       3) Join CP-MI and pooled SHAP by normalized feature name\n",
    "#       4) Compute hybrid_score per subspace and write:\n",
    "#             FeatureRankOUT_HYBRID/<setup>_<win>_<kfold>_0_{compute|memory|sensors}.csv\n",
    "#       5) Save the joined table for audit:\n",
    "#             Results/Explainability_SHAP_BestPlatforms/cmp_cpmi_vs_shap/PLATFORM_<setup>_WIN<w>_KF<k>__SHAP_vs_CPMI_joined.csv\n",
    "#\n",
    "# Inputs:\n",
    "#   - Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#   - FeatureRankOUT/<setup>_<win>_<kfold>_0_{compute|memory|sensors}.csv  (CP-MI ranks)\n",
    "#   - Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN<w>_KF<k>_PCT<p>_M<method>.csv (SHAP)\n",
    "#\n",
    "# Output:\n",
    "#   - FeatureRankOUT_HYBRID/<setup>_<win>_<kfold>_0_{compute|memory|sensors}.csv\n",
    "#\n",
    "# Notes:\n",
    "#   - If SHAP for a given (win,kfold) isn't available (no SHAP files), it will SKIP that combo.\n",
    "#   - If CP-MI ranks missing for a given (win,kfold), it will SKIP that combo.\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "import glob, re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SUBSPACES = (\"compute\", \"memory\", \"sensors\")\n",
    "\n",
    "# ------------------ Normalization ------------------\n",
    "def _norm_name(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \"_\", str(s)).lower()\n",
    "    s = s.replace(\"%\", \"pct\")\n",
    "    s = re.sub(r\"[^a-z0-9_]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def _read_csv(p: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ------------------ Locate CP-MI rank CSVs ------------------\n",
    "def _find_cpmi_csvs(root: Path, setup: str, win: int, kfold: int, rank_dirs=None) -> Dict[str, Optional[Path]]:\n",
    "    out = {s: None for s in SUBSPACES}\n",
    "    search_roots = rank_dirs or [\n",
    "        root / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"octaneX\" / \"FeatureRankOUT\",\n",
    "        Path.home() / \"Desktop\" / \"octaneX\" / \"FeatureRankOUT\",\n",
    "    ]\n",
    "    for sub in SUBSPACES:\n",
    "        fname = f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "        for rd in search_roots:\n",
    "            p = Path(rd) / fname\n",
    "            if p.exists():\n",
    "                out[sub] = p\n",
    "                break\n",
    "        if out[sub] is None:\n",
    "            for rd in search_roots:\n",
    "                rd = Path(rd)\n",
    "                if not rd.exists():\n",
    "                    continue\n",
    "                hits = list(rd.rglob(f\"*{setup}*{sub}*CPMI*.csv\")) + list(rd.rglob(f\"*{setup}*{sub}*CP-MI*.csv\"))\n",
    "                if hits:\n",
    "                    out[sub] = Path(hits[0])\n",
    "                    break\n",
    "    return out\n",
    "\n",
    "def _read_cpmi_rank(root: Path, setup: str, win: int, kfold: int, rank_dirs=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a long table:\n",
    "      feature_cpmi, subspace_cpmi, cpmi_score, cpmi_rank, feature_norm\n",
    "    \"\"\"\n",
    "    paths = _find_cpmi_csvs(root, setup, win, kfold, rank_dirs=rank_dirs)\n",
    "    rows = []\n",
    "    for sub, p in paths.items():\n",
    "        if p is None:\n",
    "            continue\n",
    "        df = _read_csv(p)\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        cols_lower = {c.lower(): c for c in df.columns}\n",
    "        fcol = cols_lower.get(\"feature\") or list(df.columns)[0]\n",
    "\n",
    "        # Prefer a numeric score column if present\n",
    "        num_cols = [c for c in df.columns if c != fcol and pd.api.types.is_numeric_dtype(df[c])]\n",
    "        if num_cols:\n",
    "            s_col = num_cols[0]\n",
    "            scores = pd.to_numeric(df[s_col], errors=\"coerce\").fillna(0.0).values\n",
    "        else:\n",
    "            # fallback: descending pseudo-scores by position\n",
    "            scores = np.linspace(1.0, 0.0, num=len(df), endpoint=False)\n",
    "\n",
    "        tmp = pd.DataFrame({\n",
    "            \"feature_cpmi\": df[fcol].astype(str).values,\n",
    "            \"subspace_cpmi\": sub,\n",
    "            \"cpmi_score\": scores\n",
    "        })\n",
    "        tmp[\"cpmi_rank\"] = pd.Series(scores).rank(ascending=False, method=\"dense\").astype(int)\n",
    "        tmp[\"feature_norm\"] = tmp[\"feature_cpmi\"].map(_norm_name)\n",
    "        rows.append(tmp)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"feature_cpmi\",\"subspace_cpmi\",\"cpmi_score\",\"cpmi_rank\",\"feature_norm\"])\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# ------------------ SHAP BESTPLAT full reader ------------------\n",
    "def _find_shap_bestplat(expl_dir: Path, setup: str, anomaly: str, win: int, kfold: int, pct: int, method: str) -> Optional[Path]:\n",
    "    exact = expl_dir / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT{pct}_M{method}.csv\"\n",
    "    if exact.exists():\n",
    "        return exact\n",
    "    # fallback: any pct for same (setup,anomaly,win,kfold,method)\n",
    "    pat = str(expl_dir / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT*_M{method}.csv\")\n",
    "    hits = sorted(glob.glob(pat))\n",
    "    return Path(hits[0]) if hits else None\n",
    "\n",
    "def _read_shap_bestplat(expl_dir: Path, setup: str, anomaly: str, win: int, kfold: int, pct: int, method: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns long table:\n",
    "      feature_shap, subspace_shap, shap_mean_abs, shap_rank, feature_norm\n",
    "    \"\"\"\n",
    "    p = _find_shap_bestplat(expl_dir, setup, anomaly, win, kfold, pct, method)\n",
    "    if p is None:\n",
    "        return pd.DataFrame(columns=[\"feature_shap\",\"subspace_shap\",\"shap_mean_abs\",\"shap_rank\",\"feature_norm\"])\n",
    "    df = _read_csv(p)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"feature_shap\",\"subspace_shap\",\"shap_mean_abs\",\"shap_rank\",\"feature_norm\"])\n",
    "\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    fcol = cols_lower.get(\"feature\") or list(df.columns)[0]\n",
    "    scol = cols_lower.get(\"shap_mean_abs\") or cols_lower.get(\"importance\")\n",
    "    subc = cols_lower.get(\"subspace\")\n",
    "\n",
    "    out = pd.DataFrame({\"feature_shap\": df[fcol].astype(str)})\n",
    "    out[\"subspace_shap\"] = df[subc].astype(str).str.lower() if subc else \"compute\"\n",
    "    out[\"shap_mean_abs\"] = pd.to_numeric(df[scol], errors=\"coerce\").fillna(0.0) if scol else 0.0\n",
    "    out[\"feature_norm\"]  = out[\"feature_shap\"].map(_norm_name)\n",
    "\n",
    "    # If multiple rows map to same normalized feature/subspace, keep MAX SHAP\n",
    "    out = (out.groupby([\"feature_norm\",\"subspace_shap\"], as_index=False)[\"shap_mean_abs\"]\n",
    "              .max()\n",
    "              .merge(out.drop_duplicates([\"feature_norm\",\"subspace_shap\"])[[\"feature_norm\",\"subspace_shap\",\"feature_shap\"]],\n",
    "                     on=[\"feature_norm\",\"subspace_shap\"], how=\"left\"))\n",
    "\n",
    "    out[\"shap_rank\"] = out.groupby(\"subspace_shap\")[\"shap_mean_abs\"].rank(ascending=False, method=\"dense\").astype(int)\n",
    "    return out[[\"feature_shap\",\"subspace_shap\",\"shap_mean_abs\",\"shap_rank\",\"feature_norm\"]]\n",
    "\n",
    "# ------------------ Pool SHAP across platform anomalies for a given (win,kfold) ------------------\n",
    "def _pool_shap_for_platform(details_df: pd.DataFrame, expl_dir: Path, setup: str, win: int, kfold: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pool SHAP across all anomalies for this (setup,win,kfold):\n",
    "      - Use best_pct_by_median + method per anomaly from BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "      - Load SHAP_BESTPLAT_full_* for each anomaly (if exists)\n",
    "      - Pool by (feature_norm, subspace) using MAX shap_mean_abs across anomalies\n",
    "    \"\"\"\n",
    "    mask = (\n",
    "        (details_df[\"setup\"].astype(str).str.upper() == setup.upper()) &\n",
    "        (pd.to_numeric(details_df[\"win\"], errors=\"coerce\") == int(win)) &\n",
    "        (pd.to_numeric(details_df[\"kfold\"], errors=\"coerce\") == int(kfold))\n",
    "    )\n",
    "    sub = details_df.loc[mask].copy()\n",
    "    if sub.empty:\n",
    "        return pd.DataFrame(columns=[\"feature_shap\",\"subspace_shap\",\"shap_mean_abs\",\"shap_rank\",\"feature_norm\"])\n",
    "\n",
    "    shap_parts = []\n",
    "    for _, r in sub.iterrows():\n",
    "        anomaly = str(r[\"anomaly\"]).strip().upper()\n",
    "        pct = int(pd.to_numeric(r[\"best_pct_by_median\"], errors=\"coerce\"))\n",
    "        method = str(r[\"method\"]).strip()\n",
    "        s = _read_shap_bestplat(expl_dir, setup, anomaly, win, kfold, pct, method)\n",
    "        if not s.empty:\n",
    "            s = s.copy()\n",
    "            s[\"anomaly\"] = anomaly\n",
    "            shap_parts.append(s)\n",
    "\n",
    "    if not shap_parts:\n",
    "        return pd.DataFrame(columns=[\"feature_shap\",\"subspace_shap\",\"shap_mean_abs\",\"shap_rank\",\"feature_norm\"])\n",
    "\n",
    "    shap_all = pd.concat(shap_parts, ignore_index=True)\n",
    "\n",
    "    pooled = (shap_all.groupby([\"feature_norm\",\"subspace_shap\"], as_index=False)[\"shap_mean_abs\"]\n",
    "                    .max()\n",
    "                    .merge(shap_all.drop_duplicates([\"feature_norm\",\"subspace_shap\"])[[\"feature_norm\",\"subspace_shap\",\"feature_shap\"]],\n",
    "                           on=[\"feature_norm\",\"subspace_shap\"], how=\"left\"))\n",
    "\n",
    "    pooled[\"shap_rank\"] = pooled.groupby(\"subspace_shap\")[\"shap_mean_abs\"].rank(ascending=False, method=\"dense\").astype(int)\n",
    "    return pooled[[\"feature_shap\",\"subspace_shap\",\"shap_mean_abs\",\"shap_rank\",\"feature_norm\"]]\n",
    "\n",
    "# ------------------ Hybrid scoring + writer ------------------\n",
    "def _percentile(series: pd.Series) -> pd.Series:\n",
    "    x = pd.Series(series).astype(float).fillna(0.0)\n",
    "    if len(x) <= 1:\n",
    "        return pd.Series(np.zeros(len(x)), index=x.index, dtype=float)\n",
    "    r = x.rank(ascending=True, method=\"average\")\n",
    "    return (r - 1) / (len(x) - 1)\n",
    "\n",
    "def _write_hybrid_from_joined(root: Path, joined_df: pd.DataFrame, setup: str, win: int, kfold: int,\n",
    "                              alpha=0.60, beta=0.40, lam=0.05, q=0.80):\n",
    "    \"\"\"\n",
    "    Writes one hybrid CSV per subspace:\n",
    "      FeatureRankOUT_HYBRID/<setup>_<win>_<kfold>_0_<sub>.csv\n",
    "    \"\"\"\n",
    "    hyb_dir = root / \"FeatureRankOUT_HYBRID\"\n",
    "    hyb_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for sub in (\"compute\", \"memory\", \"sensors\"):\n",
    "        sub_df = joined_df[joined_df[\"subspace\"] == sub].copy()\n",
    "        if sub_df.empty:\n",
    "            continue\n",
    "\n",
    "        sub_df[\"cpmi_score\"] = pd.to_numeric(sub_df.get(\"cpmi_score\", 0.0), errors=\"coerce\").fillna(0.0)\n",
    "        sub_df[\"shap_mean_abs\"] = pd.to_numeric(sub_df.get(\"shap_mean_abs\", 0.0), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "        sub_df[\"r_cpmi\"] = _percentile(sub_df[\"cpmi_score\"])\n",
    "        sub_df[\"r_shap\"] = _percentile(sub_df[\"shap_mean_abs\"])\n",
    "\n",
    "        thr = np.nanpercentile(sub_df[\"r_shap\"], q * 100.0) if len(sub_df) else 1.0\n",
    "        sub_df[\"bonus\"] = (sub_df[\"r_shap\"] >= thr).astype(float) * lam\n",
    "\n",
    "        sub_df[\"hybrid_score\"] = alpha * sub_df[\"r_cpmi\"] + beta * sub_df[\"r_shap\"] + sub_df[\"bonus\"]\n",
    "        sub_df[\"hybrid_rank\"]  = sub_df[\"hybrid_score\"].rank(ascending=False, method=\"dense\").astype(int)\n",
    "\n",
    "        out = hyb_dir / f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "        sub_df[[\"feature_display\",\"hybrid_score\",\"hybrid_rank\",\"cpmi_score\",\"shap_mean_abs\"]].to_csv(out, index=False)\n",
    "        print(f\"[OK] Hybrid written → {out}\")\n",
    "\n",
    "# ------------------ Discover available (win,kfold) combos ------------------\n",
    "def _discover_grid_from_cpmi(root: Path, setup: str, rank_dirs=None) -> List[Tuple[int,int]]:\n",
    "    \"\"\"\n",
    "    Scan FeatureRankOUT files for this setup to discover all (win,kfold) combos present.\n",
    "    Looks for: <setup>_<win>_<kfold>_0_<sub>.csv\n",
    "    \"\"\"\n",
    "    search_roots = rank_dirs or [\n",
    "        root / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"octaneX\" / \"FeatureRankOUT\",\n",
    "        Path.home() / \"Desktop\" / \"octaneX\" / \"FeatureRankOUT\",\n",
    "    ]\n",
    "    combos = set()\n",
    "    pat = re.compile(rf\"^{re.escape(setup)}_(\\d+)_(\\d+)_0_(compute|memory|sensors)\\.csv$\", re.I)\n",
    "\n",
    "    for rd in search_roots:\n",
    "        rd = Path(rd)\n",
    "        if not rd.exists():\n",
    "            continue\n",
    "        for p in rd.glob(f\"{setup}_*_*_0_*.csv\"):\n",
    "            m = pat.match(p.name)\n",
    "            if not m:\n",
    "                continue\n",
    "            win = int(m.group(1))\n",
    "            kf  = int(m.group(2))\n",
    "            combos.add((win, kf))\n",
    "\n",
    "    return sorted(combos, key=lambda x: (x[0], x[1]))\n",
    "\n",
    "# ------------------ Main driver ------------------\n",
    "def build_hybrid_rankouts_per_platform_across_grid(\n",
    "    root_path: str,\n",
    "    alpha=0.60, beta=0.40, lam=0.05, q=0.80,\n",
    "    rank_dirs=None,\n",
    "):\n",
    "    root = Path(root_path).expanduser().resolve()\n",
    "    res_dir  = root / \"Results\"\n",
    "    expl_dir = res_dir / \"Explainability_SHAP_BestPlatforms\"\n",
    "    details_path = res_dir / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "\n",
    "    if not details_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing per-platform details CSV: {details_path}\")\n",
    "\n",
    "    details = pd.read_csv(details_path)\n",
    "    details.columns = [c.lower() for c in details.columns]\n",
    "    if \"best_method\" in details.columns and \"method\" not in details.columns:\n",
    "        details = details.rename(columns={\"best_method\":\"method\"})\n",
    "    if \"best_pct_by_median\" not in details.columns and \"pct\" in details.columns:\n",
    "        details = details.rename(columns={\"pct\":\"best_pct_by_median\"})\n",
    "\n",
    "    need = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"}\n",
    "    miss = need - set(details.columns)\n",
    "    if miss:\n",
    "        raise KeyError(f\"Details CSV missing required columns: {sorted(miss)}\")\n",
    "\n",
    "    out_join_dir = expl_dir / \"cmp_cpmi_vs_shap\"\n",
    "    out_join_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for setup in [\"DDR4\", \"DDR5\"]:\n",
    "        print(f\"\\n[PLATFORM] {setup}: discovering CP-MI grid ...\")\n",
    "        combos = _discover_grid_from_cpmi(root, setup, rank_dirs=rank_dirs)\n",
    "        if not combos:\n",
    "            print(f\"[WARN] No CP-MI rank files found for {setup}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[INFO] {setup}: found {len(combos)} (WIN,KF) combos from CP-MI ranks.\")\n",
    "\n",
    "        for win, kfold in combos:\n",
    "            # 1) load CP-MI ranks for this (win,kfold)\n",
    "            cpmi = _read_cpmi_rank(root, setup, win, kfold, rank_dirs=rank_dirs)\n",
    "            if cpmi.empty:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # 2) pool SHAP for this (win,kfold) using per-platform details (best pct/method per anomaly)\n",
    "            shap = _pool_shap_for_platform(details, expl_dir, setup, win, kfold)\n",
    "            if shap.empty:\n",
    "                # no SHAP available for this combo -> skip\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # 3) join\n",
    "            joined = pd.merge(\n",
    "                cpmi.drop_duplicates([\"feature_norm\",\"subspace_cpmi\"]),\n",
    "                shap.drop_duplicates([\"feature_norm\",\"subspace_shap\"]),\n",
    "                on=\"feature_norm\",\n",
    "                how=\"outer\",\n",
    "                suffixes=(\"_cpmi\", \"_shap\")\n",
    "            )\n",
    "\n",
    "            joined[\"feature_display\"] = joined[\"feature_cpmi\"].fillna(joined[\"feature_shap\"])\n",
    "            joined[\"subspace\"] = joined[\"subspace_cpmi\"].fillna(joined[\"subspace_shap\"])\n",
    "\n",
    "            joined[\"cpmi_rank\"] = pd.to_numeric(joined.get(\"cpmi_rank\"), errors=\"coerce\")\n",
    "            joined[\"shap_rank\"] = pd.to_numeric(joined.get(\"shap_rank\"), errors=\"coerce\")\n",
    "            joined[\"cpmi_score\"] = pd.to_numeric(joined.get(\"cpmi_score\"), errors=\"coerce\").fillna(0.0)\n",
    "            joined[\"shap_mean_abs\"] = pd.to_numeric(joined.get(\"shap_mean_abs\"), errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "            joined[\"delta_rank\"]  = joined[\"cpmi_rank\"].fillna(np.inf) - joined[\"shap_rank\"].fillna(np.inf)\n",
    "            joined[\"delta_score\"] = joined[\"shap_mean_abs\"] - joined[\"cpmi_score\"]\n",
    "\n",
    "            joined = joined.sort_values([\"subspace\", \"shap_rank\", \"cpmi_rank\"], na_position=\"last\")\n",
    "\n",
    "            out_csv = out_join_dir / f\"PLATFORM_{setup}_WIN{win}_KF{kfold}__SHAP_vs_CPMI_joined.csv\"\n",
    "            joined.to_csv(out_csv, index=False)\n",
    "            print(f\"[OK] Joined saved → {out_csv.name}\")\n",
    "\n",
    "            # 4) write hybrid rankouts per subspace\n",
    "            _write_hybrid_from_joined(\n",
    "                root, joined, setup, win, kfold,\n",
    "                alpha=alpha, beta=beta, lam=lam, q=q\n",
    "            )\n",
    "\n",
    "            processed += 1\n",
    "\n",
    "    print(f\"\\n[DONE] Hybrid rankouts built for {processed} (platform,WIN,KF) combos. Skipped={skipped}.\")\n",
    "    print(f\"       Output folder: {root/'FeatureRankOUT_HYBRID'}\")\n",
    "\n",
    "# -------------------- RUN HERE --------------------\n",
    "build_hybrid_rankouts_per_platform_across_grid(\n",
    "    root_path=\"/Users/hsiaopingni/octaneX_v7_4functions\",\n",
    "    alpha=0.60, beta=0.40, lam=0.05, q=0.80,\n",
    "    rank_dirs=None,   # optionally pass your own RANK_DIRS list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8438bfb-1b81-4c0e-ac68-3a59aed0c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] PLATFORM_CFG (auto): {'DDR4': {'win': 512, 'kfold': 3}, 'DDR5': {'win': 1024, 'kfold': 5}}\n",
      "Saved:\n",
      " - /Users/hsiaopingni/octaneX_v7_4functions/Results/_editor_package/workloads_performance_platform.csv\n",
      " - /Users/hsiaopingni/octaneX_v7_4functions/Results/_editor_package/sweep_counts_by_subspace_platform.csv\n",
      " - /Users/hsiaopingni/octaneX_v7_4functions/Results/_editor_package/sweep_feature_counts_per_subspace_DDR4.png\n",
      " - /Users/hsiaopingni/octaneX_v7_4functions/Results/_editor_package/sweep_feature_counts_per_subspace_DDR5.png\n",
      " - /Users/hsiaopingni/octaneX_v7_4functions/Results/_editor_package/sweep_feature_counts_per_subspace_PLATFORM.png\n"
     ]
    }
   ],
   "source": [
    "# === PER-PLATFORM editor package export (AUTO from per-platform winners) ===\n",
    "# UPDATED per your current per-platform pipeline.\n",
    "#\n",
    "# What this does (per platform):\n",
    "#   - Uses per-platform winners (auto; no hard-coded WIN/K/PCT):\n",
    "#       Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
    "#       Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#   - Exports:\n",
    "#       1) _editor_package/workloads_performance_platform.csv\n",
    "#          (platform + anomaly rows with chosen WIN/K, best_pct, best_method and metrics if present)\n",
    "#       2) _editor_package/sweep_counts_by_subspace_platform.csv\n",
    "#          (counts vs top-% thresholds for CP-MI ranks at the chosen (WIN,K) per platform)\n",
    "#          OPTIONAL: also include HYBRID ranks if FeatureRankOUT_HYBRID exists (toggle below)\n",
    "#       3) Plots:\n",
    "#          - _editor_package/sweep_feature_counts_per_subspace_<setup>.png\n",
    "#          - _editor_package/sweep_feature_counts_per_subspace_PLATFORM.png\n",
    "#\n",
    "# Inputs:\n",
    "#   - Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
    "#   - Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#   - FeatureRankOUT/<setup>_<win>_<kfold>_0_{compute|memory|sensors}.csv\n",
    "#   - (optional) FeatureRankOUT_HYBRID/<setup>_<win>_<kfold>_0_{compute|memory|sensors}.csv\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----------- CONFIG ----------\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES  = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "# normalize if notebook points too deep\n",
    "if RES.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\"):\n",
    "    RES = RES.parent\n",
    "\n",
    "BEST_PLATFORM        = RES / \"BEST_in_DesignSpace_Post_per_platform.csv\"\n",
    "BEST_PLATFORM_DETAIL = RES / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "\n",
    "RANK_DIR = ROOT / \"FeatureRankOUT\"            # CP-MI baseline ranks\n",
    "HYB_DIR  = ROOT / \"FeatureRankOUT_HYBRID\"     # optional hybrid ranks\n",
    "OUT_DIR  = RES / \"_editor_package\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUBSPACES = (\"compute\",\"memory\",\"sensors\")\n",
    "TOP_PCTS  = list(range(10, 101, 10))\n",
    "\n",
    "INCLUDE_HYBRID_COUNTS = True   # set False if you only want CP-MI counts\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def read_platform_best(best_csv: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(best_csv)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    # minimal expected columns: setup, win, kfold\n",
    "    need = {\"setup\",\"win\",\"kfold\"}\n",
    "    miss = need - set(df.columns)\n",
    "    if miss:\n",
    "        raise KeyError(f\"{best_csv} missing required columns: {sorted(miss)}. Have: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def read_platform_details(details_csv: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(details_csv)\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    if \"best_method\" in df.columns and \"method\" not in df.columns:\n",
    "        df = df.rename(columns={\"best_method\":\"method\"})\n",
    "    if \"best_pct_by_median\" not in df.columns and \"pct\" in df.columns:\n",
    "        df = df.rename(columns={\"pct\":\"best_pct_by_median\"})\n",
    "    need = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"}\n",
    "    miss = need - set(df.columns)\n",
    "    if miss:\n",
    "        raise KeyError(f\"{details_csv} missing required columns: {sorted(miss)}. Have: {list(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "def pick_score_col(df: pd.DataFrame) -> str | None:\n",
    "    for c in (\"cpmi_score\",\"hybrid_score\",\"shap_mean_abs\"):\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # fallback: first numeric col after feature if any\n",
    "    nums = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return nums[0] if nums else (df.columns[1] if df.shape[1] >= 2 else None)\n",
    "\n",
    "def load_rank_table(rank_dir: Path, setup: str, win: int, kfold: int, subspace: str) -> pd.DataFrame:\n",
    "    p = rank_dir / f\"{setup}_{win}_{kfold}_0_{subspace}.csv\"\n",
    "    if not p.exists():\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(p)\n",
    "    # feature column may differ by source\n",
    "    feat_col = (\n",
    "        \"feature\" if \"feature\" in df.columns else\n",
    "        (\"feature_display\" if \"feature_display\" in df.columns else df.columns[0])\n",
    "    )\n",
    "    df = df.rename(columns={feat_col: \"feature\"})\n",
    "    return df\n",
    "\n",
    "def top_k_count(n_total: int, pct: int) -> int:\n",
    "    k = int(math.ceil(n_total * pct / 100.0))\n",
    "    return max(1, k) if pct > 0 else 0\n",
    "\n",
    "def enumerate_mean_rank(values: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Your prior 'enumerated mean rank 1..5' scheme.\n",
    "    \"\"\"\n",
    "    if values is None or len(values) == 0:\n",
    "        return np.nan\n",
    "    v = pd.to_numeric(values, errors=\"coerce\").fillna(0.0)\n",
    "    r = v.rank(ascending=True, method=\"average\")\n",
    "    p = (r - 1) / (len(v) - 1) if len(v) > 1 else pd.Series([0.0]*len(v))\n",
    "    buckets = 5 - (p*5).astype(int).clip(0,4)\n",
    "    return float(buckets.mean())\n",
    "\n",
    "# ---------- Load platform winners ----------\n",
    "if not BEST_PLATFORM.exists():\n",
    "    raise FileNotFoundError(f\"Missing platform BEST CSV: {BEST_PLATFORM}\")\n",
    "if not BEST_PLATFORM_DETAIL.exists():\n",
    "    raise FileNotFoundError(f\"Missing platform BEST details CSV: {BEST_PLATFORM_DETAIL}\")\n",
    "\n",
    "best_plat = read_platform_best(BEST_PLATFORM)\n",
    "detail    = read_platform_details(BEST_PLATFORM_DETAIL)\n",
    "\n",
    "# Build PLATFORM_CFG automatically\n",
    "PLATFORM_CFG = {}\n",
    "for _, r in best_plat.iterrows():\n",
    "    setup = str(r[\"setup\"]).strip()\n",
    "    PLATFORM_CFG[setup] = dict(\n",
    "        win=int(pd.to_numeric(r[\"win\"], errors=\"coerce\")),\n",
    "        kfold=int(pd.to_numeric(r[\"kfold\"], errors=\"coerce\")),\n",
    "    )\n",
    "\n",
    "if not PLATFORM_CFG:\n",
    "    raise RuntimeError(\"No platform configs found in BEST_in_DesignSpace_Post_per_platform.csv\")\n",
    "\n",
    "print(\"[OK] PLATFORM_CFG (auto):\", PLATFORM_CFG)\n",
    "\n",
    "# ---------- A) workloads_performance_platform.csv ----------\n",
    "# Use the per-platform details (anomaly-specific best pct & method).\n",
    "# Include any metric columns that exist (roc/auc/pr/iqr/min/max etc.).\n",
    "metric_cols = [c for c in detail.columns if any(tok in c for tok in [\"auc\", \"roc\", \"iqr\", \"min\", \"max\", \"median\", \"n_runs\"])]\n",
    "\n",
    "perf_rows = []\n",
    "for setup, cfg in PLATFORM_CFG.items():\n",
    "    win, kfold = int(cfg[\"win\"]), int(cfg[\"kfold\"])\n",
    "    sub = detail[\n",
    "        (detail[\"setup\"].astype(str).str.upper() == setup.upper()) &\n",
    "        (pd.to_numeric(detail[\"win\"], errors=\"coerce\") == win) &\n",
    "        (pd.to_numeric(detail[\"kfold\"], errors=\"coerce\") == kfold)\n",
    "    ].copy()\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    sub[\"platform_win\"] = win\n",
    "    sub[\"platform_kfold\"] = kfold\n",
    "    cols_keep = [\"setup\",\"anomaly\",\"platform_win\",\"platform_kfold\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"] + metric_cols\n",
    "    cols_keep = [c for c in cols_keep if c in sub.columns]\n",
    "    perf_rows.append(sub[cols_keep])\n",
    "\n",
    "perf_platform = pd.concat(perf_rows, ignore_index=True) if perf_rows else pd.DataFrame()\n",
    "perf_csv = OUT_DIR / \"workloads_performance_platform.csv\"\n",
    "perf_platform.to_csv(perf_csv, index=False)\n",
    "\n",
    "# ---------- B) sweep_counts_by_subspace_platform.csv ----------\n",
    "rows = []\n",
    "for setup, cfg in PLATFORM_CFG.items():\n",
    "    win, kfold = int(cfg[\"win\"]), int(cfg[\"kfold\"])\n",
    "\n",
    "    for sub in SUBSPACES:\n",
    "        # ---- CP-MI counts ----\n",
    "        df_rank = load_rank_table(RANK_DIR, setup, win, kfold, sub)\n",
    "        if not df_rank.empty:\n",
    "            score_col = pick_score_col(df_rank)\n",
    "            n_total = len(df_rank)\n",
    "            enum_mean = enumerate_mean_rank(df_rank[score_col]) if score_col and score_col in df_rank.columns else np.nan\n",
    "\n",
    "            for pct in TOP_PCTS:\n",
    "                k = top_k_count(n_total, pct)\n",
    "                rows.append({\n",
    "                    \"setup\": setup, \"win\": win, \"kfold\": kfold,\n",
    "                    \"rank_source\": \"CPMI\",\n",
    "                    \"subspace\": sub, \"top_pct\": pct,\n",
    "                    \"n_total\": n_total, \"n_selected\": k,\n",
    "                    \"enumerated_mean_rank_1to5\": enum_mean\n",
    "                })\n",
    "        else:\n",
    "            print(f\"[WARN] Missing CP-MI rank file for {setup} WIN={win} K={kfold} sub={sub}\")\n",
    "\n",
    "        # ---- HYBRID counts (optional) ----\n",
    "        if INCLUDE_HYBRID_COUNTS:\n",
    "            df_hyb = load_rank_table(HYB_DIR, setup, win, kfold, sub)\n",
    "            if not df_hyb.empty:\n",
    "                score_col_h = pick_score_col(df_hyb)\n",
    "                n_total_h = len(df_hyb)\n",
    "                enum_mean_h = enumerate_mean_rank(df_hyb[score_col_h]) if score_col_h and score_col_h in df_hyb.columns else np.nan\n",
    "\n",
    "                for pct in TOP_PCTS:\n",
    "                    k = top_k_count(n_total_h, pct)\n",
    "                    rows.append({\n",
    "                        \"setup\": setup, \"win\": win, \"kfold\": kfold,\n",
    "                        \"rank_source\": \"HYBRID\",\n",
    "                        \"subspace\": sub, \"top_pct\": pct,\n",
    "                        \"n_total\": n_total_h, \"n_selected\": k,\n",
    "                        \"enumerated_mean_rank_1to5\": enum_mean_h\n",
    "                    })\n",
    "\n",
    "sweep_platform = pd.DataFrame(rows)\n",
    "sweep_csv = OUT_DIR / \"sweep_counts_by_subspace_platform.csv\"\n",
    "sweep_platform.to_csv(sweep_csv, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", perf_csv)\n",
    "print(\" -\", sweep_csv)\n",
    "\n",
    "# ---------- C) Plot: top-% vs number of features per subspace (per platform) ----------\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 200,\n",
    "    \"savefig.dpi\": 300,\n",
    "    \"font.size\": 12,\n",
    "    \"axes.labelsize\": 10,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"legend.fontsize\": 11,\n",
    "    \"lines.linewidth\": 2.2,\n",
    "})\n",
    "\n",
    "colors = {\"compute\":\"#1f77b4\", \"memory\":\"#66b3ff\", \"sensors\":\"#ff7f0e\"}\n",
    "\n",
    "# One plot per platform, CPMI only (clean)\n",
    "for setup, cfg in PLATFORM_CFG.items():\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "    d0 = sweep_platform[(sweep_platform[\"rank_source\"]==\"CPMI\") & (sweep_platform[\"setup\"]==setup)]\n",
    "    if d0.empty:\n",
    "        plt.close(fig)\n",
    "        continue\n",
    "\n",
    "    for sub in SUBSPACES:\n",
    "        d = d0[d0[\"subspace\"]==sub]\n",
    "        if d.empty:\n",
    "            continue\n",
    "        g = d.groupby(\"top_pct\")[\"n_selected\"].median().reset_index()\n",
    "        ax.plot(g[\"top_pct\"], g[\"n_selected\"], marker=\"o\", label=sub.capitalize(), color=colors[sub])\n",
    "\n",
    "    ax.set_xlabel(\"Top-% threshold\")\n",
    "    ax.set_ylabel(\"# features selected\")\n",
    "    ax.set_title(f\"Feature count vs top-% threshold (CP-MI) — {setup} (WIN={cfg['win']}, K={cfg['kfold']})\")\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "    fig.tight_layout(rect=[0,0,0.82,1])\n",
    "    outp = OUT_DIR / f\"sweep_feature_counts_per_subspace_{setup}.png\"\n",
    "    fig.savefig(outp, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\" -\", outp)\n",
    "\n",
    "# Combined plot (two platforms shown by linestyle)\n",
    "fig, ax = plt.subplots(figsize=(7.8, 4.7))\n",
    "linestyles = {\"DDR4\":\"-\", \"DDR5\":\"--\"}\n",
    "\n",
    "for setup, cfg in PLATFORM_CFG.items():\n",
    "    d0 = sweep_platform[(sweep_platform[\"rank_source\"]==\"CPMI\") & (sweep_platform[\"setup\"]==setup)]\n",
    "    if d0.empty:\n",
    "        continue\n",
    "    for sub in SUBSPACES:\n",
    "        d = d0[d0[\"subspace\"]==sub]\n",
    "        if d.empty:\n",
    "            continue\n",
    "        g = d.groupby(\"top_pct\")[\"n_selected\"].median().reset_index()\n",
    "        ax.plot(g[\"top_pct\"], g[\"n_selected\"], marker=\"o\",\n",
    "                label=f\"{setup} {sub.capitalize()}\",\n",
    "                color=colors[sub], linestyle=linestyles.get(setup, \"-\"))\n",
    "\n",
    "ax.set_xlabel(\"Top-% threshold\")\n",
    "ax.set_ylabel(\"# features selected\")\n",
    "ax.set_title(\"Feature count vs top-% threshold (per subspace, CP-MI) — per platform\")\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=False)\n",
    "fig.tight_layout(rect=[0,0,0.78,1])\n",
    "\n",
    "out_comb = OUT_DIR / \"sweep_feature_counts_per_subspace_PLATFORM.png\"\n",
    "fig.savefig(out_comb, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "print(\" -\", out_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "914caccb-6abf-4408-9fa5-1ce90e578d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] PLATFORM_CFG (auto): {'DDR4': {'win': 512, 'kfold': 3}, 'DDR5': {'win': 1024, 'kfold': 5}}\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/_editor_package/enumrank__DDR4_PLATFORM_WIN512_KF3__compute.csv\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/_editor_package/enumrank__DDR4_PLATFORM_WIN512_KF3__memory.csv\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/_editor_package/enumrank__DDR4_PLATFORM_WIN512_KF3__sensors.csv\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/_editor_package/enumrank__DDR5_PLATFORM_WIN1024_KF5__compute.csv\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/_editor_package/enumrank__DDR5_PLATFORM_WIN1024_KF5__memory.csv\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/_editor_package/enumrank__DDR5_PLATFORM_WIN1024_KF5__sensors.csv\n"
     ]
    }
   ],
   "source": [
    "# === PER-PLATFORM enumrank tables (AUTO WIN/K from per-platform winners) ===\n",
    "# UPDATED so you do NOT hard-code DDR4/DDR5 WIN/K.\n",
    "#\n",
    "# Builds ONE enumrank CSV per platform × subspace using the chosen (WIN,K) per platform from:\n",
    "#   Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
    "#\n",
    "# Output (6 files total):\n",
    "#   OUT_DIR/enumrank__DDR4_PLATFORM_WIN<win>_KF<kfold>__compute.csv\n",
    "#   OUT_DIR/enumrank__DDR4_PLATFORM_WIN<win>_KF<kfold>__memory.csv\n",
    "#   OUT_DIR/enumrank__DDR4_PLATFORM_WIN<win>_KF<kfold>__sensors.csv\n",
    "#   OUT_DIR/enumrank__DDR5_PLATFORM_WIN<win>_KF<kfold>__compute.csv\n",
    "#   OUT_DIR/enumrank__DDR5_PLATFORM_WIN<win>_KF<kfold>__memory.csv\n",
    "#   OUT_DIR/enumrank__DDR5_PLATFORM_WIN<win>_KF<kfold>__sensors.csv\n",
    "#\n",
    "# Notes:\n",
    "# - Uses CP-MI rank tables from FeatureRankOUT by (setup,win,kfold,subspace).\n",
    "# - No anomaly dimension; “PLATFORM” indicates platform-level export.\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- paths ----------\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES  = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "# normalize if notebook points too deep\n",
    "if RES.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\"):\n",
    "    RES = RES.parent\n",
    "\n",
    "BEST_PLATFORM = RES / \"BEST_in_DesignSpace_Post_per_platform.csv\"\n",
    "if not BEST_PLATFORM.exists():\n",
    "    raise FileNotFoundError(f\"Missing per-platform winners CSV: {BEST_PLATFORM}\")\n",
    "\n",
    "RANK_DIR = Path(globals().get(\"RANK_DIR\", ROOT / \"FeatureRankOUT\"))\n",
    "OUT_DIR  = Path(globals().get(\"OUT_DIR\", RES / \"_editor_package\"))\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUBSPACES = (\"compute\",\"memory\",\"sensors\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def pick_score_col(df: pd.DataFrame) -> str | None:\n",
    "    for c in (\"cpmi_score\",\"hybrid_score\",\"shap_mean_abs\",\"score\"):\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    feat_col = (\n",
    "        \"feature\" if \"feature\" in df.columns else\n",
    "        (\"feature_display\" if \"feature_display\" in df.columns else df.columns[0])\n",
    "    )\n",
    "    nums = [c for c in df.columns if c != feat_col and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    return nums[0] if nums else None\n",
    "\n",
    "def load_rank_table(rank_dir: Path, setup: str, win: int, kfold: int, subspace: str) -> pd.DataFrame:\n",
    "    p = rank_dir / f\"{setup}_{win}_{kfold}_0_{subspace}.csv\"\n",
    "    if not p.exists():\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(p)\n",
    "    feat_col = (\n",
    "        \"feature\" if \"feature\" in df.columns else\n",
    "        (\"feature_display\" if \"feature_display\" in df.columns else df.columns[0])\n",
    "    )\n",
    "    df = df.rename(columns={feat_col: \"feature\"})\n",
    "    return df\n",
    "\n",
    "def enum_bucket_per_feature(scores: pd.Series) -> pd.Series:\n",
    "    \"\"\"Return a 1..5 bucket per feature (5=top 20%).\"\"\"\n",
    "    s = pd.to_numeric(scores, errors=\"coerce\").fillna(0.0)\n",
    "    if len(s) <= 1:\n",
    "        return pd.Series([3]*len(s), index=s.index, dtype=int)  # neutral if trivial\n",
    "    # percentile rank 0..1 (higher = better)\n",
    "    r = s.rank(ascending=True, method=\"average\")\n",
    "    p = (r - 1) / (len(s) - 1)\n",
    "    # map to 1..5 (5 best)\n",
    "    return (5 - (p*5).astype(int).clip(0,4)).astype(int)\n",
    "\n",
    "def write_enumrank_platform_table(setup: str, win: int, kfold: int, subspace: str) -> Path | None:\n",
    "    df = load_rank_table(RANK_DIR, setup, win, kfold, subspace)\n",
    "    if df.empty:\n",
    "        print(f\"[SKIP] Missing rank table: {setup} WIN={win} KF={kfold} sub={subspace}\")\n",
    "        return None\n",
    "\n",
    "    score_col = pick_score_col(df)\n",
    "    if score_col is None:\n",
    "        print(f\"[SKIP] No score col found in rank table for {setup} WIN={win} KF={kfold} sub={subspace}\")\n",
    "        return None\n",
    "\n",
    "    tmp = df[[\"feature\", score_col]].copy()\n",
    "    tmp = tmp.rename(columns={score_col: \"score\"})\n",
    "    tmp[\"score\"] = pd.to_numeric(tmp[\"score\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    tmp[\"EnumRank_1to5\"] = enum_bucket_per_feature(tmp[\"score\"])\n",
    "\n",
    "    # Percentile for transparency (0..1)\n",
    "    if len(tmp) > 1:\n",
    "        r = tmp[\"score\"].rank(ascending=True, method=\"average\")\n",
    "        tmp[\"Percentile_0to1\"] = (r - 1) / (len(tmp) - 1)\n",
    "    else:\n",
    "        tmp[\"Percentile_0to1\"] = 0.5\n",
    "\n",
    "    tmp = tmp.sort_values([\"EnumRank_1to5\",\"score\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "    out_csv = OUT_DIR / f\"enumrank__{setup}_PLATFORM_WIN{win}_KF{kfold}__{subspace}.csv\"\n",
    "    tmp.to_csv(out_csv, index=False)\n",
    "    print(\"[WROTE]\", out_csv)\n",
    "    return out_csv\n",
    "\n",
    "# ---------- Load per-platform winners (AUTO WIN/K) ----------\n",
    "bp = pd.read_csv(BEST_PLATFORM).copy()\n",
    "bp.columns = [c.lower() for c in bp.columns]\n",
    "need = {\"setup\",\"win\",\"kfold\"}\n",
    "missing = need - set(bp.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"{BEST_PLATFORM} missing required columns {sorted(missing)}. Have: {list(bp.columns)}\")\n",
    "\n",
    "# Build PLATFORM_CFG automatically\n",
    "PLATFORM_CFG = {}\n",
    "for _, r in bp.iterrows():\n",
    "    setup = str(r[\"setup\"]).strip()\n",
    "    win = int(pd.to_numeric(r[\"win\"], errors=\"coerce\"))\n",
    "    kf  = int(pd.to_numeric(r[\"kfold\"], errors=\"coerce\"))\n",
    "    PLATFORM_CFG[setup] = dict(win=win, kfold=kf)\n",
    "\n",
    "print(\"[OK] PLATFORM_CFG (auto):\", PLATFORM_CFG)\n",
    "\n",
    "# ---------- Run per platform × subspace ----------\n",
    "for setup, cfg in PLATFORM_CFG.items():\n",
    "    win, kfold = int(cfg[\"win\"]), int(cfg[\"kfold\"])\n",
    "    for sub in SUBSPACES:\n",
    "        write_enumrank_platform_table(setup, win, kfold, sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d31cf9-af26-4d8f-bb04-37ce9ebe93f6",
   "metadata": {},
   "source": [
    "# **HYBRID OCTANE — Best-Case Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf060a-9031-4aaa-b6bf-bb4f2318969b",
   "metadata": {},
   "source": [
    "# *FULL MULTI-WIN / MULTI-K PIPELINE (AUTHENTIC) - hybrid feature ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "153c23ea-47a4-4cb9-9969-f8fcb631ab85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Cannot write to /Volumes/Untitled/octaneX_results. Using local: /Users/hsiaopingni/octaneX_v7_4functions/Results\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=32 KF=3 (group 1)\n",
      "\n",
      "[RUN] DDR4 RH WIN=32 KF=3 (group 2)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=32 KF=5 (group 3)\n",
      "\n",
      "[RUN] DDR4 RH WIN=32 KF=5 (group 4)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=32 KF=10 (group 5)\n",
      "\n",
      "[RUN] DDR4 RH WIN=32 KF=10 (group 6)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=64 KF=3 (group 7)\n",
      "\n",
      "[RUN] DDR4 RH WIN=64 KF=3 (group 8)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=64 KF=5 (group 9)\n",
      "\n",
      "[RUN] DDR4 RH WIN=64 KF=5 (group 10)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=64 KF=10 (group 11)\n",
      "\n",
      "[RUN] DDR4 RH WIN=64 KF=10 (group 12)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=128 KF=3 (group 13)\n",
      "\n",
      "[RUN] DDR4 RH WIN=128 KF=3 (group 14)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=128 KF=5 (group 15)\n",
      "\n",
      "[RUN] DDR4 RH WIN=128 KF=5 (group 16)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=128 KF=10 (group 17)\n",
      "\n",
      "[RUN] DDR4 RH WIN=128 KF=10 (group 18)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=512 KF=3 (group 19)\n",
      "\n",
      "[RUN] DDR4 RH WIN=512 KF=3 (group 20)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=512 KF=5 (group 21)\n",
      "\n",
      "[RUN] DDR4 RH WIN=512 KF=5 (group 22)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=512 KF=10 (group 23)\n",
      "\n",
      "[RUN] DDR4 RH WIN=512 KF=10 (group 24)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=1024 KF=3 (group 25)\n",
      "\n",
      "[RUN] DDR4 RH WIN=1024 KF=3 (group 26)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=1024 KF=5 (group 27)\n",
      "\n",
      "[RUN] DDR4 RH WIN=1024 KF=5 (group 28)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=1024 KF=10 (group 29)\n",
      "\n",
      "[RUN] DDR4 RH WIN=1024 KF=10 (group 30)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=32 KF=3 (group 31)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=32 KF=3 (group 32)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=32 KF=5 (group 33)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=32 KF=5 (group 34)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=32 KF=10 (group 35)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=32 KF=10 (group 36)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=64 KF=3 (group 37)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=64 KF=3 (group 38)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=64 KF=5 (group 39)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=64 KF=5 (group 40)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=64 KF=10 (group 41)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=64 KF=10 (group 42)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=128 KF=3 (group 43)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=128 KF=3 (group 44)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=128 KF=5 (group 45)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=128 KF=5 (group 46)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=128 KF=10 (group 47)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=128 KF=10 (group 48)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=512 KF=3 (group 49)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=512 KF=3 (group 50)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=512 KF=5 (group 51)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=512 KF=5 (group 52)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=512 KF=10 (group 53)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=512 KF=10 (group 54)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=1024 KF=3 (group 55)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=1024 KF=3 (group 56)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=1024 KF=5 (group 57)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=1024 KF=5 (group 58)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=1024 KF=10 (group 59)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=1024 KF=10 (group 60)\n",
      "\n",
      "[OK] ALLRUNS metrics → /Users/hsiaopingni/octaneX_v7_4functions/Results/per_run_metrics_all_PIPELINE.csv\n",
      "[DONE] Pipeline + DSE plots + weight tables per platform\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ===============================================================================================\n",
    "# FULL MULTI-WIN / MULTI-K PIPELINE (AUTHENTIC)\n",
    "# + DSE PLOTS (all cases)\n",
    "# + TABLE_SUBSPACEWEIGHTS (all cases)\n",
    "#\n",
    "# UPDATE (your request):\n",
    "#   ✅ Auto-adjust y-axis for EVERY DSE subplot (all WIN/K) to remove empty white space below.\n",
    "#   ✅ Do NOT show any values/regions above 1.00 (metrics are clipped at 1.00).\n",
    "#   ✅ BUT still leave a small visual headroom so markers/lines at 1.00 are NOT cut off.\n",
    "#\n",
    "# How:\n",
    "#   - All data are clipped to [0, 1.00] (never > 1.00).\n",
    "#   - Y-axis upper limit is allowed up to 1.02 for DISPLAY ONLY (headroom),\n",
    "#     but data are never plotted above 1.00.\n",
    "#   - Each subplot computes tight y-limits from its own bands/median, adds padding,\n",
    "#     then clamps to [0, 1.02] with a forced minimum headroom above 1.00 if needed.\n",
    "# ===============================================================================================\n",
    "\n",
    "import gc, re, hashlib, warnings, unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ===============================================================================================\n",
    "# 0) PATHS\n",
    "# ===============================================================================================\n",
    "DATA_DIR = Path(\"/Users/hsiaopingni/Desktop/SLM_RAS-main/HW_TELEMETRY_DATA_COLLECTION/TELEMETRY_DATA\")\n",
    "ROOT     = Path(\"/Users/hsiaopingni/octaneX_v7_4functions\")\n",
    "RES_DIR  = ROOT / \"Results\"\n",
    "RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXT_DRIVE = Path(\"/Volumes/Untitled\")\n",
    "EXT_RES   = EXT_DRIVE / \"octaneX_results\"\n",
    "\n",
    "def _can_write_dir(p: Path) -> bool:\n",
    "    try:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        t = p / \".write_test\"\n",
    "        t.write_text(\"ok\")\n",
    "        t.unlink()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if _can_write_dir(EXT_RES):\n",
    "    RES_DIR = EXT_RES\n",
    "    RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[OK] Using external results dir: {RES_DIR}\")\n",
    "else:\n",
    "    print(f\"[WARN] Cannot write to {EXT_RES}. Using local: {RES_DIR}\")\n",
    "\n",
    "RANK_DIRS = [\n",
    "    ROOT / \"FeatureRankOUT_HYBRID\",\n",
    "    EXT_DRIVE / \"FeatureRankOUT_HYBRID\",\n",
    "    EXT_DRIVE / \"octaneX\" / \"FeatureRankOUT_HYBRID\",\n",
    "]\n",
    "\n",
    "# ===============================================================================================\n",
    "# 1) RUN CONFIG\n",
    "# ===============================================================================================\n",
    "SETUPS = [\"DDR4\", \"DDR5\"]\n",
    "ANOMALIES_BY_SETUP = {\"DDR4\": [\"DROOP\", \"RH\"], \"DDR5\": [\"DROOP\", \"SPECTRE\"]}\n",
    "\n",
    "WINS   = [32, 64, 128, 512, 1024]\n",
    "KFOLDS = [3, 5, 10]\n",
    "PCT_SWEEP = list(range(10, 101, 10))\n",
    "\n",
    "OVERLAP_RATIO       = 0.50\n",
    "OVERLAP_RATIO_DROOP = 0.80\n",
    "\n",
    "ROBUST_WINSOR = (2.0, 98.0)\n",
    "SEED = 1337\n",
    "\n",
    "GC_EVERY_N_PCTS   = 3\n",
    "GC_EVERY_N_GROUPS = 1\n",
    "\n",
    "META = [\"label\", \"setup\", \"run_id\"]\n",
    "DROOP_META_COLS = [\"droop_center_found\", \"droop_best_score_z\", \"droop_frac_ge_thr\", \"droop_vcols\"]\n",
    "\n",
    "DROP_LOW_VARIANCE_COLS = True\n",
    "LOW_VAR_EPS = 1e-10\n",
    "\n",
    "# DROOP boost (small, optional)\n",
    "DROOP_BOOST = True\n",
    "DROOP_BOOST_ALPHA = 0.20\n",
    "DROOP_TRANSIENT_PAT = re.compile(r\"__(drop|range|slope|min|max|std)\", re.I)\n",
    "\n",
    "METHOD_ORDER = [\"dC_aJ\", \"dC_aM\", \"dE_aJ\", \"dE_aM\"]\n",
    "\n",
    "# ===============================================================================================\n",
    "# 2) PLOT CONFIG\n",
    "# ===============================================================================================\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 13.5,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"figure.titlesize\": 20,\n",
    "})\n",
    "\n",
    "METHOD_TITLE = {\n",
    "    \"dC_aJ\": r\"Scoring function $d_C(X)$ with Aggregate $a_J$\",\n",
    "    \"dC_aM\": r\"Scoring function $d_C(X)$ with Aggregate $a_M$\",\n",
    "    \"dE_aJ\": r\"Scoring function $d_E(X)$ with Aggregate $a_J$\",\n",
    "    \"dE_aM\": r\"Scoring function $d_E(X)$ with Aggregate $a_M$\",\n",
    "}\n",
    "\n",
    "# ---- Data never exceeds 1.00 ----\n",
    "Y_DATA_MAX = 1.00\n",
    "Y_DATA_MIN = 0.00\n",
    "\n",
    "# ---- Display headroom so points at 1.00 aren't cut off ----\n",
    "Y_DISPLAY_MAX = 1.02      # axis can go slightly above 1.00\n",
    "Y_HEADROOM = 0.015        # if ymax is near 1.00, ensure at least this much headroom\n",
    "\n",
    "# y-axis auto-range per subplot\n",
    "Y_AUTO     = True\n",
    "Y_PAD_FRAC = 0.06\n",
    "Y_MIN_SPAN = 0.04\n",
    "\n",
    "ALPHA_MINMAX = 0.28\n",
    "ALPHA_IQR    = 0.60\n",
    "GRID_ALPHA   = 0.22\n",
    "GRID_LS      = \"--\"\n",
    "GRID_LW      = 0.6\n",
    "\n",
    "FIGSIZE = (13.6, 8.4)\n",
    "WSPACE  = 0.30\n",
    "HSPACE  = 0.42\n",
    "TOP     = 0.88\n",
    "BOTTOM  = 0.20\n",
    "LEFT    = 0.075\n",
    "RIGHT   = 0.985\n",
    "SUPTITLE_Y = 0.975\n",
    "LEGEND_Y   = 0.055\n",
    "\n",
    "BAND_EPS = 0.008\n",
    "def _ensure_visible_band(lo: np.ndarray, hi: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    lo = lo.astype(float).copy()\n",
    "    hi = hi.astype(float).copy()\n",
    "    flat = (np.abs(hi - lo) < 1e-12)\n",
    "    if np.any(flat):\n",
    "        lo2 = lo - BAND_EPS/2\n",
    "        hi2 = hi + BAND_EPS/2\n",
    "        lo[flat] = np.clip(lo2[flat], Y_DATA_MIN, Y_DATA_MAX)\n",
    "        hi[flat] = np.clip(hi2[flat], Y_DATA_MIN, Y_DATA_MAX)\n",
    "    return lo, hi\n",
    "\n",
    "def _clip_metric(a):\n",
    "    \"\"\"Hard clip metrics (data) to [0,1].\"\"\"\n",
    "    return np.clip(np.asarray(a, float), Y_DATA_MIN, Y_DATA_MAX)\n",
    "\n",
    "def _auto_ylim_from_arrays(arrs: List[np.ndarray]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Tight ylim based on plotted data bands/median.\n",
    "    Axis can extend above 1.00 up to 1.02 for visual headroom, but data are clipped at 1.00.\n",
    "    \"\"\"\n",
    "    vals = []\n",
    "    for a in arrs:\n",
    "        if a is None:\n",
    "            continue\n",
    "        a = np.asarray(a, float).ravel()\n",
    "        a = a[np.isfinite(a)]\n",
    "        if a.size:\n",
    "            vals.append(a)\n",
    "    if not vals:\n",
    "        return (Y_DATA_MIN, Y_DISPLAY_MAX)\n",
    "\n",
    "    v = np.concatenate(vals)\n",
    "    ymin = float(np.min(v))\n",
    "    ymax = float(np.max(v))\n",
    "    if not np.isfinite(ymin) or not np.isfinite(ymax):\n",
    "        return (Y_DATA_MIN, Y_DISPLAY_MAX)\n",
    "\n",
    "    # base padding\n",
    "    span = max(ymax - ymin, Y_MIN_SPAN)\n",
    "    pad = Y_PAD_FRAC * span\n",
    "    ymin2 = float(np.clip(ymin - pad, Y_DATA_MIN, Y_DATA_MAX))\n",
    "    ymax2 = float(np.clip(ymax + pad, Y_DATA_MIN, Y_DATA_MAX))\n",
    "\n",
    "    # enforce minimal span\n",
    "    if ymax2 - ymin2 < Y_MIN_SPAN:\n",
    "        mid = 0.5 * (ymin2 + ymax2)\n",
    "        ymin2 = float(np.clip(mid - Y_MIN_SPAN/2, Y_DATA_MIN, Y_DATA_MAX))\n",
    "        ymax2 = float(np.clip(mid + Y_MIN_SPAN/2, Y_DATA_MIN, Y_DATA_MAX))\n",
    "\n",
    "    # ---- headroom: if we are close to 1.00, allow y-axis to go slightly above 1.00 (up to 1.02)\n",
    "    # This prevents markers/lines at 1.00 from being clipped by the axis boundary.\n",
    "    if ymax2 >= (Y_DATA_MAX - 1e-6):\n",
    "        ymax_display = min(Y_DISPLAY_MAX, Y_DATA_MAX + Y_HEADROOM)\n",
    "        ymax2 = max(ymax2, ymax_display)\n",
    "    else:\n",
    "        # still allow a tiny pad beyond ymax (but not beyond 1.02)\n",
    "        ymax2 = min(Y_DISPLAY_MAX, ymax2)\n",
    "\n",
    "    # lower bound should never go below 0\n",
    "    ymin2 = max(Y_DATA_MIN, ymin2)\n",
    "\n",
    "    return ymin2, ymax2\n",
    "\n",
    "# ===============================================================================================\n",
    "# 3) WEIGHT CANDIDATES (Table-3 + extras) + LOOKUP\n",
    "# ===============================================================================================\n",
    "def build_weight_table() -> pd.DataFrame:\n",
    "    paper_cases = [\n",
    "        (\"T3_01\", \"Case 1\",  1,    0,    0),\n",
    "        (\"T3_02\", \"Case 2\",  0,    1,    0),\n",
    "        (\"T3_03\", \"Case 3\",  0,    0,    1),\n",
    "        (\"T3_04\", \"Case 4\",  1/3,  1/3,  1/3),\n",
    "        (\"T3_05\", \"Case 5\",  1/4,  1/4,  2/4),\n",
    "        (\"T3_06\", \"Case 6\",  1/5,  1/5,  3/5),\n",
    "        (\"T3_07\", \"Case 7\",  1/6,  1/6,  4/6),\n",
    "        (\"T3_08\", \"Case 8\",  1/8,  2/8,  5/8),\n",
    "        (\"T3_09\", \"Case 9\",  1/8,  1/8,  6/8),\n",
    "        (\"T3_10\", \"Case 10\", 1/10, 1/10, 8/10),\n",
    "        (\"T3_11\", \"Case 11\", 2/3,  1/3,  1/3),\n",
    "        (\"T3_12\", \"Case 12\", 3/4,  1/4,  1/4),\n",
    "        (\"T3_13\", \"Case 13\", 5/8,  2/8,  1/8),\n",
    "        (\"T3_14\", \"Case 14\", 6/8,  1/8,  1/8),\n",
    "        (\"T3_15\", \"Case 15\", 1/20, 1/20, 18/20),\n",
    "        (\"T3_16\", \"Case 16\", 1/40, 1/40, 38/40),\n",
    "    ]\n",
    "    rows = []\n",
    "    for cid, name, wM, wC, wS in paper_cases:\n",
    "        rows.append({\"weight_case_id\": cid, \"weight_case_name\": name, \"wC\": float(wC), \"wM\": float(wM), \"wS\": float(wS), \"source\": \"Table3\"})\n",
    "    extras = [\n",
    "        (\"EX_01\", \"SensorOnly\", 0.0, 0.0, 1.0),\n",
    "        (\"EX_02\", \"Sensor90\",   0.05, 0.05, 0.90),\n",
    "        (\"EX_03\", \"Sensor95\",   0.025, 0.025, 0.95),\n",
    "        (\"EX_04\", \"Sensor98\",   0.01, 0.01, 0.98),\n",
    "    ]\n",
    "    for cid, name, wC, wM, wS in extras:\n",
    "        rows.append({\"weight_case_id\": cid, \"weight_case_name\": name, \"wC\": float(wC), \"wM\": float(wM), \"wS\": float(wS), \"source\": \"Extra\"})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"_key\"] = df.apply(lambda r: (round(r[\"wC\"], 6), round(r[\"wM\"], 6), round(r[\"wS\"], 6)), axis=1)\n",
    "    df = df.drop_duplicates(\"_key\").drop(columns=[\"_key\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "WEIGHT_TABLE = build_weight_table()\n",
    "\n",
    "def _weight_lookup(w: Tuple[float,float,float]) -> Tuple[str,str,str]:\n",
    "    wC,wM,wS = (round(float(w[0]),6), round(float(w[1]),6), round(float(w[2]),6))\n",
    "    m = WEIGHT_TABLE[(WEIGHT_TABLE[\"wC\"].round(6)==wC) & (WEIGHT_TABLE[\"wM\"].round(6)==wM) & (WEIGHT_TABLE[\"wS\"].round(6)==wS)]\n",
    "    if len(m)>0:\n",
    "        r = m.iloc[0]\n",
    "        return str(r[\"weight_case_id\"]), str(r[\"weight_case_name\"]), str(r[\"source\"])\n",
    "    return (\"CUSTOM\",\"Custom\",\"Search\")\n",
    "\n",
    "# ===============================================================================================\n",
    "# 4) IO + workload parsing\n",
    "# ===============================================================================================\n",
    "RUNID2PATH: Dict[str,str] = {}\n",
    "\n",
    "def detect_setup_from_path(p: Path):\n",
    "    s = str(p).lower()\n",
    "    if \"ddr4\" in s: return \"DDR4\"\n",
    "    if \"ddr5\" in s: return \"DDR5\"\n",
    "    return None\n",
    "\n",
    "def is_benign_path(p: Path) -> bool:\n",
    "    s = str(p).lower()\n",
    "    if \"benign\" in s:\n",
    "        return True\n",
    "    bad = [\"attack\",\"anom\",\"fault\",\"inject\",\"trojan\",\"mal\",\"rh\",\"droop\",\"spectre\",\"trrespass\"]\n",
    "    return not any(b in s for b in bad)\n",
    "\n",
    "def is_anomaly_path(p: Path, anomaly: str) -> bool:\n",
    "    return (anomaly.lower() in str(p).lower()) and (not is_benign_path(p))\n",
    "\n",
    "def iter_raw_csvs(root: Path):\n",
    "    for p in root.rglob(\"*.csv\"):\n",
    "        yield p\n",
    "\n",
    "def read_csv_clean(p: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(p)\n",
    "    return df.loc[:, ~df.columns.str.startswith(\"Unnamed\")]\n",
    "\n",
    "def mk_run_id(path: Path) -> str:\n",
    "    rid = f\"run_{hashlib.md5(str(path).encode('utf-8')).hexdigest()[:10]}\"\n",
    "    RUNID2PATH[rid] = str(path)\n",
    "    return rid\n",
    "\n",
    "_WORKLOADS = [\"dft\",\"dj\",\"dp\",\"gl\",\"gs\",\"ha\",\"ja\",\"mm\",\"ni\",\"oe\",\"pi\",\"sh\",\"tr\"]\n",
    "def workload_from_path(p: Path) -> str:\n",
    "    stem = (p.stem or \"\").lower()\n",
    "    m = re.search(r\"_([a-z]{2,3})$\", stem)\n",
    "    if m and m.group(1) in _WORKLOADS:\n",
    "        return m.group(1).upper()\n",
    "    for tok in _WORKLOADS:\n",
    "        if tok in stem:\n",
    "            return tok.upper()\n",
    "    return \"UNK\"\n",
    "\n",
    "def collect_raw_pairs_by_setup(data_dir: Path, which: str, anomaly: Optional[str] = None):\n",
    "    out = {\"DDR4\": [], \"DDR5\": []}\n",
    "    for p in iter_raw_csvs(data_dir):\n",
    "        setup = detect_setup_from_path(p)\n",
    "        if setup is None:\n",
    "            continue\n",
    "        try:\n",
    "            if which == \"benign\":\n",
    "                if not is_benign_path(p):\n",
    "                    continue\n",
    "            else:\n",
    "                if anomaly is None or not is_anomaly_path(p, anomaly):\n",
    "                    continue\n",
    "            df = read_csv_clean(p)\n",
    "            out[setup].append((p, df))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] read failed {p}: {e}\")\n",
    "    return out\n",
    "\n",
    "def telemetry_cols(df: pd.DataFrame):\n",
    "    exclude = set(META) | set(DROOP_META_COLS) | {\"workload\"}\n",
    "    return [c for c in df.columns if (c not in exclude) and (df[c].dtype.kind in \"fcbiu\")]\n",
    "\n",
    "def drop_low_variance_cols(df: pd.DataFrame, cols: List[str], eps: float = 1e-10) -> List[str]:\n",
    "    v = df[cols].astype(float).var(axis=0, ddof=0)\n",
    "    keep = v[v > eps].index.tolist()\n",
    "    return keep\n",
    "\n",
    "# ===============================================================================================\n",
    "# 5) Scaling\n",
    "# ===============================================================================================\n",
    "def robust_scale_train(Xb_np: np.ndarray, winsor=(2.0, 98.0)):\n",
    "    Q1, Q2 = np.percentile(Xb_np, winsor[0], axis=0), np.percentile(Xb_np, winsor[1], axis=0)\n",
    "    Xb_clip = np.clip(Xb_np, Q1, Q2)\n",
    "    mu = Xb_clip.mean(axis=0)\n",
    "    sd = Xb_clip.std(axis=0, ddof=0) + 1e-9\n",
    "    return mu, sd, Q1, Q2\n",
    "\n",
    "def apply_robust_scale(X: pd.DataFrame, mu, sd, Q1, Q2):\n",
    "    Xc = np.clip(X.to_numpy(dtype=float), Q1, Q2)\n",
    "    Z  = (Xc - mu) / sd\n",
    "    return pd.DataFrame(Z, columns=X.columns, index=X.index)\n",
    "\n",
    "# ===============================================================================================\n",
    "# 6) Windowing (adds sensor transient feats incl CPU Voltage)\n",
    "# ===============================================================================================\n",
    "SENSOR_PAT = re.compile(r\"cpu\\s*voltage|volt|vdd|vcore|vin|vout|power|energy|joule|current|amps?|temp|thermal|hot\", re.I)\n",
    "\n",
    "def window_collapse_means(df: pd.DataFrame, win: int, setup: str, run_id: str, label: str,\n",
    "                          overlap_ratio: float, src_path: str=\"\") -> pd.DataFrame:\n",
    "    cols_all = telemetry_cols(df)\n",
    "    if not cols_all:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    sensor_cols = [c for c in cols_all if SENSOR_PAT.search(c)]\n",
    "    if \"CPU Voltage\" in cols_all and \"CPU Voltage\" not in sensor_cols:\n",
    "        sensor_cols.append(\"CPU Voltage\")\n",
    "\n",
    "    n = len(df)\n",
    "    stride = max(1, int(round(win * (1 - overlap_ratio))))\n",
    "    starts = list(range(0, n - win + 1, stride))\n",
    "    if not starts:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for start in starts:\n",
    "        chunk = df.iloc[start:start+win]\n",
    "        X = chunk[cols_all].astype(float)\n",
    "        means = X.mean(axis=0, numeric_only=True)\n",
    "\n",
    "        tfeat = {}\n",
    "        if sensor_cols:\n",
    "            Xs = X[sensor_cols]\n",
    "            mins  = Xs.min(axis=0)\n",
    "            maxs  = Xs.max(axis=0)\n",
    "            stds  = Xs.std(axis=0, ddof=0)\n",
    "            means_s = Xs.mean(axis=0)\n",
    "            drops  = (means_s - mins)\n",
    "            ranges = (maxs - mins)\n",
    "            slope  = (Xs.iloc[-1] - Xs.iloc[0])\n",
    "            for c in sensor_cols:\n",
    "                tfeat[f\"{c}__min\"]   = float(mins[c])\n",
    "                tfeat[f\"{c}__max\"]   = float(maxs[c])\n",
    "                tfeat[f\"{c}__std\"]   = float(stds[c])\n",
    "                tfeat[f\"{c}__drop\"]  = float(drops[c])\n",
    "                tfeat[f\"{c}__range\"] = float(ranges[c])\n",
    "                tfeat[f\"{c}__slope\"] = float(slope[c])\n",
    "\n",
    "        row = pd.concat([means, pd.Series(tfeat)]).to_frame().T\n",
    "        row[\"setup\"]    = setup\n",
    "        row[\"run_id\"]   = run_id\n",
    "        row[\"label\"]    = label\n",
    "        row[\"workload\"] = workload_from_path(Path(src_path))\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "def build_windowed_raw_means(pairs, setup: str, win: int, label: str, overlap_ratio: float) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for p, df in pairs:\n",
    "        rid = mk_run_id(p)\n",
    "        agg = window_collapse_means(df, win=win, setup=setup, run_id=rid, label=label,\n",
    "                                    overlap_ratio=overlap_ratio, src_path=str(p))\n",
    "        if not agg.empty:\n",
    "            out.append(agg)\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "# ===============================================================================================\n",
    "# 7) Rank lists + robust mapping + CPU Voltage family append for DROOP\n",
    "# ===============================================================================================\n",
    "def _read_rank_feats(p: Path):\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "    except Exception:\n",
    "        return []\n",
    "    if df.empty:\n",
    "        return []\n",
    "    col = \"feature\" if \"feature\" in df.columns else df.columns[0]\n",
    "    return df[col].dropna().astype(str).tolist()\n",
    "\n",
    "def load_full_rank_lists(setup: str, win: int, kfold: int):\n",
    "    def _find_file(setup, win, kfold, sub):\n",
    "        fname = f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "        for d in RANK_DIRS:\n",
    "            pp = d / fname\n",
    "            if pp.exists():\n",
    "                return pp\n",
    "        return None\n",
    "    out = {}\n",
    "    for sub in [\"compute\",\"memory\",\"sensors\"]:\n",
    "        pp = _find_file(setup, win, kfold, sub)\n",
    "        out[sub] = _read_rank_feats(pp) if pp else []\n",
    "    return out\n",
    "\n",
    "def slice_by_percent(full_lists: Dict[str, List[str]], pct: int):\n",
    "    sel = {}\n",
    "    frac = pct / 100.0\n",
    "    for sub in (\"compute\",\"memory\",\"sensors\"):\n",
    "        feats = full_lists.get(sub, []) or []\n",
    "        k = int(np.ceil(len(feats) * frac))\n",
    "        if frac > 0 and len(feats) > 0:\n",
    "            k = max(1, k)\n",
    "        sel[sub] = feats[:min(len(feats), max(0, k))]\n",
    "    return sel\n",
    "\n",
    "ALIAS_MAP = {\n",
    "    \"voltage\": [\"cpu\",\"voltage\",\"volt\",\"vcore\",\"vdd\",\"vin\",\"vout\"],\n",
    "    \"power\": [\"power\",\"energy\",\"joules\",\"current\",\"amps\"],\n",
    "    \"temperature\": [\"temp\",\"thermal\",\"hot\"],\n",
    "    \"frequency\": [\"freq\",\"afreq\",\"cfreq\",\"clock\",\"clk\",\"mhz\",\"ghz\"],\n",
    "    \"ipc\": [\"ipc\",\"physipc\"],\n",
    "    \"cache\": [\"l1\",\"l2\",\"l3\",\"hit\",\"miss\",\"evict\",\"fill\",\"mpi\"],\n",
    "    \"bandwidth\": [\"read\",\"write\",\"bw\",\"bandwidth\"],\n",
    "}\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s)).lower()\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        out.append(ch if ch.isalnum() else \"_\")\n",
    "    s = \"\".join(out)\n",
    "    while \"__\" in s:\n",
    "        s = s.replace(\"__\",\"_\")\n",
    "    return s.strip(\"_\")\n",
    "\n",
    "def _tokens(s: str):\n",
    "    t = _norm(s).replace(\"_\",\" \").split()\n",
    "    exp = []\n",
    "    for tok in t:\n",
    "        exp.append(tok)\n",
    "        for k, aliases in ALIAS_MAP.items():\n",
    "            if tok in aliases:\n",
    "                exp.append(k)\n",
    "    return set(exp)\n",
    "\n",
    "def build_column_index(cols: List[str]):\n",
    "    norm2orig = {}\n",
    "    tok_index = {}\n",
    "    for c in cols:\n",
    "        nc = _norm(c)\n",
    "        norm2orig.setdefault(nc, c)\n",
    "        tok_index[c] = _tokens(c)\n",
    "    return norm2orig, tok_index\n",
    "\n",
    "def map_ranks_to_existing(ranked_list: List[str], norm2orig, tok_index):\n",
    "    mapped, seen = [], set()\n",
    "    for r in ranked_list:\n",
    "        nr = _norm(r)\n",
    "        cand = norm2orig.get(nr, None)\n",
    "        if cand and cand not in seen:\n",
    "            mapped.append(cand); seen.add(cand); continue\n",
    "        rtoks = _tokens(r)\n",
    "        best_c, best_j = None, 0.0\n",
    "        for c, ctoks in tok_index.items():\n",
    "            inter = len(rtoks & ctoks)\n",
    "            union = len(rtoks | ctoks) or 1\n",
    "            j = inter / union\n",
    "            if j > best_j:\n",
    "                best_j, best_c = j, c\n",
    "        if best_c and best_j >= 0.60 and best_c not in seen:\n",
    "            mapped.append(best_c); seen.add(best_c)\n",
    "    return mapped\n",
    "\n",
    "def intersect_selection_with_columns_robust(sel_raw: Dict[str, List[str]], xb_cols: set):\n",
    "    norm2orig, tok_index = build_column_index(list(xb_cols))\n",
    "    out = {}\n",
    "    for sub in (\"compute\",\"memory\",\"sensors\"):\n",
    "        ranked = sel_raw.get(sub, []) or []\n",
    "        mapped = map_ranks_to_existing(ranked, norm2orig, tok_index)\n",
    "        out[sub] = [c for c in mapped if c in xb_cols]\n",
    "    return out\n",
    "\n",
    "def force_cpu_voltage_family(sel: Dict[str, List[str]], xb_cols: set) -> Dict[str, List[str]]:\n",
    "    out = dict(sel)\n",
    "    sensors_now = out.get(\"sensors\", []) or []\n",
    "    must = []\n",
    "    if \"CPU Voltage\" in xb_cols:\n",
    "        must.append(\"CPU Voltage\")\n",
    "    for suf in [\"__min\",\"__max\",\"__std\",\"__drop\",\"__range\",\"__slope\"]:\n",
    "        c = f\"CPU Voltage{suf}\"\n",
    "        if c in xb_cols:\n",
    "            must.append(c)\n",
    "    out[\"sensors\"] = list(dict.fromkeys(sensors_now + must))\n",
    "    return out\n",
    "\n",
    "# ===============================================================================================\n",
    "# 8) Balanced evaluation helper\n",
    "# ===============================================================================================\n",
    "def balanced_concat(df_ben: pd.DataFrame, df_anom: pd.DataFrame, seed: int = 1337):\n",
    "    if df_ben is None or df_ben.empty:\n",
    "        raise ValueError(\"balanced_concat(): df_ben is empty\")\n",
    "    if df_anom is None or df_anom.empty:\n",
    "        raise ValueError(\"balanced_concat(): df_anom is empty\")\n",
    "\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "    nb = int(len(df_ben))\n",
    "    na = int(len(df_anom))\n",
    "    n  = int(min(nb, na))\n",
    "\n",
    "    idx_b = rng.choice(nb, size=n, replace=(nb < n))\n",
    "    idx_a = rng.choice(na, size=n, replace=(na < n))\n",
    "\n",
    "    ben_s = df_ben.iloc[idx_b].copy().reset_index(drop=True)\n",
    "    anm_s = df_anom.iloc[idx_a].copy().reset_index(drop=True)\n",
    "\n",
    "    df_eval = pd.concat([ben_s, anm_s], ignore_index=True)\n",
    "    y_true  = np.concatenate([np.zeros(len(ben_s), dtype=int),\n",
    "                              np.ones(len(anm_s), dtype=int)])\n",
    "    return df_eval, y_true\n",
    "\n",
    "# ===============================================================================================\n",
    "# 9) Weight selection helpers\n",
    "# ===============================================================================================\n",
    "def split_holdout(y, test_size=0.3, seed=SEED):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    idx = np.arange(len(y))\n",
    "    _, val_idx = next(sss.split(idx, y))\n",
    "    return val_idx\n",
    "\n",
    "def pick_best_w_per_method_PRfirst(y_true: np.ndarray, scores_by_w: Dict[Tuple[float,float,float], Dict[str,np.ndarray]], val_idx: np.ndarray):\n",
    "    yy = y_true[val_idx]\n",
    "    best = {m: (-1.0, -1.0, -1.0, None) for m in [\"dE_aM\",\"dE_aJ\",\"dC_aM\",\"dC_aJ\"]}\n",
    "    for w, md in scores_by_w.items():\n",
    "        wS = float(w[2])\n",
    "        for m, s in md.items():\n",
    "            sv = np.asarray(s)[val_idx]\n",
    "            pr  = float(average_precision_score(yy, sv))\n",
    "            roc = float(roc_auc_score(yy, sv))\n",
    "            key = (pr, roc, wS)\n",
    "            if key > best[m][:3]:\n",
    "                best[m] = (pr, roc, wS, w)\n",
    "    return {m: best[m][3] for m in best}\n",
    "\n",
    "# ===============================================================================================\n",
    "# 10) DSE plots\n",
    "# ===============================================================================================\n",
    "def summarize_for_dse(per_run_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"pct\"]\n",
    "    def q1(x): return float(np.nanpercentile(x, 25))\n",
    "    def q3(x): return float(np.nanpercentile(x, 75))\n",
    "    g = (per_run_df.groupby(keys, as_index=False)\n",
    "            .agg(\n",
    "                roc_auc_median=(\"roc_auc\",\"median\"),\n",
    "                roc_auc_q1=(\"roc_auc\", q1),\n",
    "                roc_auc_q3=(\"roc_auc\", q3),\n",
    "                roc_auc_min=(\"roc_auc\",\"min\"),\n",
    "                roc_auc_max=(\"roc_auc\",\"max\"),\n",
    "                auc_pr_median=(\"auc_pr\",\"median\"),\n",
    "                auc_pr_q1=(\"auc_pr\", q1),\n",
    "                auc_pr_q3=(\"auc_pr\", q3),\n",
    "                auc_pr_min=(\"auc_pr\",\"min\"),\n",
    "                auc_pr_max=(\"auc_pr\",\"max\"),\n",
    "                n_runs=(\"run_id\",\"nunique\"),\n",
    "            ))\n",
    "    return g.sort_values(keys).reset_index(drop=True)\n",
    "\n",
    "def plot_dse_grid(summary_case: pd.DataFrame, metric: str, setup: str, anomaly: str,\n",
    "                  win: int, kfold: int, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    x_ticks = sorted(summary_case[\"pct\"].unique().tolist())\n",
    "    if not x_ticks:\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=FIGSIZE, sharex=False, sharey=False)\n",
    "    axes = axes.flatten()\n",
    "    fig.subplots_adjust(left=LEFT, right=RIGHT, top=TOP, bottom=BOTTOM, wspace=WSPACE, hspace=HSPACE)\n",
    "\n",
    "    xlab = \"Top-ranked features used (%)\"\n",
    "    ylab = \"ROC-AUC\" if metric == \"roc_auc\" else \"AUC-PR\"\n",
    "\n",
    "    for i, method in enumerate(METHOD_ORDER):\n",
    "        ax = axes[i]\n",
    "        ax.set_title(METHOD_TITLE.get(method, method), pad=7)\n",
    "        ax.set_xlim(min(x_ticks), max(x_ticks))\n",
    "        ax.set_xticks(x_ticks)\n",
    "        ax.grid(True, alpha=GRID_ALPHA, linestyle=GRID_LS, linewidth=GRID_LW)\n",
    "\n",
    "        sub = summary_case[summary_case[\"method\"] == method].copy().sort_values(\"pct\")\n",
    "        if sub.empty:\n",
    "            ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "            ax.set_ylim(Y_DATA_MIN, Y_DISPLAY_MAX)\n",
    "        else:\n",
    "            x    = sub[\"pct\"].to_numpy(dtype=float)\n",
    "            med  = _clip_metric(sub[f\"{metric}_median\"].to_numpy(dtype=float))\n",
    "            q1   = _clip_metric(sub[f\"{metric}_q1\"].to_numpy(dtype=float))\n",
    "            q3   = _clip_metric(sub[f\"{metric}_q3\"].to_numpy(dtype=float))\n",
    "            vmin = _clip_metric(sub[f\"{metric}_min\"].to_numpy(dtype=float))\n",
    "            vmax = _clip_metric(sub[f\"{metric}_max\"].to_numpy(dtype=float))\n",
    "\n",
    "            lo_mm, hi_mm = np.minimum(vmin, vmax), np.maximum(vmin, vmax)\n",
    "            lo_iq, hi_iq = np.minimum(q1, q3),     np.maximum(q1, q3)\n",
    "            lo_mm, hi_mm = _ensure_visible_band(lo_mm, hi_mm)\n",
    "            lo_iq, hi_iq = _ensure_visible_band(lo_iq, hi_iq)\n",
    "\n",
    "            ax.fill_between(x, lo_mm, hi_mm, alpha=ALPHA_MINMAX, label=\"min-max\", linewidth=0.0)\n",
    "            ax.fill_between(x, lo_iq, hi_iq, alpha=ALPHA_IQR,   label=\"IQR\",     linewidth=0.0)\n",
    "            ax.plot(x, med, color=\"black\", marker=\"o\", linewidth=2.0, markersize=5.5, zorder=5, label=\"Median\")\n",
    "\n",
    "            if Y_AUTO:\n",
    "                ylo, yhi = _auto_ylim_from_arrays([lo_mm, hi_mm, lo_iq, hi_iq, med])\n",
    "                ax.set_ylim(ylo, yhi)\n",
    "            else:\n",
    "                ax.set_ylim(Y_DATA_MIN, Y_DISPLAY_MAX)\n",
    "\n",
    "        ax.set_xlabel(xlab, labelpad=6)\n",
    "        ax.set_ylabel(ylab, labelpad=6)\n",
    "        ax.tick_params(axis=\"x\", labelbottom=True, pad=3)\n",
    "        ax.tick_params(axis=\"y\", labelleft=True, pad=3)\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"lower center\", bbox_to_anchor=(0.5, LEGEND_Y), ncol=3, frameon=False)\n",
    "    fig.suptitle(f\"{setup}: {anomaly} (WIN={win}, K={kfold})\", y=SUPTITLE_Y)\n",
    "\n",
    "    base = f\"DSE_{metric.upper()}_{setup}_{anomaly}_WIN{int(win)}_KF{int(kfold)}\"\n",
    "    fig.savefig(out_dir / f\"{base}.png\", dpi=600, bbox_inches=\"tight\", pad_inches=0.06)\n",
    "    fig.savefig(out_dir / f\"{base}.pdf\", bbox_inches=\"tight\", pad_inches=0.06)\n",
    "    plt.close(fig)\n",
    "\n",
    "def write_all_dse_plots(per_run_df: pd.DataFrame):\n",
    "    out_root = RES_DIR / \"DesignSpace\" / \"PaperStyle_ALLRUNS\"\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "    dse = summarize_for_dse(per_run_df)\n",
    "    for (setup, anomaly, win, kfold), sub in dse.groupby([\"setup\",\"anomaly\",\"win\",\"kfold\"]):\n",
    "        out_dir = out_root / str(setup) / str(anomaly) / f\"WIN{int(win)}_KF{int(kfold)}\"\n",
    "        plot_dse_grid(sub, \"roc_auc\", str(setup), str(anomaly), int(win), int(kfold), out_dir)\n",
    "        plot_dse_grid(sub, \"auc_pr\",  str(setup), str(anomaly), int(win), int(kfold), out_dir)\n",
    "\n",
    "# ===============================================================================================\n",
    "# 11) Subspace-weight tables (same as before; omitted here for brevity)\n",
    "# ===============================================================================================\n",
    "def build_workload_weight_tables(per_run_df: pd.DataFrame, setup: str, win: int, kfold: int) -> None:\n",
    "    # (same implementation you already use; keep unchanged)\n",
    "    base_dir = RES_DIR / \"Table_SubspaceWeights\" / setup / f\"WIN{win}_KF{kfold}\"\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rep_method = \"dE_aM\"\n",
    "    for anomaly in ANOMALIES_BY_SETUP[setup]:\n",
    "        df = per_run_df[\n",
    "            (per_run_df[\"setup\"]==setup) &\n",
    "            (per_run_df[\"anomaly\"]==anomaly) &\n",
    "            (per_run_df[\"win\"]==win) &\n",
    "            (per_run_df[\"kfold\"]==kfold)\n",
    "        ].copy()\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        df[\"_wkey\"] = df.apply(lambda r: (round(r[\"wC\"],6), round(r[\"wM\"],6), round(r[\"wS\"],6)), axis=1)\n",
    "        rows = []\n",
    "        for (wl, method), g in df.groupby([\"workload\",\"method\"]):\n",
    "            mode_key = g[\"_wkey\"].value_counts().idxmax()\n",
    "            wC, wM, wS = mode_key\n",
    "            wid, wname, wsrc = _weight_lookup((wC,wM,wS))\n",
    "            rows.append({\n",
    "                \"workload\": wl,\n",
    "                \"method\": method,\n",
    "                \"chosen_wC\": wC, \"chosen_wM\": wM, \"chosen_wS\": wS,\n",
    "                \"weight_case_id\": wid,\n",
    "                \"weight_case_name\": wname,\n",
    "                \"weight_case_source\": wsrc,\n",
    "                \"n_rows\": int(len(g)),\n",
    "                \"median_roc\": float(np.nanmedian(g[\"roc_auc\"])),\n",
    "                \"median_pr\": float(np.nanmedian(g[\"auc_pr\"])),\n",
    "            })\n",
    "        df_out = pd.DataFrame(rows).sort_values([\"workload\",\"method\"]).reset_index(drop=True)\n",
    "        (base_dir / f\"Table_SubspaceWeights_{setup}_WIN{win}_KF{kfold}_{anomaly}.csv\").write_text(df_out.to_csv(index=False))\n",
    "\n",
    "# ===============================================================================================\n",
    "# 12) Pipeline core (same logic as your current version; unchanged except DSE y-axis)\n",
    "# ===============================================================================================\n",
    "def run_full_pipeline() -> pd.DataFrame:\n",
    "    all_rows = []\n",
    "    group_counter = 0\n",
    "\n",
    "    benign_pairs = collect_raw_pairs_by_setup(DATA_DIR, which=\"benign\")\n",
    "    anomaly_pairs_cache: Dict[Tuple[str,str], List[Tuple[Path,pd.DataFrame]]] = {}\n",
    "\n",
    "    def get_anom_pairs(setup: str, anomaly: str):\n",
    "        key = (setup, anomaly.upper())\n",
    "        if key not in anomaly_pairs_cache:\n",
    "            anomaly_pairs_cache[key] = collect_raw_pairs_by_setup(DATA_DIR, which=\"anomaly\", anomaly=anomaly)[setup]\n",
    "        return anomaly_pairs_cache[key]\n",
    "\n",
    "    for setup in SETUPS:\n",
    "        anomalies = ANOMALIES_BY_SETUP.get(setup, [])\n",
    "        ben_pairs_all = benign_pairs.get(setup, [])\n",
    "        if not ben_pairs_all:\n",
    "            print(f\"[SKIP] {setup}: no benign RAW files\")\n",
    "            continue\n",
    "\n",
    "        for win in WINS:\n",
    "            df_ben_all = build_windowed_raw_means(ben_pairs_all, setup=setup, win=win, label=\"BENIGN\",\n",
    "                                                  overlap_ratio=OVERLAP_RATIO)\n",
    "            if df_ben_all.empty:\n",
    "                print(f\"[SKIP] {setup} WIN={win}: benign windowed empty\")\n",
    "                continue\n",
    "\n",
    "            Xb_cols = telemetry_cols(df_ben_all)\n",
    "            if DROP_LOW_VARIANCE_COLS:\n",
    "                Xb_cols = drop_low_variance_cols(df_ben_all, Xb_cols, eps=LOW_VAR_EPS)\n",
    "            if not Xb_cols:\n",
    "                print(f\"[SKIP] {setup} WIN={win}: no telemetry cols after low-var drop\")\n",
    "                continue\n",
    "\n",
    "            Xb = df_ben_all[Xb_cols].astype(float)\n",
    "            mu_s, sd_s, Q1, Q2 = robust_scale_train(Xb.values, winsor=ROBUST_WINSOR)\n",
    "            Xb_base = apply_robust_scale(Xb, mu_s, sd_s, Q1, Q2)\n",
    "            xb_cols_set = set(Xb_base.columns)\n",
    "\n",
    "            ben_by_wl = {}\n",
    "            for wl in df_ben_all[\"workload\"].astype(str).unique():\n",
    "                ben_by_wl[wl] = df_ben_all[df_ben_all[\"workload\"].astype(str)==wl].copy()\n",
    "\n",
    "            for kfold in KFOLDS:\n",
    "                wdir = RES_DIR / \"Table_SubspaceWeights\" / setup / f\"WIN{win}_KF{kfold}\"\n",
    "                wdir.mkdir(parents=True, exist_ok=True)\n",
    "                WEIGHT_TABLE.to_csv(wdir / \"Table_SubspaceWeights_CANDIDATES.csv\", index=False)\n",
    "\n",
    "                full_lists = load_full_rank_lists(setup, win, kfold)\n",
    "\n",
    "                for anomaly in anomalies:\n",
    "                    group_counter += 1\n",
    "                    print(f\"\\n[RUN] {setup} {anomaly} WIN={win} KF={kfold} (group {group_counter})\")\n",
    "\n",
    "                    overlap = OVERLAP_RATIO_DROOP if anomaly.upper() == \"DROOP\" else OVERLAP_RATIO\n",
    "                    an_pairs_all = get_anom_pairs(setup, anomaly)\n",
    "                    if not an_pairs_all:\n",
    "                        print(f\"[SKIP] {setup} {anomaly}: no anomaly RAW files\")\n",
    "                        continue\n",
    "\n",
    "                    df_anom = build_windowed_raw_means(an_pairs_all, setup=setup, win=win, label=anomaly,\n",
    "                                                       overlap_ratio=overlap)\n",
    "                    if df_anom.empty:\n",
    "                        print(f\"[SKIP] {setup} {anomaly} WIN={win}: anomaly windowed empty\")\n",
    "                        continue\n",
    "\n",
    "                    run_ids = sorted(df_anom[\"run_id\"].astype(str).unique())\n",
    "\n",
    "                    for rid in run_ids:\n",
    "                        dfa_run = df_anom[df_anom[\"run_id\"].astype(str) == rid].copy()\n",
    "                        if dfa_run.empty:\n",
    "                            continue\n",
    "\n",
    "                        wl = str(dfa_run[\"workload\"].iloc[0]) if \"workload\" in dfa_run.columns else \"UNK\"\n",
    "                        df_ben = ben_by_wl.get(wl, df_ben_all)\n",
    "                        if df_ben.empty:\n",
    "                            df_ben = df_ben_all\n",
    "\n",
    "                        df_eval, y_true = balanced_concat(df_ben, dfa_run, seed=SEED)\n",
    "\n",
    "                        Xe_all  = df_eval[Xb_cols].astype(float)\n",
    "                        Xe_base = apply_robust_scale(Xe_all, mu_s, sd_s, Q1, Q2)\n",
    "\n",
    "                        for idx_p, pct in enumerate(PCT_SWEEP, start=1):\n",
    "                            sel_raw = slice_by_percent(full_lists, pct)\n",
    "                            sel = intersect_selection_with_columns_robust(sel_raw, xb_cols_set)\n",
    "                            if (not sel.get(\"compute\")) or (not sel.get(\"memory\")) or (not sel.get(\"sensors\")):\n",
    "                                continue\n",
    "                            if anomaly.upper() == \"DROOP\":\n",
    "                                sel = force_cpu_voltage_family(sel, xb_cols_set)\n",
    "\n",
    "                            refs = _build_refs(Xb_base, sel, eps=1e-12)\n",
    "                            benign_norm = fit_parts_normalizer_on_benign(Xb_base, refs)\n",
    "\n",
    "                            val_idx = split_holdout(y_true, test_size=0.3, seed=SEED)\n",
    "\n",
    "                            scores_by_w = {}\n",
    "                            for _, wr in WEIGHT_TABLE.iterrows():\n",
    "                                w = (float(wr[\"wC\"]), float(wr[\"wM\"]), float(wr[\"wS\"]))\n",
    "                                scores_by_w[w] = score_four_methods_benign_norm(Xe_base, refs, w, benign_norm)\n",
    "\n",
    "                            best_w_for = pick_best_w_per_method_PRfirst(y_true, scores_by_w, val_idx)\n",
    "\n",
    "                            for method in METHOD_ORDER:\n",
    "                                w_star = best_w_for[method]\n",
    "                                y_score = score_four_methods_benign_norm(Xe_base, refs, w_star, benign_norm)[method]\n",
    "                                y_score = np.nan_to_num(y_score, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                                if DROOP_BOOST and anomaly.upper() == \"DROOP\":\n",
    "                                    boost = droop_boost_score(Xe_base, sel)\n",
    "                                    y_score = (1.0 - DROOP_BOOST_ALPHA) * y_score + DROOP_BOOST_ALPHA * boost\n",
    "\n",
    "                                roc = float(np.clip(roc_auc_score(y_true, y_score), Y_DATA_MIN, Y_DATA_MAX))\n",
    "                                pr  = float(np.clip(average_precision_score(y_true, y_score), Y_DATA_MIN, Y_DATA_MAX))\n",
    "\n",
    "                                wid, wname, wsrc = _weight_lookup(w_star)\n",
    "\n",
    "                                all_rows.append({\n",
    "                                    \"setup\": setup,\n",
    "                                    \"anomaly\": anomaly,\n",
    "                                    \"win\": int(win),\n",
    "                                    \"kfold\": int(kfold),\n",
    "                                    \"pct\": int(pct),\n",
    "                                    \"run_id\": str(rid),\n",
    "                                    \"workload\": str(wl),\n",
    "                                    \"method\": method,\n",
    "                                    \"roc_auc\": roc,\n",
    "                                    \"auc_pr\": pr,\n",
    "                                    \"weight_case_id\": wid,\n",
    "                                    \"weight_case_name\": wname,\n",
    "                                    \"weight_case_source\": wsrc,\n",
    "                                    \"wC\": float(w_star[0]),\n",
    "                                    \"wM\": float(w_star[1]),\n",
    "                                    \"wS\": float(w_star[2]),\n",
    "                                })\n",
    "\n",
    "                            if idx_p % GC_EVERY_N_PCTS == 0:\n",
    "                                gc.collect()\n",
    "\n",
    "                    if group_counter % GC_EVERY_N_GROUPS == 0:\n",
    "                        gc.collect()\n",
    "\n",
    "    per_run_df = pd.DataFrame(all_rows)\n",
    "    out_all = RES_DIR / \"per_run_metrics_all_PIPELINE.csv\"\n",
    "    per_run_df.to_csv(out_all, index=False)\n",
    "    print(f\"\\n[OK] ALLRUNS metrics → {out_all}\")\n",
    "\n",
    "    for setup in SETUPS:\n",
    "        for win in WINS:\n",
    "            for kfold in KFOLDS:\n",
    "                build_workload_weight_tables(per_run_df, setup=setup, win=win, kfold=kfold)\n",
    "\n",
    "    write_all_dse_plots(per_run_df)\n",
    "    return per_run_df\n",
    "\n",
    "# ===============================================================================================\n",
    "# 13) MAIN\n",
    "# ===============================================================================================\n",
    "def main():\n",
    "    run_full_pipeline()\n",
    "    print(\"[DONE] Pipeline + DSE plots + weight tables per platform\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1f57bfed-9843-4a54-a377-de3a81300cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Cannot write to /Volumes/Untitled/octaneX_results. Using local: /Users/hsiaopingni/octaneX_v7_4functions/Results\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=32 KF=3 (group 1)\n",
      "\n",
      "[RUN] DDR4 RH WIN=32 KF=3 (group 2)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=32 KF=5 (group 3)\n",
      "\n",
      "[RUN] DDR4 RH WIN=32 KF=5 (group 4)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=32 KF=10 (group 5)\n",
      "\n",
      "[RUN] DDR4 RH WIN=32 KF=10 (group 6)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=64 KF=3 (group 7)\n",
      "\n",
      "[RUN] DDR4 RH WIN=64 KF=3 (group 8)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=64 KF=5 (group 9)\n",
      "\n",
      "[RUN] DDR4 RH WIN=64 KF=5 (group 10)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=64 KF=10 (group 11)\n",
      "\n",
      "[RUN] DDR4 RH WIN=64 KF=10 (group 12)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=128 KF=3 (group 13)\n",
      "\n",
      "[RUN] DDR4 RH WIN=128 KF=3 (group 14)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=128 KF=5 (group 15)\n",
      "\n",
      "[RUN] DDR4 RH WIN=128 KF=5 (group 16)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=128 KF=10 (group 17)\n",
      "\n",
      "[RUN] DDR4 RH WIN=128 KF=10 (group 18)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=512 KF=3 (group 19)\n",
      "\n",
      "[RUN] DDR4 RH WIN=512 KF=3 (group 20)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=512 KF=5 (group 21)\n",
      "\n",
      "[RUN] DDR4 RH WIN=512 KF=5 (group 22)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=512 KF=10 (group 23)\n",
      "\n",
      "[RUN] DDR4 RH WIN=512 KF=10 (group 24)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=1024 KF=3 (group 25)\n",
      "\n",
      "[RUN] DDR4 RH WIN=1024 KF=3 (group 26)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=1024 KF=5 (group 27)\n",
      "\n",
      "[RUN] DDR4 RH WIN=1024 KF=5 (group 28)\n",
      "\n",
      "[RUN] DDR4 DROOP WIN=1024 KF=10 (group 29)\n",
      "\n",
      "[RUN] DDR4 RH WIN=1024 KF=10 (group 30)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=32 KF=3 (group 31)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=32 KF=3 (group 32)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=32 KF=5 (group 33)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=32 KF=5 (group 34)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=32 KF=10 (group 35)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=32 KF=10 (group 36)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=64 KF=3 (group 37)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=64 KF=3 (group 38)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=64 KF=5 (group 39)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=64 KF=5 (group 40)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=64 KF=10 (group 41)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=64 KF=10 (group 42)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=128 KF=3 (group 43)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=128 KF=3 (group 44)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=128 KF=5 (group 45)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=128 KF=5 (group 46)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=128 KF=10 (group 47)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=128 KF=10 (group 48)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=512 KF=3 (group 49)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=512 KF=3 (group 50)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=512 KF=5 (group 51)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=512 KF=5 (group 52)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=512 KF=10 (group 53)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=512 KF=10 (group 54)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=1024 KF=3 (group 55)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=1024 KF=3 (group 56)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=1024 KF=5 (group 57)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=1024 KF=5 (group 58)\n",
      "\n",
      "[RUN] DDR5 DROOP WIN=1024 KF=10 (group 59)\n",
      "\n",
      "[RUN] DDR5 SPECTRE WIN=1024 KF=10 (group 60)\n",
      "\n",
      "[OK] ALLRUNS metrics → /Users/hsiaopingni/octaneX_v7_4functions/Results/per_run_metrics_all_PIPELINE.csv\n",
      "[DONE] Pipeline + DSE plots + weight tables per platform\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ===============================================================================================\n",
    "# FULL MULTI-WIN / MULTI-K PIPELINE (AUTHENTIC)\n",
    "# + DSE PLOTS (all cases)\n",
    "# + TABLE_SUBSPACEWEIGHTS (all cases)\n",
    "#\n",
    "# UPDATE (your request):\n",
    "#   ✅ Auto-adjust y-axis for EVERY DSE subplot (all WIN/K) to remove empty white space below.\n",
    "#   ✅ Do NOT show any values/regions above 1.00 (metrics are clipped at 1.00).\n",
    "#   ✅ BUT still leave a small visual headroom so markers/lines at 1.00 are NOT cut off.\n",
    "#\n",
    "# FIX (your request, v2):\n",
    "#   ✅ RH plots: further reduce white space above y=1.00 by tightening the DISPLAY headroom:\n",
    "#        - reduce global Y_DISPLAY_MAX from 1.02 → 1.006\n",
    "#        - reduce adaptive headroom floor/cap so near-saturated plots only get ~0.2%–0.6% headroom\n",
    "#      This matches the DROOP-style “tight top” while still preventing marker clipping at y=1.00.\n",
    "#\n",
    "# NOTE:\n",
    "#   - DATA are still clipped to <= 1.00 (never plotted above 1.00).\n",
    "#   - Only the y-axis DISPLAY max can go slightly above 1.00.\n",
    "# ===============================================================================================\n",
    "\n",
    "import gc, re, hashlib, warnings, unicodedata\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ===============================================================================================\n",
    "# 0) PATHS\n",
    "# ===============================================================================================\n",
    "DATA_DIR = Path(\"/Users/hsiaopingni/Desktop/SLM_RAS-main/HW_TELEMETRY_DATA_COLLECTION/TELEMETRY_DATA\")\n",
    "ROOT     = Path(\"/Users/hsiaopingni/octaneX_v7_4functions\")\n",
    "RES_DIR  = ROOT / \"Results\"\n",
    "RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EXT_DRIVE = Path(\"/Volumes/Untitled\")\n",
    "EXT_RES   = EXT_DRIVE / \"octaneX_results\"\n",
    "\n",
    "def _can_write_dir(p: Path) -> bool:\n",
    "    try:\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        t = p / \".write_test\"\n",
    "        t.write_text(\"ok\")\n",
    "        t.unlink()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if _can_write_dir(EXT_RES):\n",
    "    RES_DIR = EXT_RES\n",
    "    RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"[OK] Using external results dir: {RES_DIR}\")\n",
    "else:\n",
    "    print(f\"[WARN] Cannot write to {EXT_RES}. Using local: {RES_DIR}\")\n",
    "\n",
    "RANK_DIRS = [\n",
    "    ROOT / \"FeatureRankOUT_HYBRID\",\n",
    "    EXT_DRIVE / \"FeatureRankOUT_HYBRID\",\n",
    "    EXT_DRIVE / \"octaneX\" / \"FeatureRankOUT_HYBRID\",\n",
    "]\n",
    "\n",
    "# ===============================================================================================\n",
    "# 1) RUN CONFIG\n",
    "# ===============================================================================================\n",
    "SETUPS = [\"DDR4\", \"DDR5\"]\n",
    "ANOMALIES_BY_SETUP = {\"DDR4\": [\"DROOP\", \"RH\"], \"DDR5\": [\"DROOP\", \"SPECTRE\"]}\n",
    "\n",
    "WINS   = [32, 64, 128, 512, 1024]\n",
    "KFOLDS = [3, 5, 10]\n",
    "PCT_SWEEP = list(range(10, 101, 10))\n",
    "\n",
    "OVERLAP_RATIO       = 0.50\n",
    "OVERLAP_RATIO_DROOP = 0.80\n",
    "\n",
    "ROBUST_WINSOR = (2.0, 98.0)\n",
    "SEED = 1337\n",
    "\n",
    "GC_EVERY_N_PCTS   = 3\n",
    "GC_EVERY_N_GROUPS = 1\n",
    "\n",
    "META = [\"label\", \"setup\", \"run_id\"]\n",
    "DROOP_META_COLS = [\"droop_center_found\", \"droop_best_score_z\", \"droop_frac_ge_thr\", \"droop_vcols\"]\n",
    "\n",
    "DROP_LOW_VARIANCE_COLS = True\n",
    "LOW_VAR_EPS = 1e-10\n",
    "\n",
    "# DROOP boost (small, optional)\n",
    "DROOP_BOOST = True\n",
    "DROOP_BOOST_ALPHA = 0.20\n",
    "DROOP_TRANSIENT_PAT = re.compile(r\"__(drop|range|slope|min|max|std)\", re.I)\n",
    "\n",
    "METHOD_ORDER = [\"dC_aJ\", \"dC_aM\", \"dE_aJ\", \"dE_aM\"]\n",
    "\n",
    "# ===============================================================================================\n",
    "# 2) PLOT CONFIG\n",
    "# ===============================================================================================\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 13.5,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"figure.titlesize\": 20,\n",
    "})\n",
    "\n",
    "METHOD_TITLE = {\n",
    "    \"dC_aJ\": r\"Scoring function $d_C(X)$ with Aggregate $a_J$\",\n",
    "    \"dC_aM\": r\"Scoring function $d_C(X)$ with Aggregate $a_M$\",\n",
    "    \"dE_aJ\": r\"Scoring function $d_E(X)$ with Aggregate $a_J$\",\n",
    "    \"dE_aM\": r\"Scoring function $d_E(X)$ with Aggregate $a_M$\",\n",
    "}\n",
    "\n",
    "# ---- Data never exceeds 1.00 ----\n",
    "Y_DATA_MAX = 1.00\n",
    "Y_DATA_MIN = 0.00\n",
    "\n",
    "# ---- Display headroom so points at 1.00 aren't cut off ----\n",
    "# (TIGHTENED for RH plots)\n",
    "Y_DISPLAY_MAX = 1.006     # was 1.02 → reduce top whitespace while still preventing clipping\n",
    "Y_HEADROOM    = 0.006     # cap for adaptive headroom (display-only), data remain <= 1.00\n",
    "\n",
    "# y-axis auto-range per subplot\n",
    "Y_AUTO     = True\n",
    "Y_PAD_FRAC = 0.06\n",
    "Y_MIN_SPAN = 0.04\n",
    "\n",
    "ALPHA_MINMAX = 0.28\n",
    "ALPHA_IQR    = 0.60\n",
    "GRID_ALPHA   = 0.22\n",
    "GRID_LS      = \"--\"\n",
    "GRID_LW      = 0.6\n",
    "\n",
    "FIGSIZE = (13.6, 8.4)\n",
    "WSPACE  = 0.30\n",
    "HSPACE  = 0.42\n",
    "TOP     = 0.88\n",
    "BOTTOM  = 0.20\n",
    "LEFT    = 0.075\n",
    "RIGHT   = 0.985\n",
    "SUPTITLE_Y = 0.975\n",
    "LEGEND_Y   = 0.055\n",
    "\n",
    "BAND_EPS = 0.008\n",
    "def _ensure_visible_band(lo: np.ndarray, hi: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    lo = lo.astype(float).copy()\n",
    "    hi = hi.astype(float).copy()\n",
    "    flat = (np.abs(hi - lo) < 1e-12)\n",
    "    if np.any(flat):\n",
    "        lo2 = lo - BAND_EPS/2\n",
    "        hi2 = hi + BAND_EPS/2\n",
    "        lo[flat] = np.clip(lo2[flat], Y_DATA_MIN, Y_DATA_MAX)\n",
    "        hi[flat] = np.clip(hi2[flat], Y_DATA_MIN, Y_DATA_MAX)\n",
    "    return lo, hi\n",
    "\n",
    "def _clip_metric(a):\n",
    "    \"\"\"Hard clip metrics (data) to [0,1].\"\"\"\n",
    "    return np.clip(np.asarray(a, float), Y_DATA_MIN, Y_DATA_MAX)\n",
    "\n",
    "def _auto_ylim_from_arrays(arrs: List[np.ndarray]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Tight ylim based on plotted data bands/median.\n",
    "    Axis can extend slightly above 1.00 up to Y_DISPLAY_MAX for visual headroom,\n",
    "    but data are clipped at 1.00.\n",
    "\n",
    "    v2: Even tighter headroom for near-saturated plots (RH).\n",
    "    \"\"\"\n",
    "    vals = []\n",
    "    for a in arrs:\n",
    "        if a is None:\n",
    "            continue\n",
    "        a = np.asarray(a, float).ravel()\n",
    "        a = a[np.isfinite(a)]\n",
    "        if a.size:\n",
    "            vals.append(a)\n",
    "    if not vals:\n",
    "        return (Y_DATA_MIN, Y_DISPLAY_MAX)\n",
    "\n",
    "    v = np.concatenate(vals)\n",
    "    ymin = float(np.min(v))\n",
    "    ymax = float(np.max(v))\n",
    "    if not np.isfinite(ymin) or not np.isfinite(ymax):\n",
    "        return (Y_DATA_MIN, Y_DISPLAY_MAX)\n",
    "\n",
    "    # base padding\n",
    "    raw_span = max(ymax - ymin, 1e-12)\n",
    "    span = max(ymax - ymin, Y_MIN_SPAN)\n",
    "    pad = Y_PAD_FRAC * span\n",
    "    ymin2 = float(np.clip(ymin - pad, Y_DATA_MIN, Y_DATA_MAX))\n",
    "    ymax2 = float(np.clip(ymax + pad, Y_DATA_MIN, Y_DATA_MAX))\n",
    "\n",
    "    # enforce minimal span inside [0,1]\n",
    "    if ymax2 - ymin2 < Y_MIN_SPAN:\n",
    "        mid = 0.5 * (ymin2 + ymax2)\n",
    "        ymin2 = float(np.clip(mid - Y_MIN_SPAN/2, Y_DATA_MIN, Y_DATA_MAX))\n",
    "        ymax2 = float(np.clip(mid + Y_MIN_SPAN/2, Y_DATA_MIN, Y_DATA_MAX))\n",
    "\n",
    "    # ---- ADAPTIVE headroom near 1.00 (TIGHTER) ----\n",
    "    if ymax2 >= (Y_DATA_MAX - 1e-6):\n",
    "        # headroom ~0.2%–0.6% for near-saturated cases:\n",
    "        # - floor prevents marker clipping\n",
    "        # - cap keeps RH from showing large white space\n",
    "        head = min(Y_HEADROOM, max(0.002, 0.10 * max(raw_span, 0.02)))  # floor 0.002, typically 0.002–0.006\n",
    "        if ymin2 >= 0.85:\n",
    "            head = min(head, 0.004)  # extra-tight for RH-like (high lower bound)\n",
    "        ymax_display = min(Y_DISPLAY_MAX, Y_DATA_MAX + head)\n",
    "        ymax2 = max(ymax2, ymax_display)\n",
    "    else:\n",
    "        ymax2 = min(Y_DISPLAY_MAX, ymax2)\n",
    "\n",
    "    ymin2 = max(Y_DATA_MIN, ymin2)\n",
    "    return ymin2, ymax2\n",
    "\n",
    "# ===============================================================================================\n",
    "# 3) WEIGHT CANDIDATES (Table-3 + extras) + LOOKUP\n",
    "# ===============================================================================================\n",
    "def build_weight_table() -> pd.DataFrame:\n",
    "    paper_cases = [\n",
    "        (\"T3_01\", \"Case 1\",  1,    0,    0),\n",
    "        (\"T3_02\", \"Case 2\",  0,    1,    0),\n",
    "        (\"T3_03\", \"Case 3\",  0,    0,    1),\n",
    "        (\"T3_04\", \"Case 4\",  1/3,  1/3,  1/3),\n",
    "        (\"T3_05\", \"Case 5\",  1/4,  1/4,  2/4),\n",
    "        (\"T3_06\", \"Case 6\",  1/5,  1/5,  3/5),\n",
    "        (\"T3_07\", \"Case 7\",  1/6,  1/6,  4/6),\n",
    "        (\"T3_08\", \"Case 8\",  1/8,  2/8,  5/8),\n",
    "        (\"T3_09\", \"Case 9\",  1/8,  1/8,  6/8),\n",
    "        (\"T3_10\", \"Case 10\", 1/10, 1/10, 8/10),\n",
    "        (\"T3_11\", \"Case 11\", 2/3,  1/3,  1/3),\n",
    "        (\"T3_12\", \"Case 12\", 3/4,  1/4,  1/4),\n",
    "        (\"T3_13\", \"Case 13\", 5/8,  2/8,  1/8),\n",
    "        (\"T3_14\", \"Case 14\", 6/8,  1/8,  1/8),\n",
    "        (\"T3_15\", \"Case 15\", 1/20, 1/20, 18/20),\n",
    "        (\"T3_16\", \"Case 16\", 1/40, 1/40, 38/40),\n",
    "    ]\n",
    "    rows = []\n",
    "    for cid, name, wM, wC, wS in paper_cases:\n",
    "        rows.append({\"weight_case_id\": cid, \"weight_case_name\": name, \"wC\": float(wC), \"wM\": float(wM), \"wS\": float(wS), \"source\": \"Table3\"})\n",
    "    extras = [\n",
    "        (\"EX_01\", \"SensorOnly\", 0.0, 0.0, 1.0),\n",
    "        (\"EX_02\", \"Sensor90\",   0.05, 0.05, 0.90),\n",
    "        (\"EX_03\", \"Sensor95\",   0.025, 0.025, 0.95),\n",
    "        (\"EX_04\", \"Sensor98\",   0.01, 0.01, 0.98),\n",
    "    ]\n",
    "    for cid, name, wC, wM, wS in extras:\n",
    "        rows.append({\"weight_case_id\": cid, \"weight_case_name\": name, \"wC\": float(wC), \"wM\": float(wM), \"wS\": float(wS), \"source\": \"Extra\"})\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"_key\"] = df.apply(lambda r: (round(r[\"wC\"], 6), round(r[\"wM\"], 6), round(r[\"wS\"], 6)), axis=1)\n",
    "    df = df.drop_duplicates(\"_key\").drop(columns=[\"_key\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "WEIGHT_TABLE = build_weight_table()\n",
    "\n",
    "def _weight_lookup(w: Tuple[float,float,float]) -> Tuple[str,str,str]:\n",
    "    wC,wM,wS = (round(float(w[0]),6), round(float(w[1]),6), round(float(w[2]),6))\n",
    "    m = WEIGHT_TABLE[(WEIGHT_TABLE[\"wC\"].round(6)==wC) & (WEIGHT_TABLE[\"wM\"].round(6)==wM) & (WEIGHT_TABLE[\"wS\"].round(6)==wS)]\n",
    "    if len(m)>0:\n",
    "        r = m.iloc[0]\n",
    "        return str(r[\"weight_case_id\"]), str(r[\"weight_case_name\"]), str(r[\"source\"])\n",
    "    return (\"CUSTOM\",\"Custom\",\"Search\")\n",
    "\n",
    "# ===============================================================================================\n",
    "# 4) IO + workload parsing\n",
    "# ===============================================================================================\n",
    "RUNID2PATH: Dict[str,str] = {}\n",
    "\n",
    "def detect_setup_from_path(p: Path):\n",
    "    s = str(p).lower()\n",
    "    if \"ddr4\" in s: return \"DDR4\"\n",
    "    if \"ddr5\" in s: return \"DDR5\"\n",
    "    return None\n",
    "\n",
    "def is_benign_path(p: Path) -> bool:\n",
    "    s = str(p).lower()\n",
    "    if \"benign\" in s:\n",
    "        return True\n",
    "    bad = [\"attack\",\"anom\",\"fault\",\"inject\",\"trojan\",\"mal\",\"rh\",\"droop\",\"spectre\",\"trrespass\"]\n",
    "    return not any(b in s for b in bad)\n",
    "\n",
    "def is_anomaly_path(p: Path, anomaly: str) -> bool:\n",
    "    return (anomaly.lower() in str(p).lower()) and (not is_benign_path(p))\n",
    "\n",
    "def iter_raw_csvs(root: Path):\n",
    "    for p in root.rglob(\"*.csv\"):\n",
    "        yield p\n",
    "\n",
    "def read_csv_clean(p: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(p)\n",
    "    return df.loc[:, ~df.columns.str.startswith(\"Unnamed\")]\n",
    "\n",
    "def mk_run_id(path: Path) -> str:\n",
    "    rid = f\"run_{hashlib.md5(str(path).encode('utf-8')).hexdigest()[:10]}\"\n",
    "    RUNID2PATH[rid] = str(path)\n",
    "    return rid\n",
    "\n",
    "_WORKLOADS = [\"dft\",\"dj\",\"dp\",\"gl\",\"gs\",\"ha\",\"ja\",\"mm\",\"ni\",\"oe\",\"pi\",\"sh\",\"tr\"]\n",
    "def workload_from_path(p: Path) -> str:\n",
    "    stem = (p.stem or \"\").lower()\n",
    "    m = re.search(r\"_([a-z]{2,3})$\", stem)\n",
    "    if m and m.group(1) in _WORKLOADS:\n",
    "        return m.group(1).upper()\n",
    "    for tok in _WORKLOADS:\n",
    "        if tok in stem:\n",
    "            return tok.upper()\n",
    "    return \"UNK\"\n",
    "\n",
    "def collect_raw_pairs_by_setup(data_dir: Path, which: str, anomaly: Optional[str] = None):\n",
    "    out = {\"DDR4\": [], \"DDR5\": []}\n",
    "    for p in iter_raw_csvs(data_dir):\n",
    "        setup = detect_setup_from_path(p)\n",
    "        if setup is None:\n",
    "            continue\n",
    "        try:\n",
    "            if which == \"benign\":\n",
    "                if not is_benign_path(p):\n",
    "                    continue\n",
    "            else:\n",
    "                if anomaly is None or not is_anomaly_path(p, anomaly):\n",
    "                    continue\n",
    "            df = read_csv_clean(p)\n",
    "            out[setup].append((p, df))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] read failed {p}: {e}\")\n",
    "    return out\n",
    "\n",
    "def telemetry_cols(df: pd.DataFrame):\n",
    "    exclude = set(META) | set(DROOP_META_COLS) | {\"workload\"}\n",
    "    return [c for c in df.columns if (c not in exclude) and (df[c].dtype.kind in \"fcbiu\")]\n",
    "\n",
    "def drop_low_variance_cols(df: pd.DataFrame, cols: List[str], eps: float = 1e-10) -> List[str]:\n",
    "    v = df[cols].astype(float).var(axis=0, ddof=0)\n",
    "    keep = v[v > eps].index.tolist()\n",
    "    return keep\n",
    "\n",
    "# ===============================================================================================\n",
    "# 5) Scaling\n",
    "# ===============================================================================================\n",
    "def robust_scale_train(Xb_np: np.ndarray, winsor=(2.0, 98.0)):\n",
    "    Q1, Q2 = np.percentile(Xb_np, winsor[0], axis=0), np.percentile(Xb_np, winsor[1], axis=0)\n",
    "    Xb_clip = np.clip(Xb_np, Q1, Q2)\n",
    "    mu = Xb_clip.mean(axis=0)\n",
    "    sd = Xb_clip.std(axis=0, ddof=0) + 1e-9\n",
    "    return mu, sd, Q1, Q2\n",
    "\n",
    "def apply_robust_scale(X: pd.DataFrame, mu, sd, Q1, Q2):\n",
    "    Xc = np.clip(X.to_numpy(dtype=float), Q1, Q2)\n",
    "    Z  = (Xc - mu) / sd\n",
    "    return pd.DataFrame(Z, columns=X.columns, index=X.index)\n",
    "\n",
    "# ===============================================================================================\n",
    "# 6) Windowing (adds sensor transient feats incl CPU Voltage)\n",
    "# ===============================================================================================\n",
    "SENSOR_PAT = re.compile(r\"cpu\\s*voltage|volt|vdd|vcore|vin|vout|power|energy|joule|current|amps?|temp|thermal|hot\", re.I)\n",
    "\n",
    "def window_collapse_means(df: pd.DataFrame, win: int, setup: str, run_id: str, label: str,\n",
    "                          overlap_ratio: float, src_path: str=\"\") -> pd.DataFrame:\n",
    "    cols_all = telemetry_cols(df)\n",
    "    if not cols_all:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    sensor_cols = [c for c in cols_all if SENSOR_PAT.search(c)]\n",
    "    if \"CPU Voltage\" in cols_all and \"CPU Voltage\" not in sensor_cols:\n",
    "        sensor_cols.append(\"CPU Voltage\")\n",
    "\n",
    "    n = len(df)\n",
    "    stride = max(1, int(round(win * (1 - overlap_ratio))))\n",
    "    starts = list(range(0, n - win + 1, stride))\n",
    "    if not starts:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for start in starts:\n",
    "        chunk = df.iloc[start:start+win]\n",
    "        X = chunk[cols_all].astype(float)\n",
    "        means = X.mean(axis=0, numeric_only=True)\n",
    "\n",
    "        tfeat = {}\n",
    "        if sensor_cols:\n",
    "            Xs = X[sensor_cols]\n",
    "            mins  = Xs.min(axis=0)\n",
    "            maxs  = Xs.max(axis=0)\n",
    "            stds  = Xs.std(axis=0, ddof=0)\n",
    "            means_s = Xs.mean(axis=0)\n",
    "            drops  = (means_s - mins)\n",
    "            ranges = (maxs - mins)\n",
    "            slope  = (Xs.iloc[-1] - Xs.iloc[0])\n",
    "            for c in sensor_cols:\n",
    "                tfeat[f\"{c}__min\"]   = float(mins[c])\n",
    "                tfeat[f\"{c}__max\"]   = float(maxs[c])\n",
    "                tfeat[f\"{c}__std\"]   = float(stds[c])\n",
    "                tfeat[f\"{c}__drop\"]  = float(drops[c])\n",
    "                tfeat[f\"{c}__range\"] = float(ranges[c])\n",
    "                tfeat[f\"{c}__slope\"] = float(slope[c])\n",
    "\n",
    "        row = pd.concat([means, pd.Series(tfeat)]).to_frame().T\n",
    "        row[\"setup\"]    = setup\n",
    "        row[\"run_id\"]   = run_id\n",
    "        row[\"label\"]    = label\n",
    "        row[\"workload\"] = workload_from_path(Path(src_path))\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "def build_windowed_raw_means(pairs, setup: str, win: int, label: str, overlap_ratio: float) -> pd.DataFrame:\n",
    "    out = []\n",
    "    for p, df in pairs:\n",
    "        rid = mk_run_id(p)\n",
    "        agg = window_collapse_means(df, win=win, setup=setup, run_id=rid, label=label,\n",
    "                                    overlap_ratio=overlap_ratio, src_path=str(p))\n",
    "        if not agg.empty:\n",
    "            out.append(agg)\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n",
    "\n",
    "# ===============================================================================================\n",
    "# 7) Rank lists + robust mapping + CPU Voltage family append for DROOP\n",
    "# ===============================================================================================\n",
    "def _read_rank_feats(p: Path):\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "    except Exception:\n",
    "        return []\n",
    "    if df.empty:\n",
    "        return []\n",
    "    col = \"feature\" if \"feature\" in df.columns else df.columns[0]\n",
    "    return df[col].dropna().astype(str).tolist()\n",
    "\n",
    "def load_full_rank_lists(setup: str, win: int, kfold: int):\n",
    "    def _find_file(setup, win, kfold, sub):\n",
    "        fname = f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "        for d in RANK_DIRS:\n",
    "            pp = d / fname\n",
    "            if pp.exists():\n",
    "                return pp\n",
    "        return None\n",
    "    out = {}\n",
    "    for sub in [\"compute\",\"memory\",\"sensors\"]:\n",
    "        pp = _find_file(setup, win, kfold, sub)\n",
    "        out[sub] = _read_rank_feats(pp) if pp else []\n",
    "    return out\n",
    "\n",
    "def slice_by_percent(full_lists: Dict[str, List[str]], pct: int):\n",
    "    sel = {}\n",
    "    frac = pct / 100.0\n",
    "    for sub in (\"compute\",\"memory\",\"sensors\"):\n",
    "        feats = full_lists.get(sub, []) or []\n",
    "        k = int(np.ceil(len(feats) * frac))\n",
    "        if frac > 0 and len(feats) > 0:\n",
    "            k = max(1, k)\n",
    "        sel[sub] = feats[:min(len(feats), max(0, k))]\n",
    "    return sel\n",
    "\n",
    "ALIAS_MAP = {\n",
    "    \"voltage\": [\"cpu\",\"voltage\",\"volt\",\"vcore\",\"vdd\",\"vin\",\"vout\"],\n",
    "    \"power\": [\"power\",\"energy\",\"joules\",\"current\",\"amps\"],\n",
    "    \"temperature\": [\"temp\",\"thermal\",\"hot\"],\n",
    "    \"frequency\": [\"freq\",\"afreq\",\"cfreq\",\"clock\",\"clk\",\"mhz\",\"ghz\"],\n",
    "    \"ipc\": [\"ipc\",\"physipc\"],\n",
    "    \"cache\": [\"l1\",\"l2\",\"l3\",\"hit\",\"miss\",\"evict\",\"fill\",\"mpi\"],\n",
    "    \"bandwidth\": [\"read\",\"write\",\"bw\",\"bandwidth\"],\n",
    "}\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s)).lower()\n",
    "    out = []\n",
    "    for ch in s:\n",
    "        out.append(ch if ch.isalnum() else \"_\")\n",
    "    s = \"\".join(out)\n",
    "    while \"__\" in s:\n",
    "        s = s.replace(\"__\",\"_\")\n",
    "    return s.strip(\"_\")\n",
    "\n",
    "def _tokens(s: str):\n",
    "    t = _norm(s).replace(\"_\",\" \").split()\n",
    "    exp = []\n",
    "    for tok in t:\n",
    "        exp.append(tok)\n",
    "        for k, aliases in ALIAS_MAP.items():\n",
    "            if tok in aliases:\n",
    "                exp.append(k)\n",
    "    return set(exp)\n",
    "\n",
    "def build_column_index(cols: List[str]):\n",
    "    norm2orig = {}\n",
    "    tok_index = {}\n",
    "    for c in cols:\n",
    "        nc = _norm(c)\n",
    "        norm2orig.setdefault(nc, c)\n",
    "        tok_index[c] = _tokens(c)\n",
    "    return norm2orig, tok_index\n",
    "\n",
    "def map_ranks_to_existing(ranked_list: List[str], norm2orig, tok_index):\n",
    "    mapped, seen = [], set()\n",
    "    for r in ranked_list:\n",
    "        nr = _norm(r)\n",
    "        cand = norm2orig.get(nr, None)\n",
    "        if cand and cand not in seen:\n",
    "            mapped.append(cand); seen.add(cand); continue\n",
    "        rtoks = _tokens(r)\n",
    "        best_c, best_j = None, 0.0\n",
    "        for c, ctoks in tok_index.items():\n",
    "            inter = len(rtoks & ctoks)\n",
    "            union = len(rtoks | ctoks) or 1\n",
    "            j = inter / union\n",
    "            if j > best_j:\n",
    "                best_j, best_c = j, c\n",
    "        if best_c and best_j >= 0.60 and best_c not in seen:\n",
    "            mapped.append(best_c); seen.add(best_c)\n",
    "    return mapped\n",
    "\n",
    "def intersect_selection_with_columns_robust(sel_raw: Dict[str, List[str]], xb_cols: set):\n",
    "    norm2orig, tok_index = build_column_index(list(xb_cols))\n",
    "    out = {}\n",
    "    for sub in (\"compute\",\"memory\",\"sensors\"):\n",
    "        ranked = sel_raw.get(sub, []) or []\n",
    "        mapped = map_ranks_to_existing(ranked, norm2orig, tok_index)\n",
    "        out[sub] = [c for c in mapped if c in xb_cols]\n",
    "    return out\n",
    "\n",
    "def force_cpu_voltage_family(sel: Dict[str, List[str]], xb_cols: set) -> Dict[str, List[str]]:\n",
    "    out = dict(sel)\n",
    "    sensors_now = out.get(\"sensors\", []) or []\n",
    "    must = []\n",
    "    if \"CPU Voltage\" in xb_cols:\n",
    "        must.append(\"CPU Voltage\")\n",
    "    for suf in [\"__min\",\"__max\",\"__std\",\"__drop\",\"__range\",\"__slope\"]:\n",
    "        c = f\"CPU Voltage{suf}\"\n",
    "        if c in xb_cols:\n",
    "            must.append(c)\n",
    "    out[\"sensors\"] = list(dict.fromkeys(sensors_now + must))\n",
    "    return out\n",
    "\n",
    "# ===============================================================================================\n",
    "# 8) Balanced evaluation helper\n",
    "# ===============================================================================================\n",
    "def balanced_concat(df_ben: pd.DataFrame, df_anom: pd.DataFrame, seed: int = 1337):\n",
    "    if df_ben is None or df_ben.empty:\n",
    "        raise ValueError(\"balanced_concat(): df_ben is empty\")\n",
    "    if df_anom is None or df_anom.empty:\n",
    "        raise ValueError(\"balanced_concat(): df_anom is empty\")\n",
    "\n",
    "    rng = np.random.default_rng(int(seed))\n",
    "    nb = int(len(df_ben))\n",
    "    na = int(len(df_anom))\n",
    "    n  = int(min(nb, na))\n",
    "\n",
    "    idx_b = rng.choice(nb, size=n, replace=(nb < n))\n",
    "    idx_a = rng.choice(na, size=n, replace=(na < n))\n",
    "\n",
    "    ben_s = df_ben.iloc[idx_b].copy().reset_index(drop=True)\n",
    "    anm_s = df_anom.iloc[idx_a].copy().reset_index(drop=True)\n",
    "\n",
    "    df_eval = pd.concat([ben_s, anm_s], ignore_index=True)\n",
    "    y_true  = np.concatenate([np.zeros(len(ben_s), dtype=int),\n",
    "                              np.ones(len(anm_s), dtype=int)])\n",
    "    return df_eval, y_true\n",
    "\n",
    "# ===============================================================================================\n",
    "# 9) Weight selection helpers\n",
    "# ===============================================================================================\n",
    "def split_holdout(y, test_size=0.3, seed=SEED):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    idx = np.arange(len(y))\n",
    "    _, val_idx = next(sss.split(idx, y))\n",
    "    return val_idx\n",
    "\n",
    "def pick_best_w_per_method_PRfirst(y_true: np.ndarray, scores_by_w: Dict[Tuple[float,float,float], Dict[str,np.ndarray]], val_idx: np.ndarray):\n",
    "    yy = y_true[val_idx]\n",
    "    best = {m: (-1.0, -1.0, -1.0, None) for m in [\"dE_aM\",\"dE_aJ\",\"dC_aM\",\"dC_aJ\"]}\n",
    "    for w, md in scores_by_w.items():\n",
    "        wS = float(w[2])\n",
    "        for m, s in md.items():\n",
    "            sv = np.asarray(s)[val_idx]\n",
    "            pr  = float(average_precision_score(yy, sv))\n",
    "            roc = float(roc_auc_score(yy, sv))\n",
    "            key = (pr, roc, wS)\n",
    "            if key > best[m][:3]:\n",
    "                best[m] = (pr, roc, wS, w)\n",
    "    return {m: best[m][3] for m in best}\n",
    "\n",
    "# ===============================================================================================\n",
    "# 10) DSE plots\n",
    "# ===============================================================================================\n",
    "def summarize_for_dse(per_run_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"pct\"]\n",
    "    def q1(x): return float(np.nanpercentile(x, 25))\n",
    "    def q3(x): return float(np.nanpercentile(x, 75))\n",
    "    g = (per_run_df.groupby(keys, as_index=False)\n",
    "            .agg(\n",
    "                roc_auc_median=(\"roc_auc\",\"median\"),\n",
    "                roc_auc_q1=(\"roc_auc\", q1),\n",
    "                roc_auc_q3=(\"roc_auc\", q3),\n",
    "                roc_auc_min=(\"roc_auc\",\"min\"),\n",
    "                roc_auc_max=(\"roc_auc\",\"max\"),\n",
    "                auc_pr_median=(\"auc_pr\",\"median\"),\n",
    "                auc_pr_q1=(\"auc_pr\", q1),\n",
    "                auc_pr_q3=(\"auc_pr\", q3),\n",
    "                auc_pr_min=(\"auc_pr\",\"min\"),\n",
    "                auc_pr_max=(\"auc_pr\",\"max\"),\n",
    "                n_runs=(\"run_id\",\"nunique\"),\n",
    "            ))\n",
    "    return g.sort_values(keys).reset_index(drop=True)\n",
    "\n",
    "def plot_dse_grid(summary_case: pd.DataFrame, metric: str, setup: str, anomaly: str,\n",
    "                  win: int, kfold: int, out_dir: Path):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    x_ticks = sorted(summary_case[\"pct\"].unique().tolist())\n",
    "    if not x_ticks:\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=FIGSIZE, sharex=False, sharey=False)\n",
    "    axes = axes.flatten()\n",
    "    fig.subplots_adjust(left=LEFT, right=RIGHT, top=TOP, bottom=BOTTOM, wspace=WSPACE, hspace=HSPACE)\n",
    "\n",
    "    xlab = \"Top-ranked features used (%)\"\n",
    "    ylab = \"ROC-AUC\" if metric == \"roc_auc\" else \"AUC-PR\"\n",
    "\n",
    "    for i, method in enumerate(METHOD_ORDER):\n",
    "        ax = axes[i]\n",
    "        ax.set_title(METHOD_TITLE.get(method, method), pad=7)\n",
    "        ax.set_xlim(min(x_ticks), max(x_ticks))\n",
    "        ax.set_xticks(x_ticks)\n",
    "        ax.grid(True, alpha=GRID_ALPHA, linestyle=GRID_LS, linewidth=GRID_LW)\n",
    "\n",
    "        sub = summary_case[summary_case[\"method\"] == method].copy().sort_values(\"pct\")\n",
    "        if sub.empty:\n",
    "            ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\", transform=ax.transAxes)\n",
    "            ax.set_ylim(Y_DATA_MIN, Y_DISPLAY_MAX)\n",
    "        else:\n",
    "            x    = sub[\"pct\"].to_numpy(dtype=float)\n",
    "            med  = _clip_metric(sub[f\"{metric}_median\"].to_numpy(dtype=float))\n",
    "            q1   = _clip_metric(sub[f\"{metric}_q1\"].to_numpy(dtype=float))\n",
    "            q3   = _clip_metric(sub[f\"{metric}_q3\"].to_numpy(dtype=float))\n",
    "            vmin = _clip_metric(sub[f\"{metric}_min\"].to_numpy(dtype=float))\n",
    "            vmax = _clip_metric(sub[f\"{metric}_max\"].to_numpy(dtype=float))\n",
    "\n",
    "            lo_mm, hi_mm = np.minimum(vmin, vmax), np.maximum(vmin, vmax)\n",
    "            lo_iq, hi_iq = np.minimum(q1, q3),     np.maximum(q1, q3)\n",
    "            lo_mm, hi_mm = _ensure_visible_band(lo_mm, hi_mm)\n",
    "            lo_iq, hi_iq = _ensure_visible_band(lo_iq, hi_iq)\n",
    "\n",
    "            ax.fill_between(x, lo_mm, hi_mm, alpha=ALPHA_MINMAX, label=\"min-max\", linewidth=0.0)\n",
    "            ax.fill_between(x, lo_iq, hi_iq, alpha=ALPHA_IQR,   label=\"IQR\",     linewidth=0.0)\n",
    "            ax.plot(x, med, color=\"black\", marker=\"o\", linewidth=2.0, markersize=5.5, zorder=5, label=\"Median\")\n",
    "\n",
    "            if Y_AUTO:\n",
    "                ylo, yhi = _auto_ylim_from_arrays([lo_mm, hi_mm, lo_iq, hi_iq, med])\n",
    "                ax.set_ylim(ylo, yhi)\n",
    "            else:\n",
    "                ax.set_ylim(Y_DATA_MIN, Y_DISPLAY_MAX)\n",
    "\n",
    "        ax.set_xlabel(xlab, labelpad=6)\n",
    "        ax.set_ylabel(ylab, labelpad=6)\n",
    "        ax.tick_params(axis=\"x\", labelbottom=True, pad=3)\n",
    "        ax.tick_params(axis=\"y\", labelleft=True, pad=3)\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"lower center\", bbox_to_anchor=(0.5, LEGEND_Y), ncol=3, frameon=False)\n",
    "    fig.suptitle(f\"{setup}: {anomaly} (WIN={win}, K={kfold})\", y=SUPTITLE_Y)\n",
    "\n",
    "    base = f\"DSE_{metric.upper()}_{setup}_{anomaly}_WIN{int(win)}_KF{int(kfold)}\"\n",
    "    fig.savefig(out_dir / f\"{base}.png\", dpi=600, bbox_inches=\"tight\", pad_inches=0.06)\n",
    "    fig.savefig(out_dir / f\"{base}.pdf\", bbox_inches=\"tight\", pad_inches=0.06)\n",
    "    plt.close(fig)\n",
    "\n",
    "def write_all_dse_plots(per_run_df: pd.DataFrame):\n",
    "    out_root = RES_DIR / \"DesignSpace\" / \"PaperStyle_ALLRUNS\"\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "    dse = summarize_for_dse(per_run_df)\n",
    "    for (setup, anomaly, win, kfold), sub in dse.groupby([\"setup\",\"anomaly\",\"win\",\"kfold\"]):\n",
    "        out_dir = out_root / str(setup) / str(anomaly) / f\"WIN{int(win)}_KF{int(kfold)}\"\n",
    "        plot_dse_grid(sub, \"roc_auc\", str(setup), str(anomaly), int(win), int(kfold), out_dir)\n",
    "        plot_dse_grid(sub, \"auc_pr\",  str(setup), str(anomaly), int(win), int(kfold), out_dir)\n",
    "\n",
    "# ===============================================================================================\n",
    "# 11) Subspace-weight tables (same as before; omitted here for brevity)\n",
    "# ===============================================================================================\n",
    "def build_workload_weight_tables(per_run_df: pd.DataFrame, setup: str, win: int, kfold: int) -> None:\n",
    "    # (same implementation you already use; keep unchanged)\n",
    "    base_dir = RES_DIR / \"Table_SubspaceWeights\" / setup / f\"WIN{win}_KF{kfold}\"\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rep_method = \"dE_aM\"\n",
    "    for anomaly in ANOMALIES_BY_SETUP[setup]:\n",
    "        df = per_run_df[\n",
    "            (per_run_df[\"setup\"]==setup) &\n",
    "            (per_run_df[\"anomaly\"]==anomaly) &\n",
    "            (per_run_df[\"win\"]==win) &\n",
    "            (per_run_df[\"kfold\"]==kfold)\n",
    "        ].copy()\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        df[\"_wkey\"] = df.apply(lambda r: (round(r[\"wC\"],6), round(r[\"wM\"],6), round(r[\"wS\"],6)), axis=1)\n",
    "        rows = []\n",
    "        for (wl, method), g in df.groupby([\"workload\",\"method\"]):\n",
    "            mode_key = g[\"_wkey\"].value_counts().idxmax()\n",
    "            wC, wM, wS = mode_key\n",
    "            wid, wname, wsrc = _weight_lookup((wC,wM,wS))\n",
    "            rows.append({\n",
    "                \"workload\": wl,\n",
    "                \"method\": method,\n",
    "                \"chosen_wC\": wC, \"chosen_wM\": wM, \"chosen_wS\": wS,\n",
    "                \"weight_case_id\": wid,\n",
    "                \"weight_case_name\": wname,\n",
    "                \"weight_case_source\": wsrc,\n",
    "                \"n_rows\": int(len(g)),\n",
    "                \"median_roc\": float(np.nanmedian(g[\"roc_auc\"])),\n",
    "                \"median_pr\": float(np.nanmedian(g[\"auc_pr\"])),\n",
    "            })\n",
    "        df_out = pd.DataFrame(rows).sort_values([\"workload\",\"method\"]).reset_index(drop=True)\n",
    "        (base_dir / f\"Table_SubspaceWeights_{setup}_WIN{win}_KF{kfold}_{anomaly}.csv\").write_text(df_out.to_csv(index=False))\n",
    "\n",
    "# ===============================================================================================\n",
    "# 12) Pipeline core (same logic as your current version; unchanged except DSE y-axis)\n",
    "# ===============================================================================================\n",
    "def run_full_pipeline() -> pd.DataFrame:\n",
    "    all_rows = []\n",
    "    group_counter = 0\n",
    "\n",
    "    benign_pairs = collect_raw_pairs_by_setup(DATA_DIR, which=\"benign\")\n",
    "    anomaly_pairs_cache: Dict[Tuple[str,str], List[Tuple[Path,pd.DataFrame]]] = {}\n",
    "\n",
    "    def get_anom_pairs(setup: str, anomaly: str):\n",
    "        key = (setup, anomaly.upper())\n",
    "        if key not in anomaly_pairs_cache:\n",
    "            anomaly_pairs_cache[key] = collect_raw_pairs_by_setup(DATA_DIR, which=\"anomaly\", anomaly=anomaly)[setup]\n",
    "        return anomaly_pairs_cache[key]\n",
    "\n",
    "    for setup in SETUPS:\n",
    "        anomalies = ANOMALIES_BY_SETUP.get(setup, [])\n",
    "        ben_pairs_all = benign_pairs.get(setup, [])\n",
    "        if not ben_pairs_all:\n",
    "            print(f\"[SKIP] {setup}: no benign RAW files\")\n",
    "            continue\n",
    "\n",
    "        for win in WINS:\n",
    "            df_ben_all = build_windowed_raw_means(ben_pairs_all, setup=setup, win=win, label=\"BENIGN\",\n",
    "                                                  overlap_ratio=OVERLAP_RATIO)\n",
    "            if df_ben_all.empty:\n",
    "                print(f\"[SKIP] {setup} WIN={win}: benign windowed empty\")\n",
    "                continue\n",
    "\n",
    "            Xb_cols = telemetry_cols(df_ben_all)\n",
    "            if DROP_LOW_VARIANCE_COLS:\n",
    "                Xb_cols = drop_low_variance_cols(df_ben_all, Xb_cols, eps=LOW_VAR_EPS)\n",
    "            if not Xb_cols:\n",
    "                print(f\"[SKIP] {setup} WIN={win}: no telemetry cols after low-var drop\")\n",
    "                continue\n",
    "\n",
    "            Xb = df_ben_all[Xb_cols].astype(float)\n",
    "            mu_s, sd_s, Q1, Q2 = robust_scale_train(Xb.values, winsor=ROBUST_WINSOR)\n",
    "            Xb_base = apply_robust_scale(Xb, mu_s, sd_s, Q1, Q2)\n",
    "            xb_cols_set = set(Xb_base.columns)\n",
    "\n",
    "            ben_by_wl = {}\n",
    "            for wl in df_ben_all[\"workload\"].astype(str).unique():\n",
    "                ben_by_wl[wl] = df_ben_all[df_ben_all[\"workload\"].astype(str)==wl].copy()\n",
    "\n",
    "            for kfold in KFOLDS:\n",
    "                wdir = RES_DIR / \"Table_SubspaceWeights\" / setup / f\"WIN{win}_KF{kfold}\"\n",
    "                wdir.mkdir(parents=True, exist_ok=True)\n",
    "                WEIGHT_TABLE.to_csv(wdir / \"Table_SubspaceWeights_CANDIDATES.csv\", index=False)\n",
    "\n",
    "                full_lists = load_full_rank_lists(setup, win, kfold)\n",
    "\n",
    "                for anomaly in anomalies:\n",
    "                    group_counter += 1\n",
    "                    print(f\"\\n[RUN] {setup} {anomaly} WIN={win} KF={kfold} (group {group_counter})\")\n",
    "\n",
    "                    overlap = OVERLAP_RATIO_DROOP if anomaly.upper() == \"DROOP\" else OVERLAP_RATIO\n",
    "                    an_pairs_all = get_anom_pairs(setup, anomaly)\n",
    "                    if not an_pairs_all:\n",
    "                        print(f\"[SKIP] {setup} {anomaly}: no anomaly RAW files\")\n",
    "                        continue\n",
    "\n",
    "                    df_anom = build_windowed_raw_means(an_pairs_all, setup=setup, win=win, label=anomaly,\n",
    "                                                       overlap_ratio=overlap)\n",
    "                    if df_anom.empty:\n",
    "                        print(f\"[SKIP] {setup} {anomaly} WIN={win}: anomaly windowed empty\")\n",
    "                        continue\n",
    "\n",
    "                    run_ids = sorted(df_anom[\"run_id\"].astype(str).unique())\n",
    "\n",
    "                    for rid in run_ids:\n",
    "                        dfa_run = df_anom[df_anom[\"run_id\"].astype(str) == rid].copy()\n",
    "                        if dfa_run.empty:\n",
    "                            continue\n",
    "\n",
    "                        wl = str(dfa_run[\"workload\"].iloc[0]) if \"workload\" in dfa_run.columns else \"UNK\"\n",
    "                        df_ben = ben_by_wl.get(wl, df_ben_all)\n",
    "                        if df_ben.empty:\n",
    "                            df_ben = df_ben_all\n",
    "\n",
    "                        df_eval, y_true = balanced_concat(df_ben, dfa_run, seed=SEED)\n",
    "\n",
    "                        Xe_all  = df_eval[Xb_cols].astype(float)\n",
    "                        Xe_base = apply_robust_scale(Xe_all, mu_s, sd_s, Q1, Q2)\n",
    "\n",
    "                        for idx_p, pct in enumerate(PCT_SWEEP, start=1):\n",
    "                            sel_raw = slice_by_percent(full_lists, pct)\n",
    "                            sel = intersect_selection_with_columns_robust(sel_raw, xb_cols_set)\n",
    "                            if (not sel.get(\"compute\")) or (not sel.get(\"memory\")) or (not sel.get(\"sensors\")):\n",
    "                                continue\n",
    "                            if anomaly.upper() == \"DROOP\":\n",
    "                                sel = force_cpu_voltage_family(sel, xb_cols_set)\n",
    "\n",
    "                            refs = _build_refs(Xb_base, sel, eps=1e-12)\n",
    "                            benign_norm = fit_parts_normalizer_on_benign(Xb_base, refs)\n",
    "\n",
    "                            val_idx = split_holdout(y_true, test_size=0.3, seed=SEED)\n",
    "\n",
    "                            scores_by_w = {}\n",
    "                            for _, wr in WEIGHT_TABLE.iterrows():\n",
    "                                w = (float(wr[\"wC\"]), float(wr[\"wM\"]), float(wr[\"wS\"]))\n",
    "                                scores_by_w[w] = score_four_methods_benign_norm(Xe_base, refs, w, benign_norm)\n",
    "\n",
    "                            best_w_for = pick_best_w_per_method_PRfirst(y_true, scores_by_w, val_idx)\n",
    "\n",
    "                            for method in METHOD_ORDER:\n",
    "                                w_star = best_w_for[method]\n",
    "                                y_score = score_four_methods_benign_norm(Xe_base, refs, w_star, benign_norm)[method]\n",
    "                                y_score = np.nan_to_num(y_score, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "                                if DROOP_BOOST and anomaly.upper() == \"DROOP\":\n",
    "                                    boost = droop_boost_score(Xe_base, sel)\n",
    "                                    y_score = (1.0 - DROOP_BOOST_ALPHA) * y_score + DROOP_BOOST_ALPHA * boost\n",
    "\n",
    "                                roc = float(np.clip(roc_auc_score(y_true, y_score), Y_DATA_MIN, Y_DATA_MAX))\n",
    "                                pr  = float(np.clip(average_precision_score(y_true, y_score), Y_DATA_MIN, Y_DATA_MAX))\n",
    "\n",
    "                                wid, wname, wsrc = _weight_lookup(w_star)\n",
    "\n",
    "                                all_rows.append({\n",
    "                                    \"setup\": setup,\n",
    "                                    \"anomaly\": anomaly,\n",
    "                                    \"win\": int(win),\n",
    "                                    \"kfold\": int(kfold),\n",
    "                                    \"pct\": int(pct),\n",
    "                                    \"run_id\": str(rid),\n",
    "                                    \"workload\": str(wl),\n",
    "                                    \"method\": method,\n",
    "                                    \"roc_auc\": roc,\n",
    "                                    \"auc_pr\": pr,\n",
    "                                    \"weight_case_id\": wid,\n",
    "                                    \"weight_case_name\": wname,\n",
    "                                    \"weight_case_source\": wsrc,\n",
    "                                    \"wC\": float(w_star[0]),\n",
    "                                    \"wM\": float(w_star[1]),\n",
    "                                    \"wS\": float(w_star[2]),\n",
    "                                })\n",
    "\n",
    "                            if idx_p % GC_EVERY_N_PCTS == 0:\n",
    "                                gc.collect()\n",
    "\n",
    "                    if group_counter % GC_EVERY_N_GROUPS == 0:\n",
    "                        gc.collect()\n",
    "\n",
    "    per_run_df = pd.DataFrame(all_rows)\n",
    "    out_all = RES_DIR / \"per_run_metrics_all_PIPELINE.csv\"\n",
    "    per_run_df.to_csv(out_all, index=False)\n",
    "    print(f\"\\n[OK] ALLRUNS metrics → {out_all}\")\n",
    "\n",
    "    for setup in SETUPS:\n",
    "        for win in WINS:\n",
    "            for kfold in KFOLDS:\n",
    "                build_workload_weight_tables(per_run_df, setup=setup, win=win, kfold=kfold)\n",
    "\n",
    "    write_all_dse_plots(per_run_df)\n",
    "    return per_run_df\n",
    "\n",
    "# ===============================================================================================\n",
    "# 13) MAIN\n",
    "# ===============================================================================================\n",
    "def main():\n",
    "    run_full_pipeline()\n",
    "    print(\"[DONE] Pipeline + DSE plots + weight tables per platform\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdce5a9-33c5-4d12-9e17-37bbd65694e3",
   "metadata": {},
   "source": [
    "### Print DSE stats (mean, IQR, min, max) for selected cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c11a2882-195a-44f0-a26d-dc652864abdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DSE summary (mean/IQR/min/max across PCT) — per platform’s chosen (WIN,KF) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setup</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>win</th>\n",
       "      <th>kfold</th>\n",
       "      <th>method</th>\n",
       "      <th>n_pcts</th>\n",
       "      <th>auc_pr_mean_of_medians</th>\n",
       "      <th>auc_pr_iqr_mean</th>\n",
       "      <th>auc_pr_min_over_pct</th>\n",
       "      <th>auc_pr_max_over_pct</th>\n",
       "      <th>roc_auc_mean_of_medians</th>\n",
       "      <th>roc_auc_iqr_mean</th>\n",
       "      <th>roc_auc_min_over_pct</th>\n",
       "      <th>roc_auc_max_over_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>dC_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.7445</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>dC_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.0854</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.7445</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>dE_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.7445</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>dE_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.0854</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.7445</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>RH</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>dC_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9467</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9294</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>RH</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>dC_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9360</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9155</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>RH</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>dE_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9467</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9294</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>RH</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>dE_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9360</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.9155</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>dC_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>0.1413</td>\n",
       "      <td>0.4225</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9719</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>dC_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.1194</td>\n",
       "      <td>0.4159</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9667</td>\n",
       "      <td>0.1105</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>dE_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>0.1413</td>\n",
       "      <td>0.4225</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9719</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>dE_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>0.1194</td>\n",
       "      <td>0.4159</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9667</td>\n",
       "      <td>0.1105</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>SPECTRE</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>dC_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.6038</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>SPECTRE</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>dC_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.6038</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>SPECTRE</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>dE_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.6038</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>SPECTRE</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>dE_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.6038</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   setup  anomaly   win  kfold method  n_pcts auc_pr_mean_of_medians  \\\n",
       "0   DDR4    DROOP   512      3  dC_aJ      10                 0.9996   \n",
       "1   DDR4    DROOP   512      3  dC_aM      10                 0.9989   \n",
       "2   DDR4    DROOP   512      3  dE_aJ      10                 0.9996   \n",
       "3   DDR4    DROOP   512      3  dE_aM      10                 0.9989   \n",
       "4   DDR4       RH   512      3  dC_aJ      10                 1.0000   \n",
       "5   DDR4       RH   512      3  dC_aM      10                 1.0000   \n",
       "6   DDR4       RH   512      3  dE_aJ      10                 1.0000   \n",
       "7   DDR4       RH   512      3  dE_aM      10                 1.0000   \n",
       "8   DDR5    DROOP  1024      5  dC_aJ      10                 0.9718   \n",
       "9   DDR5    DROOP  1024      5  dC_aM      10                 0.9701   \n",
       "10  DDR5    DROOP  1024      5  dE_aJ      10                 0.9718   \n",
       "11  DDR5    DROOP  1024      5  dE_aM      10                 0.9701   \n",
       "12  DDR5  SPECTRE  1024      5  dC_aJ      10                 1.0000   \n",
       "13  DDR5  SPECTRE  1024      5  dC_aM      10                 1.0000   \n",
       "14  DDR5  SPECTRE  1024      5  dE_aJ      10                 1.0000   \n",
       "15  DDR5  SPECTRE  1024      5  dE_aM      10                 1.0000   \n",
       "\n",
       "   auc_pr_iqr_mean auc_pr_min_over_pct auc_pr_max_over_pct  \\\n",
       "0           0.0899              0.6027              1.0000   \n",
       "1           0.0854              0.6027              1.0000   \n",
       "2           0.0899              0.6027              1.0000   \n",
       "3           0.0854              0.6027              1.0000   \n",
       "4           0.0000              0.9467              1.0000   \n",
       "5           0.0000              0.9360              1.0000   \n",
       "6           0.0000              0.9467              1.0000   \n",
       "7           0.0000              0.9360              1.0000   \n",
       "8           0.1413              0.4225              1.0000   \n",
       "9           0.1194              0.4159              1.0000   \n",
       "10          0.1413              0.4225              1.0000   \n",
       "11          0.1194              0.4159              1.0000   \n",
       "12          0.0049              0.6038              1.0000   \n",
       "13          0.0057              0.6038              1.0000   \n",
       "14          0.0049              0.6038              1.0000   \n",
       "15          0.0057              0.6038              1.0000   \n",
       "\n",
       "   roc_auc_mean_of_medians roc_auc_iqr_mean roc_auc_min_over_pct  \\\n",
       "0                   0.9995           0.0482               0.7445   \n",
       "1                   0.9988           0.0546               0.7445   \n",
       "2                   0.9995           0.0482               0.7445   \n",
       "3                   0.9988           0.0546               0.7445   \n",
       "4                   1.0000           0.0000               0.9294   \n",
       "5                   1.0000           0.0000               0.9155   \n",
       "6                   1.0000           0.0000               0.9294   \n",
       "7                   1.0000           0.0000               0.9155   \n",
       "8                   0.9719           0.1188               0.3889   \n",
       "9                   0.9667           0.1105               0.3704   \n",
       "10                  0.9719           0.1188               0.3889   \n",
       "11                  0.9667           0.1105               0.3704   \n",
       "12                  1.0000           0.0056               0.6636   \n",
       "13                  1.0000           0.0062               0.6636   \n",
       "14                  1.0000           0.0056               0.6636   \n",
       "15                  1.0000           0.0062               0.6636   \n",
       "\n",
       "   roc_auc_max_over_pct  \n",
       "0                1.0000  \n",
       "1                1.0000  \n",
       "2                1.0000  \n",
       "3                1.0000  \n",
       "4                1.0000  \n",
       "5                1.0000  \n",
       "6                1.0000  \n",
       "7                1.0000  \n",
       "8                1.0000  \n",
       "9                1.0000  \n",
       "10               1.0000  \n",
       "11               1.0000  \n",
       "12               1.0000  \n",
       "13               1.0000  \n",
       "14               1.0000  \n",
       "15               1.0000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Wrote CSV → /Users/hsiaopingni/octaneX_v7_4functions/Results/DSE_summary_per_platform.csv\n"
     ]
    }
   ],
   "source": [
    "# === Print DSE stats (mean, IQR, min, max) for selected cases — PER PLATFORM ===\n",
    "# This version uses your NEW pipeline outputs:\n",
    "#   - Results/per_run_metrics_all_PIPELINE.csv  (always produced)\n",
    "# and reads platform winners from:\n",
    "#   - Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#\n",
    "# For each platform (DDR4, DDR5), it prints DSE summary for the chosen (WIN,KF)\n",
    "# for each anomaly on that platform, across ALL PCT values, for each method.\n",
    "#\n",
    "# Summary per (setup, anomaly, win, kfold, method):\n",
    "#   - mean_of_medians over pct (median across run_id at each pct, then mean over pct)\n",
    "#   - mean IQR over pct\n",
    "#   - min_over_pct (global min of per-pct min across run_id)\n",
    "#   - max_over_pct (global max of per-pct max across run_id)\n",
    "#\n",
    "# Output:\n",
    "#   - prints a table (and writes CSV Results/DSE_summary_per_platform.csv)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT     = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES_DIR  = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "if RES_DIR.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\"):\n",
    "    RES_DIR = RES_DIR.parent\n",
    "\n",
    "PER_RUN = RES_DIR / \"per_run_metrics_all_PIPELINE.csv\"\n",
    "DETAILS = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "\n",
    "if not PER_RUN.exists():\n",
    "    raise FileNotFoundError(f\"Missing per-run pipeline CSV: {PER_RUN}\")\n",
    "if not DETAILS.exists():\n",
    "    raise FileNotFoundError(f\"Missing per-platform details CSV: {DETAILS}\")\n",
    "\n",
    "df = pd.read_csv(PER_RUN).copy()\n",
    "det = pd.read_csv(DETAILS).copy()\n",
    "\n",
    "# Normalize columns\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "det.columns = [c.lower() for c in det.columns]\n",
    "if \"best_method\" in det.columns and \"method\" not in det.columns:\n",
    "    det = det.rename(columns={\"best_method\":\"method\"})\n",
    "if \"best_pct_by_median\" not in det.columns and \"pct\" in det.columns:\n",
    "    det = det.rename(columns={\"pct\":\"best_pct_by_median\"})\n",
    "\n",
    "need_df = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"run_id\",\"method\",\"roc_auc\",\"auc_pr\"}\n",
    "miss = need_df - set(df.columns)\n",
    "if miss:\n",
    "    raise KeyError(f\"{PER_RUN} missing columns: {sorted(miss)}. Have: {list(df.columns)}\")\n",
    "\n",
    "need_det = {\"setup\",\"anomaly\",\"win\",\"kfold\"}\n",
    "miss2 = need_det - set(det.columns)\n",
    "if miss2:\n",
    "    raise KeyError(f\"{DETAILS} missing columns: {sorted(miss2)}. Have: {list(det.columns)}\")\n",
    "\n",
    "# Coerce numeric\n",
    "for c in [\"win\",\"kfold\",\"pct\",\"roc_auc\",\"auc_pr\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"method\"]).copy()\n",
    "df[\"roc_auc\"] = df[\"roc_auc\"].clip(0.0, 1.0)\n",
    "df[\"auc_pr\"]  = df[\"auc_pr\"].clip(0.0, 1.0)\n",
    "\n",
    "# ------------------ Build DSE stats at each pct ------------------\n",
    "def q1(x): return float(np.nanpercentile(x, 25))\n",
    "def q3(x): return float(np.nanpercentile(x, 75))\n",
    "\n",
    "keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"pct\"]\n",
    "dse = (df.groupby(keys, as_index=False)\n",
    "         .agg(\n",
    "             roc_auc_median=(\"roc_auc\",\"median\"),\n",
    "             roc_auc_q1=(\"roc_auc\", q1),\n",
    "             roc_auc_q3=(\"roc_auc\", q3),\n",
    "             roc_auc_min=(\"roc_auc\",\"min\"),\n",
    "             roc_auc_max=(\"roc_auc\",\"max\"),\n",
    "             auc_pr_median=(\"auc_pr\",\"median\"),\n",
    "             auc_pr_q1=(\"auc_pr\", q1),\n",
    "             auc_pr_q3=(\"auc_pr\", q3),\n",
    "             auc_pr_min=(\"auc_pr\",\"min\"),\n",
    "             auc_pr_max=(\"auc_pr\",\"max\"),\n",
    "             n_runs=(\"run_id\",\"nunique\"),\n",
    "         ))\n",
    "dse[\"roc_auc_iqr\"] = dse[\"roc_auc_q3\"] - dse[\"roc_auc_q1\"]\n",
    "dse[\"auc_pr_iqr\"]  = dse[\"auc_pr_q3\"]  - dse[\"auc_pr_q1\"]\n",
    "\n",
    "# ------------------ Helper: summarize over ALL PCT for a single case ------------------\n",
    "def _summarize_case_over_pct(df_case: pd.DataFrame) -> dict | None:\n",
    "    if df_case.empty:\n",
    "        return None\n",
    "\n",
    "    def agg(prefix):\n",
    "        arr_med = df_case[f\"{prefix}_median\"].to_numpy(dtype=float)\n",
    "        arr_min = df_case[f\"{prefix}_min\"].to_numpy(dtype=float)\n",
    "        arr_max = df_case[f\"{prefix}_max\"].to_numpy(dtype=float)\n",
    "        arr_iqr = df_case[f\"{prefix}_iqr\"].to_numpy(dtype=float)\n",
    "        return {\n",
    "            f\"{prefix}_mean_of_medians\": float(np.nanmean(arr_med)),\n",
    "            f\"{prefix}_iqr_mean\": float(np.nanmean(arr_iqr)),\n",
    "            f\"{prefix}_min_over_pct\": float(np.nanmin(arr_min)),\n",
    "            f\"{prefix}_max_over_pct\": float(np.nanmax(arr_max)),\n",
    "        }\n",
    "\n",
    "    out = {\n",
    "        \"pct_values\": sorted(df_case[\"pct\"].astype(int).unique().tolist()),\n",
    "        \"n_pcts\": int(df_case[\"pct\"].nunique()),\n",
    "        \"n_runs_median\": float(np.nanmedian(df_case[\"n_runs\"].astype(float).values)),\n",
    "    }\n",
    "    out.update(agg(\"auc_pr\"))\n",
    "    out.update(agg(\"roc_auc\"))\n",
    "    return out\n",
    "\n",
    "# ------------------ Per-platform selection from details CSV ------------------\n",
    "# Use (setup, win, kfold) from per-platform details (should be consistent per platform)\n",
    "platform_rows = []\n",
    "missing = []\n",
    "\n",
    "METHOD_ORDER = [\"dC_aJ\", \"dC_aM\", \"dE_aJ\", \"dE_aM\"]\n",
    "\n",
    "for setup in [\"DDR4\",\"DDR5\"]:\n",
    "    dplat = det[det[\"setup\"].astype(str).str.upper() == setup].copy()\n",
    "    if dplat.empty:\n",
    "        missing.append((setup, \"ALL\", \"NO_DETAILS\"))\n",
    "        continue\n",
    "\n",
    "    win_best = int(pd.to_numeric(dplat[\"win\"], errors=\"coerce\").dropna().iloc[0])\n",
    "    kf_best  = int(pd.to_numeric(dplat[\"kfold\"], errors=\"coerce\").dropna().iloc[0])\n",
    "\n",
    "    for anomaly in sorted(dplat[\"anomaly\"].astype(str).unique().tolist()):\n",
    "        for method in METHOD_ORDER:\n",
    "            sub = dse[\n",
    "                (dse[\"setup\"].astype(str).str.upper() == setup.upper()) &\n",
    "                (dse[\"anomaly\"].astype(str).str.upper() == str(anomaly).upper()) &\n",
    "                (pd.to_numeric(dse[\"win\"], errors=\"coerce\") == win_best) &\n",
    "                (pd.to_numeric(dse[\"kfold\"], errors=\"coerce\") == kf_best) &\n",
    "                (dse[\"method\"].astype(str) == method)\n",
    "            ].copy()\n",
    "\n",
    "            res = _summarize_case_over_pct(sub)\n",
    "            if res is None:\n",
    "                missing.append((setup, anomaly, f\"WIN{win_best}_KF{kf_best}_{method}\"))\n",
    "                continue\n",
    "\n",
    "            platform_rows.append({\n",
    "                \"setup\": setup,\n",
    "                \"anomaly\": anomaly,\n",
    "                \"win\": win_best,\n",
    "                \"kfold\": kf_best,\n",
    "                \"method\": method,\n",
    "                **res\n",
    "            })\n",
    "\n",
    "# ------------------ Present / save ------------------\n",
    "if platform_rows:\n",
    "    out = pd.DataFrame(platform_rows)\n",
    "    out_csv = RES_DIR / \"DSE_summary_per_platform.csv\"\n",
    "    out.to_csv(out_csv, index=False)\n",
    "\n",
    "    # Pretty print\n",
    "    disp_cols = [\n",
    "        \"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"n_pcts\",\n",
    "        \"auc_pr_mean_of_medians\",\"auc_pr_iqr_mean\",\"auc_pr_min_over_pct\",\"auc_pr_max_over_pct\",\n",
    "        \"roc_auc_mean_of_medians\",\"roc_auc_iqr_mean\",\"roc_auc_min_over_pct\",\"roc_auc_max_over_pct\",\n",
    "    ]\n",
    "    show = out[disp_cols].copy()\n",
    "\n",
    "    # Format numeric columns\n",
    "    num_cols = [c for c in show.columns if c not in (\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"n_pcts\")]\n",
    "    for c in num_cols:\n",
    "        show[c] = show[c].map(lambda x: f\"{float(x):.4f}\")\n",
    "\n",
    "    print(\"=== DSE summary (mean/IQR/min/max across PCT) — per platform’s chosen (WIN,KF) ===\")\n",
    "    try:\n",
    "        display(show)\n",
    "    except Exception:\n",
    "        print(show.to_string(index=False))\n",
    "\n",
    "    print(f\"\\n[OK] Wrote CSV → {out_csv}\")\n",
    "else:\n",
    "    print(\"No matching rows found for per-platform selected cases.\")\n",
    "\n",
    "if missing:\n",
    "    print(\"\\n[WARN] Missing combinations (no rows in per-run DSE aggregation):\")\n",
    "    for m in missing[:30]:\n",
    "        print(\"  - setup={}, anomaly={}, tag={}\".format(*m))\n",
    "    if len(missing) > 30:\n",
    "        print(f\"  ... and {len(missing)-30} more\")\n",
    "    print(\"Tip: verify that the per-run CSV includes these (setup, anomaly, win, kfold, method) rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1679040-8349-4323-aaa1-069ddc19d4f6",
   "metadata": {},
   "source": [
    "###  Find best Top-% (PCT) per (setup, anomaly, win, kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a8d1a18-22a0-44f2-8432-e1febeaa2221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wrote → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_pct_per_platform_WIN_KF.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setup</th>\n",
       "      <th>win</th>\n",
       "      <th>kfold</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>method</th>\n",
       "      <th>best_pct</th>\n",
       "      <th>auc_pr_median</th>\n",
       "      <th>roc_auc_median</th>\n",
       "      <th>auc_pr_iqr</th>\n",
       "      <th>roc_auc_iqr</th>\n",
       "      <th>n_runs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>dC_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>dC_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>dE_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>dE_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>RH</td>\n",
       "      <td>dC_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>RH</td>\n",
       "      <td>dC_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>RH</td>\n",
       "      <td>dE_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>RH</td>\n",
       "      <td>dE_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>dC_aJ</td>\n",
       "      <td>20</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>0.2914</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>dC_aM</td>\n",
       "      <td>20</td>\n",
       "      <td>0.9899</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.2570</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>dE_aJ</td>\n",
       "      <td>20</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>0.2914</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>DROOP</td>\n",
       "      <td>dE_aM</td>\n",
       "      <td>20</td>\n",
       "      <td>0.9899</td>\n",
       "      <td>0.9877</td>\n",
       "      <td>0.2570</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>SPECTRE</td>\n",
       "      <td>dC_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>SPECTRE</td>\n",
       "      <td>dC_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>SPECTRE</td>\n",
       "      <td>dE_aJ</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>SPECTRE</td>\n",
       "      <td>dE_aM</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   setup   win  kfold  anomaly method  best_pct auc_pr_median roc_auc_median  \\\n",
       "0   DDR4   512      3    DROOP  dC_aJ        10        1.0000         1.0000   \n",
       "1   DDR4   512      3    DROOP  dC_aM        10        1.0000         1.0000   \n",
       "2   DDR4   512      3    DROOP  dE_aJ        10        1.0000         1.0000   \n",
       "3   DDR4   512      3    DROOP  dE_aM        10        1.0000         1.0000   \n",
       "4   DDR4   512      3       RH  dC_aJ        10        1.0000         1.0000   \n",
       "5   DDR4   512      3       RH  dC_aM        10        1.0000         1.0000   \n",
       "6   DDR4   512      3       RH  dE_aJ        10        1.0000         1.0000   \n",
       "7   DDR4   512      3       RH  dE_aM        10        1.0000         1.0000   \n",
       "8   DDR5  1024      5    DROOP  dC_aJ        20        0.9971         0.9969   \n",
       "9   DDR5  1024      5    DROOP  dC_aM        20        0.9899         0.9877   \n",
       "10  DDR5  1024      5    DROOP  dE_aJ        20        0.9971         0.9969   \n",
       "11  DDR5  1024      5    DROOP  dE_aM        20        0.9899         0.9877   \n",
       "12  DDR5  1024      5  SPECTRE  dC_aJ        10        1.0000         1.0000   \n",
       "13  DDR5  1024      5  SPECTRE  dC_aM        10        1.0000         1.0000   \n",
       "14  DDR5  1024      5  SPECTRE  dE_aJ        10        1.0000         1.0000   \n",
       "15  DDR5  1024      5  SPECTRE  dE_aM        10        1.0000         1.0000   \n",
       "\n",
       "   auc_pr_iqr roc_auc_iqr  n_runs  \n",
       "0      0.0387      0.0353      13  \n",
       "1      0.0021      0.0021      13  \n",
       "2      0.0387      0.0353      13  \n",
       "3      0.0021      0.0021      13  \n",
       "4      0.0000      0.0000      13  \n",
       "5      0.0000      0.0000      13  \n",
       "6      0.0000      0.0000      13  \n",
       "7      0.0000      0.0000      13  \n",
       "8      0.2914      0.2222      13  \n",
       "9      0.2570      0.2160      13  \n",
       "10     0.2914      0.2222      13  \n",
       "11     0.2570      0.2160      13  \n",
       "12     0.0000      0.0000      13  \n",
       "13     0.0000      0.0000      13  \n",
       "14     0.0000      0.0000      13  \n",
       "15     0.0000      0.0000      13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Find best Top-% (PCT) per PLATFORM (setup) for the chosen (WIN,KF) ===\n",
    "# PER-PLATFORM version:\n",
    "#   - Reads per-run metrics from your NEW pipeline:\n",
    "#       Results/per_run_metrics_all_PIPELINE.csv\n",
    "#   - Uses per-platform chosen (WIN,KF) from:\n",
    "#       Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
    "#   - For each platform and each anomaly on that platform, finds BEST PCT\n",
    "#     at that platform's (WIN,KF), per METHOD, using:\n",
    "#       Sort key: AUCPR_median desc, then ROC_median desc, then smaller pct\n",
    "#\n",
    "# Outputs:\n",
    "#   - Prints a table\n",
    "#   - Writes Results/BEST_pct_per_platform_WIN_KF.csv\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT    = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES_DIR = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "if RES_DIR.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\"):\n",
    "    RES_DIR = RES_DIR.parent\n",
    "\n",
    "PER_RUN   = RES_DIR / \"per_run_metrics_all_PIPELINE.csv\"\n",
    "PLAT_BEST = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform.csv\"\n",
    "\n",
    "if not PER_RUN.exists():\n",
    "    raise FileNotFoundError(f\"Missing pipeline per-run CSV: {PER_RUN}\")\n",
    "if not PLAT_BEST.exists():\n",
    "    raise FileNotFoundError(f\"Missing per-platform winners CSV: {PLAT_BEST}\")\n",
    "\n",
    "df = pd.read_csv(PER_RUN).copy()\n",
    "pb = pd.read_csv(PLAT_BEST).copy()\n",
    "\n",
    "# Normalize\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "pb.columns = [c.lower() for c in pb.columns]\n",
    "\n",
    "need_df = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"run_id\",\"method\",\"roc_auc\",\"auc_pr\"}\n",
    "miss = need_df - set(df.columns)\n",
    "if miss:\n",
    "    raise KeyError(f\"{PER_RUN} missing columns: {sorted(miss)}. Have: {list(df.columns)}\")\n",
    "\n",
    "need_pb = {\"setup\",\"win\",\"kfold\"}\n",
    "miss2 = need_pb - set(pb.columns)\n",
    "if miss2:\n",
    "    raise KeyError(f\"{PLAT_BEST} missing columns: {sorted(miss2)}. Have: {list(pb.columns)}\")\n",
    "\n",
    "# Ensure numeric\n",
    "for c in [\"win\",\"kfold\",\"pct\",\"roc_auc\",\"auc_pr\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df = df.dropna(subset=[\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"method\"]).copy()\n",
    "df[\"roc_auc\"] = df[\"roc_auc\"].clip(0.0, 1.0)\n",
    "df[\"auc_pr\"]  = df[\"auc_pr\"].clip(0.0, 1.0)\n",
    "\n",
    "METHOD_ORDER = [\"dC_aJ\", \"dC_aM\", \"dE_aJ\", \"dE_aM\"]\n",
    "df = df[df[\"method\"].isin(METHOD_ORDER)].copy()\n",
    "\n",
    "# ---- Build pct-level medians per (setup, anomaly, win, kfold, method, pct) ----\n",
    "def q1(x): return float(np.nanpercentile(x, 25))\n",
    "def q3(x): return float(np.nanpercentile(x, 75))\n",
    "\n",
    "keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"pct\"]\n",
    "sumdf = (df.groupby(keys, as_index=False)\n",
    "           .agg(\n",
    "               auc_pr_median=(\"auc_pr\",\"median\"),\n",
    "               roc_auc_median=(\"roc_auc\",\"median\"),\n",
    "               auc_pr_q1=(\"auc_pr\", q1),\n",
    "               auc_pr_q3=(\"auc_pr\", q3),\n",
    "               roc_auc_q1=(\"roc_auc\", q1),\n",
    "               roc_auc_q3=(\"roc_auc\", q3),\n",
    "               n_runs=(\"run_id\",\"nunique\"),\n",
    "           ))\n",
    "sumdf[\"auc_pr_iqr\"]  = sumdf[\"auc_pr_q3\"]  - sumdf[\"auc_pr_q1\"]\n",
    "sumdf[\"roc_auc_iqr\"] = sumdf[\"roc_auc_q3\"] - sumdf[\"roc_auc_q1\"]\n",
    "sumdf[\"auc_pr_median_filled\"]  = sumdf[\"auc_pr_median\"].fillna(-np.inf)\n",
    "sumdf[\"roc_auc_median_filled\"] = sumdf[\"roc_auc_median\"].fillna(-np.inf)\n",
    "\n",
    "# ---- Per platform: choose (WIN,KF) and then best pct per anomaly×method ----\n",
    "rows = []\n",
    "for _, r in pb.iterrows():\n",
    "    setup = str(r[\"setup\"]).strip()\n",
    "    win_best = int(pd.to_numeric(r[\"win\"], errors=\"coerce\"))\n",
    "    kf_best  = int(pd.to_numeric(r[\"kfold\"], errors=\"coerce\"))\n",
    "\n",
    "    sub_plat = sumdf[\n",
    "        (sumdf[\"setup\"].astype(str).str.upper() == setup.upper()) &\n",
    "        (pd.to_numeric(sumdf[\"win\"], errors=\"coerce\") == win_best) &\n",
    "        (pd.to_numeric(sumdf[\"kfold\"], errors=\"coerce\") == kf_best)\n",
    "    ].copy()\n",
    "    if sub_plat.empty:\n",
    "        continue\n",
    "\n",
    "    for anomaly in sorted(sub_plat[\"anomaly\"].astype(str).unique().tolist()):\n",
    "        for method in METHOD_ORDER:\n",
    "            sub = sub_plat[\n",
    "                (sub_plat[\"anomaly\"].astype(str).str.upper() == str(anomaly).upper()) &\n",
    "                (sub_plat[\"method\"] == method)\n",
    "            ].copy()\n",
    "            if sub.empty:\n",
    "                continue\n",
    "\n",
    "            sub = sub.sort_values(\n",
    "                [\"auc_pr_median_filled\", \"roc_auc_median_filled\", \"pct\"],\n",
    "                ascending=[False, False, True]\n",
    "            )\n",
    "            best = sub.iloc[0]\n",
    "            rows.append({\n",
    "                \"setup\": setup,\n",
    "                \"win\": win_best,\n",
    "                \"kfold\": kf_best,\n",
    "                \"anomaly\": anomaly,\n",
    "                \"method\": method,\n",
    "                \"best_pct\": int(best[\"pct\"]),\n",
    "                \"auc_pr_median\": float(best[\"auc_pr_median\"]),\n",
    "                \"roc_auc_median\": float(best[\"roc_auc_median\"]),\n",
    "                \"auc_pr_iqr\": float(best[\"auc_pr_iqr\"]),\n",
    "                \"roc_auc_iqr\": float(best[\"roc_auc_iqr\"]),\n",
    "                \"n_runs\": int(best[\"n_runs\"]),\n",
    "            })\n",
    "\n",
    "out = pd.DataFrame(rows).sort_values([\"setup\",\"anomaly\",\"method\"]).reset_index(drop=True)\n",
    "OUT_CSV = RES_DIR / \"BEST_pct_per_platform_WIN_KF.csv\"\n",
    "out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"[OK] Wrote → {OUT_CSV}\")\n",
    "\n",
    "# Pretty print\n",
    "if out.empty:\n",
    "    print(\"[WARN] No rows produced (check that per_run_metrics_all_PIPELINE.csv contains those WIN/KF combos).\")\n",
    "else:\n",
    "    show = out.copy()\n",
    "    for c in [\"auc_pr_median\",\"roc_auc_median\",\"auc_pr_iqr\",\"roc_auc_iqr\"]:\n",
    "        show[c] = show[c].map(lambda x: f\"{x:.4f}\")\n",
    "    try:\n",
    "        display(show)\n",
    "    except Exception:\n",
    "        print(show.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc9c86-308b-4c25-8662-eacba036a383",
   "metadata": {},
   "source": [
    "## Explainability results in Overleaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09a6c490-22a1-4537-b185-e5870a27ab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_vs_CPMI_agreement_PLATFORM.csv\n",
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/figs/Explainability/SHAP_vs_CPMI_agreement_PLATFORM.png\n",
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/tables/SHAP_vs_CPMI_agreement_PLATFORM_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# === PER-PLATFORM SHAP–CPMI agreement (top-% Jaccard) + plot for Overleaf ===\n",
    "# UPDATED to be PER PLATFORM (DDR4, DDR5), not per test case.\n",
    "#\n",
    "# What it does:\n",
    "#   - Uses per-platform winners/configs from:\n",
    "#       Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#     (gives anomaly-specific best pct + method at the platform-chosen WIN/KF)\n",
    "#   - Loads SHAP per anomaly from:\n",
    "#       Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN<w>_KF<k>_PCT<p>_M<method>.csv\n",
    "#   - Loads CP-MI rank lists from:\n",
    "#       FeatureRankOUT/<setup>_<win>_<kfold>_0_<subspace>.csv\n",
    "#   - Computes Jaccard overlap between:\n",
    "#       top-% CP-MI features  vs  top-% SHAP features\n",
    "#     for each subspace, then averages across subspaces to get a platform curve.\n",
    "#\n",
    "# Outputs:\n",
    "#   - CSV: Results/Explainability_SHAP_BestPlatforms/SHAP_vs_CPMI_agreement_PLATFORM.csv\n",
    "#   - Figure: <ROOT>/figs/Explainability/SHAP_vs_CPMI_agreement_PLATFORM.png\n",
    "#   - (optional) per-platform tables: <ROOT>/tables/SHAP_vs_CPMI_agreement_PLATFORM_summary.csv\n",
    "#\n",
    "# Notes:\n",
    "#   - Agreement metric is Jaccard(top-k CP-MI, top-k SHAP) where k is computed separately for each list.\n",
    "#   - Platform aggregation:\n",
    "#       For each (setup, subspace, pct): average Jaccard across that platform's anomalies\n",
    "#       Then for plotting: average across subspaces (compute/memory/sensors)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, re\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "ROOT   = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES    = ROOT / \"Results\"\n",
    "EXPL   = RES / \"Explainability_SHAP_BestPlatforms\"\n",
    "DETAIL = RES / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "\n",
    "FIGDIR = ROOT / \"figs\" / \"Explainability\"    # copy to Overleaf/figs/Explainability\n",
    "TABDIR = ROOT / \"tables\"                     # copy to Overleaf/tables\n",
    "FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "TABDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not DETAIL.exists():\n",
    "    raise FileNotFoundError(f\"Missing platform details CSV: {DETAIL}\")\n",
    "if not EXPL.exists():\n",
    "    raise FileNotFoundError(f\"Missing SHAP platform explainability dir: {EXPL}\")\n",
    "\n",
    "# -------------------- Params --------------------\n",
    "SUBSPACES = (\"compute\",\"memory\",\"sensors\")\n",
    "PCTS = list(range(10, 101, 10))\n",
    "\n",
    "def _norm(s):\n",
    "    s = re.sub(r\"\\s+\",\"_\",str(s)).lower().replace(\"%\",\"pct\")\n",
    "    return re.sub(r\"[^a-z0-9_]+\",\"_\", s).strip(\"_\")\n",
    "\n",
    "def _cpmi_path(setup, win, kf, sub):\n",
    "    return ROOT / \"FeatureRankOUT\" / f\"{setup}_{win}_{kf}_0_{sub}.csv\"\n",
    "\n",
    "def _read_ranklist(p: Path):\n",
    "    if not p.exists():\n",
    "        return []\n",
    "    df = pd.read_csv(p)\n",
    "    # first column is usually feature; handle feature_display too\n",
    "    col0 = df.columns[0]\n",
    "    feats = df[col0].astype(str).tolist()\n",
    "    return [_norm(x) for x in feats]\n",
    "\n",
    "def _find_shap_bestplat(setup, anomaly, win, kf, pct, method):\n",
    "    # exact name from BestPlatforms script\n",
    "    p = EXPL / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kf}_PCT{pct}_M{method}.csv\"\n",
    "    if p.exists():\n",
    "        return p\n",
    "    # fallback: any pct for same (setup, anomaly, win, kf, method)\n",
    "    hits = sorted(glob.glob(str(EXPL / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kf}_PCT*_M{method}.csv\")))\n",
    "    return Path(hits[0]) if hits else None\n",
    "\n",
    "def _read_shap_norm(p: Path):\n",
    "    if (p is None) or (not p.exists()):\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(p)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    fcol = cols.get(\"feature\", df.columns[0])\n",
    "    subc = cols.get(\"subspace\", None)\n",
    "    # choose shap column robustly\n",
    "    if \"shap_mean_abs\" in cols:\n",
    "        scol = cols[\"shap_mean_abs\"]\n",
    "    else:\n",
    "        shap_like = [c for c in df.columns if \"shap\" in c.lower()]\n",
    "        scol = shap_like[0] if shap_like else df.columns[-1]\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"feature_norm\": df[fcol].astype(str).map(_norm),\n",
    "        \"shap\": pd.to_numeric(df[scol], errors=\"coerce\").fillna(0.0),\n",
    "        \"subspace\": df[subc].astype(str).str.lower() if subc else \"compute\",\n",
    "    })\n",
    "    out[\"rank\"] = out[\"shap\"].rank(ascending=False, method=\"dense\").astype(int)\n",
    "    return out\n",
    "\n",
    "def _jaccard(top1, top2):\n",
    "    inter = len(top1 & top2)\n",
    "    denom = max(1, len(top1 | top2))\n",
    "    return inter / denom\n",
    "\n",
    "def agreement_for_platform(setup: str, dplat: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns a long DataFrame with columns:\n",
    "      setup, win, kfold, anomaly, subspace, pct, jaccard\n",
    "    \"\"\"\n",
    "    setup = str(setup).strip()\n",
    "    if dplat.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Platform WIN/KF should be consistent across anomalies\n",
    "    win = int(pd.to_numeric(dplat[\"win\"], errors=\"coerce\").dropna().iloc[0])\n",
    "    kf  = int(pd.to_numeric(dplat[\"kfold\"], errors=\"coerce\").dropna().iloc[0])\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for _, r in dplat.iterrows():\n",
    "        anomaly = str(r[\"anomaly\"]).strip()\n",
    "        pct     = int(pd.to_numeric(r[\"best_pct_by_median\"], errors=\"coerce\"))\n",
    "        method  = str(r[\"method\"]).strip()\n",
    "\n",
    "        shap_csv = _find_shap_bestplat(setup, anomaly, win, kf, pct, method)\n",
    "        shap = _read_shap_norm(shap_csv)\n",
    "        if shap.empty:\n",
    "            continue\n",
    "\n",
    "        for sub in SUBSPACES:\n",
    "            cpmi = _read_ranklist(_cpmi_path(setup, win, kf, sub))\n",
    "            if not cpmi:\n",
    "                continue\n",
    "\n",
    "            shap_sub = shap[shap[\"subspace\"].astype(str).str.lower() == sub].copy()\n",
    "            shap_ranked = shap_sub.sort_values(\"rank\")[\"feature_norm\"].tolist()\n",
    "            if not shap_ranked:\n",
    "                continue\n",
    "\n",
    "            for pct2 in PCTS:\n",
    "                k1 = max(1, int(np.ceil(len(cpmi) * (pct2/100))))\n",
    "                k2 = max(1, int(np.ceil(len(shap_ranked) * (pct2/100))))\n",
    "                top1 = set(cpmi[:k1])\n",
    "                top2 = set(shap_ranked[:k2])\n",
    "                rows.append({\n",
    "                    \"setup\": setup,\n",
    "                    \"win\": win,\n",
    "                    \"kfold\": kf,\n",
    "                    \"anomaly\": anomaly,\n",
    "                    \"subspace\": sub,\n",
    "                    \"pct\": int(pct2),\n",
    "                    \"jaccard\": float(_jaccard(top1, top2)),\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# -------------------- Load per-platform detail rows --------------------\n",
    "det = pd.read_csv(DETAIL).copy()\n",
    "det.columns = [c.lower() for c in det.columns]\n",
    "if \"best_method\" in det.columns and \"method\" not in det.columns:\n",
    "    det = det.rename(columns={\"best_method\":\"method\"})\n",
    "if \"best_pct_by_median\" not in det.columns and \"pct\" in det.columns:\n",
    "    det = det.rename(columns={\"pct\":\"best_pct_by_median\"})\n",
    "\n",
    "need = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"}\n",
    "missing = need - set(det.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"{DETAIL} missing columns: {sorted(missing)}. Have: {list(det.columns)}\")\n",
    "\n",
    "# -------------------- Run for each platform --------------------\n",
    "agreeds = []\n",
    "for setup in [\"DDR4\",\"DDR5\"]:\n",
    "    dplat = det[det[\"setup\"].astype(str).str.upper() == setup].copy()\n",
    "    if dplat.empty:\n",
    "        print(\"[WARN] No detail rows for\", setup)\n",
    "        continue\n",
    "    dfp = agreement_for_platform(setup, dplat)\n",
    "    if not dfp.empty:\n",
    "        agreeds.append(dfp)\n",
    "\n",
    "agree = pd.concat(agreeds, ignore_index=True) if agreeds else pd.DataFrame()\n",
    "\n",
    "out_csv = EXPL / \"SHAP_vs_CPMI_agreement_PLATFORM.csv\"\n",
    "agree.to_csv(out_csv, index=False)\n",
    "print(\"Wrote\", out_csv)\n",
    "\n",
    "# -------------------- Plot: PLATFORM curves (averaged across anomalies + subspaces) --------------------\n",
    "if not agree.empty:\n",
    "    plt.figure(figsize=(6.6,4.3), dpi=160)\n",
    "\n",
    "    # platform avg:\n",
    "    #   for each setup,pct: mean over anomalies and subspaces\n",
    "    for setup, g in agree.groupby(\"setup\"):\n",
    "        gg = (g.groupby(\"pct\", as_index=False)[\"jaccard\"].mean())\n",
    "        win = int(g[\"win\"].iloc[0])\n",
    "        kf  = int(g[\"kfold\"].iloc[0])\n",
    "        label = f\"{setup} (W{win},K{kf})\"\n",
    "        plt.plot(gg[\"pct\"], gg[\"jaccard\"], lw=2.4, marker=\"o\", label=label)\n",
    "\n",
    "    plt.xlabel(\"Top-% threshold\")\n",
    "    plt.ylabel(\"SHAP–CPMI agreement (Jaccard)\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.legend(loc=\"lower right\", fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    fig_out = FIGDIR / \"SHAP_vs_CPMI_agreement_PLATFORM.png\"\n",
    "    plt.savefig(fig_out, dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Wrote\", fig_out)\n",
    "\n",
    "    # Optional summary table for Overleaf\n",
    "    summary = (agree.groupby([\"setup\",\"pct\"], as_index=False)[\"jaccard\"].mean())\n",
    "    summ_out = TABDIR / \"SHAP_vs_CPMI_agreement_PLATFORM_summary.csv\"\n",
    "    summary.to_csv(summ_out, index=False)\n",
    "    print(\"Wrote\", summ_out)\n",
    "else:\n",
    "    print(\"[WARN] No agreement data produced (missing SHAP or CP-MI ranks).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50522edc-8bf7-41c1-ab35-4eb184e1c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_PLATFORM_topK_DDR4.csv\n",
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/figs/Explainability/SHAP_top15_PLATFORM_DDR4.png\n",
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/SHAP_PLATFORM_topK_DDR5.csv\n",
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/figs/Explainability/SHAP_top15_PLATFORM_DDR5.png\n"
     ]
    }
   ],
   "source": [
    "# === PER-PLATFORM horizontal bar chart of top-k SHAP features (ONE per platform) ===\n",
    "# This aggregates SHAP across the platform's anomalies (equal-weight mean; missing=0)\n",
    "# using the chosen best configs in:\n",
    "#   Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#\n",
    "# Inputs:\n",
    "#   - Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#   - Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN<w>_KF<k>_PCT<p>_M<method>.csv\n",
    "#\n",
    "# Outputs:\n",
    "#   - <ROOT>/figs/Explainability/SHAP_topK_PLATFORM_<setup>.png\n",
    "#   - (optional) Results/Explainability_SHAP_BestPlatforms/SHAP_PLATFORM_topK_<setup>.csv\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, re\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "ROOT   = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES    = ROOT / \"Results\"\n",
    "EXPL   = RES / \"Explainability_SHAP_BestPlatforms\"\n",
    "DETAIL = RES / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "\n",
    "FIGDIR = ROOT / \"figs\" / \"Explainability\"\n",
    "FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not DETAIL.exists():\n",
    "    raise FileNotFoundError(f\"Missing platform details CSV: {DETAIL}\")\n",
    "if not EXPL.exists():\n",
    "    raise FileNotFoundError(f\"Missing SHAP platform explainability dir: {EXPL}\")\n",
    "\n",
    "TOPK = 15\n",
    "SUBSPACES = (\"compute\",\"memory\",\"sensors\")\n",
    "\n",
    "# Let matplotlib handle default colors; we only segment bars by subspace\n",
    "COLOR_MAP = {\"compute\": None, \"memory\": None, \"sensors\": None}\n",
    "\n",
    "def _norm(s):\n",
    "    s = re.sub(r\"\\s+\",\"_\",str(s)).lower().replace(\"%\",\"pct\")\n",
    "    return re.sub(r\"[^a-z0-9_]+\",\"_\", s).strip(\"_\")\n",
    "\n",
    "def _find_shap_bestplat(setup, anomaly, win, kf, pct, method):\n",
    "    p = EXPL / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kf}_PCT{pct}_M{method}.csv\"\n",
    "    if p.exists():\n",
    "        return p\n",
    "    hits = sorted(glob.glob(str(EXPL / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kf}_PCT*_M{method}.csv\")))\n",
    "    return Path(hits[0]) if hits else None\n",
    "\n",
    "def _read_shap_norm(p: Path):\n",
    "    if (p is None) or (not p.exists()):\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(p)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    fcol = cols.get(\"feature\", df.columns[0])\n",
    "    subc = cols.get(\"subspace\", None)\n",
    "    if \"shap_mean_abs\" in cols:\n",
    "        scol = cols[\"shap_mean_abs\"]\n",
    "    else:\n",
    "        shap_like = [c for c in df.columns if \"shap\" in c.lower()]\n",
    "        scol = shap_like[0] if shap_like else df.columns[-1]\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"feature\": df[fcol].astype(str),\n",
    "        \"feature_norm\": df[fcol].astype(str).map(_norm),\n",
    "        \"shap\": pd.to_numeric(df[scol], errors=\"coerce\").fillna(0.0),\n",
    "        \"subspace\": df[subc].astype(str).str.lower() if subc else \"compute\",\n",
    "    })\n",
    "    # If duplicates, keep max SHAP per (feature_norm, subspace)\n",
    "    out = (out.groupby([\"feature_norm\",\"subspace\"], as_index=False)[\"shap\"].max()\n",
    "             .merge(out.drop_duplicates([\"feature_norm\",\"subspace\"])[[\"feature_norm\",\"subspace\",\"feature\"]],\n",
    "                    on=[\"feature_norm\",\"subspace\"], how=\"left\"))\n",
    "    return out\n",
    "\n",
    "def _aggregate_platform_shap(details_rows: pd.DataFrame, setup: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns: DataFrame(feature_norm, feature_display, subspace, shap_platform)\n",
    "    where shap_platform is mean across anomalies (missing=0).\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    # platform win/kfold should be consistent across anomalies\n",
    "    win = int(pd.to_numeric(details_rows[\"win\"], errors=\"coerce\").dropna().iloc[0])\n",
    "    kf  = int(pd.to_numeric(details_rows[\"kfold\"], errors=\"coerce\").dropna().iloc[0])\n",
    "\n",
    "    for _, r in details_rows.iterrows():\n",
    "        anomaly = str(r[\"anomaly\"]).strip()\n",
    "        pct     = int(pd.to_numeric(r[\"best_pct_by_median\"], errors=\"coerce\"))\n",
    "        method  = str(r[\"method\"]).strip()\n",
    "\n",
    "        p = _find_shap_bestplat(setup, anomaly, win, kf, pct, method)\n",
    "        d = _read_shap_norm(p)\n",
    "        if d.empty:\n",
    "            continue\n",
    "        d = d.rename(columns={\"shap\": f\"shap_{anomaly}\"})\n",
    "        parts.append(d)\n",
    "\n",
    "    if not parts:\n",
    "        return pd.DataFrame(columns=[\"feature_norm\",\"feature_display\",\"subspace\",\"shap_platform\"])\n",
    "\n",
    "    merged = None\n",
    "    for d in parts:\n",
    "        if merged is None:\n",
    "            merged = d.copy()\n",
    "        else:\n",
    "            merged = merged.merge(d, on=[\"feature_norm\",\"subspace\"], how=\"outer\", suffixes=(\"\", \"_r\"))\n",
    "\n",
    "            # Keep a display feature column if present\n",
    "            if \"feature_r\" in merged.columns:\n",
    "                merged[\"feature\"] = merged[\"feature\"].fillna(merged[\"feature_r\"])\n",
    "                merged = merged.drop(columns=[\"feature_r\"])\n",
    "\n",
    "    shap_cols = [c for c in merged.columns if c.startswith(\"shap_\")]\n",
    "    merged[shap_cols] = merged[shap_cols].fillna(0.0)\n",
    "\n",
    "    merged[\"shap_platform\"] = merged[shap_cols].mean(axis=1)  # equal-weight across anomalies\n",
    "    merged = merged.rename(columns={\"feature\": \"feature_display\"})\n",
    "    return merged[[\"feature_norm\",\"feature_display\",\"subspace\",\"shap_platform\"]].copy()\n",
    "\n",
    "def plot_platform_shap_bar(setup: str, details_rows: pd.DataFrame):\n",
    "    plat = _aggregate_platform_shap(details_rows, setup)\n",
    "    if plat.empty:\n",
    "        print(\"[SKIP] No platform SHAP aggregated for\", setup)\n",
    "        return\n",
    "\n",
    "    # Save topk CSV for inspection\n",
    "    top_csv = EXPL / f\"SHAP_PLATFORM_topK_{setup}.csv\"\n",
    "    plat.sort_values(\"shap_platform\", ascending=False).head(200).to_csv(top_csv, index=False)\n",
    "    print(\"Wrote\", top_csv)\n",
    "\n",
    "    g = plat.sort_values(\"shap_platform\", ascending=False).head(TOPK).copy()\n",
    "    g[\"subspace\"] = g[\"subspace\"].astype(str).str.lower()\n",
    "    y = np.arange(len(g))[::-1]\n",
    "\n",
    "    plt.figure(figsize=(7.2, 5.2), dpi=160)\n",
    "\n",
    "    # Draw bars grouped by subspace (default colors)\n",
    "    for sub, gi in g.groupby(\"subspace\", sort=False):\n",
    "        idx = g.index.isin(gi.index)\n",
    "        plt.barh(y[idx], g.loc[idx, \"shap_platform\"], label=sub.capitalize())\n",
    "\n",
    "    plt.yticks(y, g[\"feature_norm\"].iloc[::-1])\n",
    "    plt.xlabel(\"SHAP (mean |value|) • platform-avg\")\n",
    "    plt.ylabel(\"Feature (normalized)\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out = FIGDIR / f\"SHAP_top{TOPK}_PLATFORM_{setup}.png\"\n",
    "    plt.savefig(out, dpi=300)\n",
    "    plt.close()\n",
    "    print(\"Wrote\", out)\n",
    "\n",
    "# -------------------- Load details and run for DDR4/DDR5 --------------------\n",
    "det = pd.read_csv(DETAIL).copy()\n",
    "det.columns = [c.lower() for c in det.columns]\n",
    "if \"best_method\" in det.columns and \"method\" not in det.columns:\n",
    "    det = det.rename(columns={\"best_method\":\"method\"})\n",
    "if \"best_pct_by_median\" not in det.columns and \"pct\" in det.columns:\n",
    "    det = det.rename(columns={\"pct\":\"best_pct_by_median\"})\n",
    "\n",
    "need = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"}\n",
    "missing = need - set(det.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"{DETAIL} missing columns: {sorted(missing)}. Have: {list(det.columns)}\")\n",
    "\n",
    "for setup in [\"DDR4\", \"DDR5\"]:\n",
    "    dplat = det[det[\"setup\"].astype(str).str.upper() == setup].copy()\n",
    "    if dplat.empty:\n",
    "        print(\"[WARN] No detail rows for\", setup)\n",
    "        continue\n",
    "    plot_platform_shap_bar(setup, dplat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b4765ed-4e33-4152-97e1-ac27094ef5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/tables/explainability_hybrid_octane_summary_PLATFORM.tex\n"
     ]
    }
   ],
   "source": [
    "# === PER-PLATFORM LaTeX table export: \"best PCT\" DSE summary (AUC-PR / ROC-AUC) ===\n",
    "# This replaces your old \"cases = [...]\" test-case list with PER-PLATFORM selection.\n",
    "#\n",
    "# Uses NEW pipeline source-of-truth:\n",
    "#   Results/per_run_metrics_all_PIPELINE.csv\n",
    "# And per-platform chosen (WIN,KF):\n",
    "#   Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
    "#\n",
    "# For each platform (DDR4, DDR5) and each anomaly on that platform:\n",
    "#   - Restrict to the platform's chosen (WIN,KF)\n",
    "#   - Choose BEST PCT by:\n",
    "#       AUCPR_median desc, then ROC_median desc, then smaller pct\n",
    "#   - Report:\n",
    "#       Best_AUC_PR = median ± IQR\n",
    "#       Best_ROC_AUC = median ± IQR\n",
    "#\n",
    "# Also adds a TOTAL row (mean ± std of the medians across platform-anomaly rows)\n",
    "#\n",
    "# Outputs:\n",
    "#   tables/explainability_hybrid_octane_summary_PLATFORM.tex\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT   = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES    = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "if RES.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\"):\n",
    "    RES = RES.parent\n",
    "\n",
    "TABDIR = Path(globals().get(\"TABDIR\", ROOT / \"tables\"))\n",
    "TABDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PER_RUN   = RES / \"per_run_metrics_all_PIPELINE.csv\"\n",
    "PLAT_BEST = RES / \"BEST_in_DesignSpace_Post_per_platform.csv\"\n",
    "\n",
    "assert PER_RUN.exists(), f\"Missing per-run CSV: {PER_RUN}\"\n",
    "assert PLAT_BEST.exists(), f\"Missing per-platform winners CSV: {PLAT_BEST}\"\n",
    "\n",
    "pr = pd.read_csv(PER_RUN).copy()\n",
    "pb = pd.read_csv(PLAT_BEST).copy()\n",
    "\n",
    "# normalize cols\n",
    "pr.columns = [c.lower() for c in pr.columns]\n",
    "pb.columns = [c.lower() for c in pb.columns]\n",
    "\n",
    "need_pr = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"method\",\"run_id\",\"auc_pr\",\"roc_auc\"}\n",
    "miss = need_pr - set(pr.columns)\n",
    "if miss:\n",
    "    raise KeyError(f\"{PER_RUN} missing columns: {sorted(miss)}. Have: {list(pr.columns)}\")\n",
    "\n",
    "need_pb = {\"setup\",\"win\",\"kfold\"}\n",
    "miss2 = need_pb - set(pb.columns)\n",
    "if miss2:\n",
    "    raise KeyError(f\"{PLAT_BEST} missing columns: {sorted(miss2)}. Have: {list(pb.columns)}\")\n",
    "\n",
    "# numeric\n",
    "for c in [\"win\",\"kfold\",\"pct\",\"auc_pr\",\"roc_auc\"]:\n",
    "    pr[c] = pd.to_numeric(pr[c], errors=\"coerce\")\n",
    "\n",
    "pr = pr.dropna(subset=[\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"method\"]).copy()\n",
    "pr[\"auc_pr\"]  = pr[\"auc_pr\"].clip(0.0, 1.0)\n",
    "pr[\"roc_auc\"] = pr[\"roc_auc\"].clip(0.0, 1.0)\n",
    "\n",
    "# summarize per pct (median/IQR across run_id)\n",
    "def q1(x): return float(np.nanpercentile(x, 25))\n",
    "def q3(x): return float(np.nanpercentile(x, 75))\n",
    "\n",
    "keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"pct\"]\n",
    "dse = (pr.groupby(keys, as_index=False)\n",
    "         .agg(\n",
    "             auc_pr_median=(\"auc_pr\",\"median\"),\n",
    "             auc_pr_q1=(\"auc_pr\", q1),\n",
    "             auc_pr_q3=(\"auc_pr\", q3),\n",
    "             roc_auc_median=(\"roc_auc\",\"median\"),\n",
    "             roc_auc_q1=(\"roc_auc\", q1),\n",
    "             roc_auc_q3=(\"roc_auc\", q3),\n",
    "             n_runs=(\"run_id\",\"nunique\"),\n",
    "         ))\n",
    "dse[\"auc_pr_iqr\"]  = dse[\"auc_pr_q3\"]  - dse[\"auc_pr_q1\"]\n",
    "dse[\"roc_auc_iqr\"] = dse[\"roc_auc_q3\"] - dse[\"roc_auc_q1\"]\n",
    "\n",
    "dse[\"auc_pr_median_filled\"]  = dse[\"auc_pr_median\"].fillna(-np.inf)\n",
    "dse[\"roc_auc_median_filled\"] = dse[\"roc_auc_median\"].fillna(-np.inf)\n",
    "\n",
    "METHOD_ORDER = [\"dC_aJ\",\"dC_aM\",\"dE_aJ\",\"dE_aM\"]\n",
    "\n",
    "# build platform config map\n",
    "plat_cfg = {str(r[\"setup\"]).strip(): (int(r[\"win\"]), int(r[\"kfold\"])) for _, r in pb.iterrows()}\n",
    "\n",
    "ANOMALIES_BY_SETUP = {\"DDR4\": [\"DROOP\",\"RH\"], \"DDR5\": [\"DROOP\",\"SPECTRE\"]}\n",
    "\n",
    "def summarize_best_pct_for_row(df_case: pd.DataFrame) -> dict | None:\n",
    "    \"\"\"\n",
    "    df_case: rows for fixed (setup, anomaly, win, kfold), across methods and pct\n",
    "    Strategy:\n",
    "      - pick BEST method+pct: AUCPR_median desc, ROC desc, smaller pct\n",
    "      - report median±IQR\n",
    "    \"\"\"\n",
    "    if df_case.empty:\n",
    "        return None\n",
    "    df_case = df_case[df_case[\"method\"].isin(METHOD_ORDER)].copy()\n",
    "    if df_case.empty:\n",
    "        return None\n",
    "    df_case[\"mrank\"] = df_case[\"method\"].map({m:i for i,m in enumerate(METHOD_ORDER)}).fillna(999).astype(int)\n",
    "\n",
    "    df_case = df_case.sort_values(\n",
    "        [\"auc_pr_median_filled\",\"roc_auc_median_filled\",\"pct\",\"mrank\"],\n",
    "        ascending=[False, False, True, True]\n",
    "    )\n",
    "    best = df_case.iloc[0]\n",
    "    return dict(\n",
    "        Method=str(best[\"method\"]),\n",
    "        PCT=f\"{int(best['pct'])} \\\\%\",\n",
    "        Best_AUC_PR=f\"{best['auc_pr_median']:.2f} $\\\\pm$ {best['auc_pr_iqr']:.2f}\",\n",
    "        Best_ROC_AUC=f\"{best['roc_auc_median']:.2f} $\\\\pm$ {best['roc_auc_iqr']:.2f}\",\n",
    "        auc_pr_median=float(best[\"auc_pr_median\"]),\n",
    "        roc_auc_median=float(best[\"roc_auc_median\"]),\n",
    "    )\n",
    "\n",
    "rows = []\n",
    "for setup in [\"DDR4\",\"DDR5\"]:\n",
    "    if setup not in plat_cfg:\n",
    "        continue\n",
    "    win, kf = plat_cfg[setup]\n",
    "    for anomaly in ANOMALIES_BY_SETUP.get(setup, []):\n",
    "        sub = dse[\n",
    "            (dse[\"setup\"].astype(str).str.upper() == setup.upper()) &\n",
    "            (dse[\"anomaly\"].astype(str).str.upper() == anomaly.upper()) &\n",
    "            (pd.to_numeric(dse[\"win\"], errors=\"coerce\") == win) &\n",
    "            (pd.to_numeric(dse[\"kfold\"], errors=\"coerce\") == kf)\n",
    "        ].copy()\n",
    "\n",
    "        s = summarize_best_pct_for_row(sub)\n",
    "        if s is None:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"Platform\": setup,\n",
    "            \"Anomaly\": anomaly,\n",
    "            \"WIN\": int(win),\n",
    "            \"KF\": int(kf),\n",
    "            \"Method\": s[\"Method\"],\n",
    "            \"PCT\": s[\"PCT\"],\n",
    "            \"Best_AUC_PR\": s[\"Best_AUC_PR\"],\n",
    "            \"Best_ROC_AUC\": s[\"Best_ROC_AUC\"],\n",
    "            \"_auc_m\": s[\"auc_pr_median\"],\n",
    "            \"_roc_m\": s[\"roc_auc_median\"],\n",
    "        })\n",
    "\n",
    "df_out = pd.DataFrame(rows)\n",
    "\n",
    "if df_out.empty:\n",
    "    raise RuntimeError(\"No rows found for per-platform selection. Check per_run_metrics_all_PIPELINE.csv and platform WIN/KF.\")\n",
    "\n",
    "# TOTAL line (mean ± std across rows, using medians)\n",
    "auc_m = df_out[\"_auc_m\"].astype(float).tolist()\n",
    "roc_m = df_out[\"_roc_m\"].astype(float).tolist()\n",
    "\n",
    "df_out.loc[len(df_out)] = {\n",
    "    \"Platform\":\"TOTAL\",\"Anomaly\":\"—\",\"WIN\":\"—\",\"KF\":\"—\",\"Method\":\"—\",\"PCT\":\"—\",\n",
    "    \"Best_AUC_PR\":f\"{np.mean(auc_m):.2f} $\\\\pm$ {np.std(auc_m):.2f}\",\n",
    "    \"Best_ROC_AUC\":f\"{np.mean(roc_m):.2f} $\\\\pm$ {np.std(roc_m):.2f}\",\n",
    "    \"_auc_m\": np.nan,\n",
    "    \"_roc_m\": np.nan,\n",
    "}\n",
    "\n",
    "# Export to LaTeX\n",
    "latex = df_out[[\"Platform\",\"Anomaly\",\"WIN\",\"KF\",\"Method\",\"PCT\",\"Best_AUC_PR\",\"Best_ROC_AUC\"]].to_latex(\n",
    "    index=False, escape=False\n",
    ")\n",
    "\n",
    "out_tex = TABDIR / \"explainability_hybrid_octane_summary_PLATFORM.tex\"\n",
    "out_tex.write_text(latex)\n",
    "print(\"Wrote\", out_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe6abdad-c795-4e93-bd6e-443ef478b97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/tables/rank_meanvar_rocauc_PLATFORM.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2z/b1lzf17n4z90pz80khfc42fw0000gp/T/ipykernel_14285/3044125755.py:108: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ranked = best.groupby(\"dataset_id\", group_keys=False).apply(_rank_within)\n"
     ]
    }
   ],
   "source": [
    "# === PER-PLATFORM rank mean/var LaTeX export (ROC-AUC) ===\n",
    "# This is the PER-PLATFORM rewrite of your rank-summary logic.\n",
    "#\n",
    "# Instead of ranking \"models\" across many datasets/workloads, we rank METHODS (dC_aJ,dC_aM,dE_aJ,dE_aM)\n",
    "# for each PLATFORM×ANOMALY dataset at the platform-chosen (WIN,KF), using the BEST pct per method.\n",
    "#\n",
    "# Inputs:\n",
    "#   - Results/per_run_metrics_all_PIPELINE.csv\n",
    "#   - Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
    "#\n",
    "# Outputs:\n",
    "#   - tables/rank_meanvar_rocauc_PLATFORM.tex\n",
    "#\n",
    "# Ranking rule per dataset_id (setup|anomaly|win|kfold):\n",
    "#   1) For each METHOD, pick best pct by ROC-AUC median (desc), tie smaller pct.\n",
    "#   2) Rank METHODS by roc_auc_median (desc) within that dataset_id (rank 1 = best).\n",
    "#   3) Aggregate mean/var rank per anomaly across platforms, plus Overall.\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT   = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES    = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "if RES.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\"):\n",
    "    RES = RES.parent\n",
    "\n",
    "TABDIR = Path(globals().get(\"TABDIR\", ROOT / \"tables\"))\n",
    "TABDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PER_RUN   = RES / \"per_run_metrics_all_PIPELINE.csv\"\n",
    "PLAT_BEST = RES / \"BEST_in_DesignSpace_Post_per_platform.csv\"\n",
    "\n",
    "assert PER_RUN.exists(), f\"Missing per-run CSV: {PER_RUN}\"\n",
    "assert PLAT_BEST.exists(), f\"Missing per-platform winners CSV: {PLAT_BEST}\"\n",
    "\n",
    "df = pd.read_csv(PER_RUN).copy()\n",
    "pb = pd.read_csv(PLAT_BEST).copy()\n",
    "\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "pb.columns = [c.lower() for c in pb.columns]\n",
    "\n",
    "need = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"method\",\"run_id\",\"roc_auc\"}\n",
    "miss = need - set(df.columns)\n",
    "if miss:\n",
    "    raise KeyError(f\"{PER_RUN} missing columns: {sorted(miss)}. Have: {list(df.columns)}\")\n",
    "\n",
    "METHODS = [\"dC_aJ\",\"dC_aM\",\"dE_aJ\",\"dE_aM\"]\n",
    "\n",
    "# numeric & clean\n",
    "for c in [\"win\",\"kfold\",\"pct\",\"roc_auc\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"method\",\"roc_auc\"]).copy()\n",
    "df = df[df[\"method\"].isin(METHODS)].copy()\n",
    "df[\"roc_auc\"] = df[\"roc_auc\"].clip(0.0, 1.0)\n",
    "\n",
    "# platform (WIN,KF) map\n",
    "plat_cfg = {str(r[\"setup\"]).strip(): (int(r[\"win\"]), int(r[\"kfold\"])) for _, r in pb.iterrows()}\n",
    "\n",
    "# restrict to platform-chosen (WIN,KF) only\n",
    "keep_parts = []\n",
    "for setup, (win, kf) in plat_cfg.items():\n",
    "    sub = df[\n",
    "        (df[\"setup\"].astype(str).str.upper() == setup.upper()) &\n",
    "        (pd.to_numeric(df[\"win\"], errors=\"coerce\") == win) &\n",
    "        (pd.to_numeric(df[\"kfold\"], errors=\"coerce\") == kf)\n",
    "    ].copy()\n",
    "    if not sub.empty:\n",
    "        keep_parts.append(sub)\n",
    "dfp = pd.concat(keep_parts, ignore_index=True) if keep_parts else pd.DataFrame()\n",
    "if dfp.empty:\n",
    "    raise RuntimeError(\"No rows after filtering to per-platform (WIN,KF). Check PLAT_BEST and PER_RUN.\")\n",
    "\n",
    "# summarize ROC-AUC median per pct\n",
    "def q1(x): return float(np.nanpercentile(x, 25))\n",
    "def q3(x): return float(np.nanpercentile(x, 75))\n",
    "\n",
    "keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"pct\"]\n",
    "dse = (dfp.groupby(keys, as_index=False)\n",
    "         .agg(\n",
    "             roc_auc_median=(\"roc_auc\",\"median\"),\n",
    "             n_runs=(\"run_id\",\"nunique\"),\n",
    "         ))\n",
    "dse[\"roc_auc_median\"] = pd.to_numeric(dse[\"roc_auc_median\"], errors=\"coerce\")\n",
    "dse = dse.dropna(subset=[\"roc_auc_median\"]).copy()\n",
    "\n",
    "# dataset_id = platform-level dataset (no workload dimension)\n",
    "dse[\"dataset_id\"] = (\n",
    "    dse[\"setup\"].astype(str) + \"|\" +\n",
    "    dse[\"anomaly\"].astype(str) + \"|\" +\n",
    "    dse[\"win\"].astype(str) + \"|\" +\n",
    "    dse[\"kfold\"].astype(str)\n",
    ")\n",
    "\n",
    "# For each dataset_id × method: pick best pct by ROC median (desc), tie smaller pct\n",
    "best = (dse.sort_values([\"dataset_id\",\"method\",\"roc_auc_median\",\"pct\"],\n",
    "                        ascending=[True, True, False, True])\n",
    "          .groupby([\"dataset_id\",\"method\"], as_index=False)\n",
    "          .head(1))\n",
    "\n",
    "# Rank METHODS within each dataset_id: higher ROC -> lower (better) rank number\n",
    "def _rank_within(df_):\n",
    "    df_ = df_.copy()\n",
    "    df_[\"rank\"] = df_[\"roc_auc_median\"].rank(ascending=False, method=\"average\")\n",
    "    return df_\n",
    "\n",
    "ranked = best.groupby(\"dataset_id\", group_keys=False).apply(_rank_within)\n",
    "\n",
    "# Aggregate mean/var rank by anomaly × method\n",
    "agg = (ranked.groupby([\"anomaly\",\"method\"], as_index=False)\n",
    "             .agg(mean_rank=(\"rank\",\"mean\"),\n",
    "                  var_rank=(\"rank\",\"var\"),\n",
    "                  n=(\"dataset_id\",\"nunique\"))\n",
    "             .sort_values([\"anomaly\",\"mean_rank\",\"method\"]))\n",
    "\n",
    "agg[\"Mean (Var)\"] = agg.apply(\n",
    "    lambda r: f\"{r['mean_rank']:.2f} ({0.0 if pd.isna(r['var_rank']) else r['var_rank']:.2f})\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Overall across anomalies\n",
    "overall = (ranked.groupby(\"method\", as_index=False)\n",
    "                 .agg(mean_rank=(\"rank\",\"mean\"),\n",
    "                      var_rank=(\"rank\",\"var\"),\n",
    "                      n=(\"dataset_id\",\"nunique\")))\n",
    "overall[\"anomaly\"] = \"Overall\"\n",
    "overall[\"Mean (Var)\"] = overall.apply(\n",
    "    lambda r: f\"{r['mean_rank']:.2f} ({0.0 if pd.isna(r['var_rank']) else r['var_rank']:.2f})\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "rank_table = pd.concat([agg, overall], ignore_index=True)\n",
    "\n",
    "# Display + LaTeX\n",
    "disp = (rank_table[[\"anomaly\",\"method\",\"n\",\"Mean (Var)\"]]\n",
    "        .rename(columns={\n",
    "            \"anomaly\":\"Anomaly\",\n",
    "            \"method\":\"Method\",\n",
    "            \"n\":\"Datasets\",\n",
    "            \"Mean (Var)\":\"Mean (Var) Rank (ROC-AUC) ↓\"\n",
    "        }))\n",
    "\n",
    "latex = disp.to_latex(index=False, escape=False)\n",
    "out_tex = TABDIR / \"rank_meanvar_rocauc_PLATFORM.tex\"\n",
    "out_tex.write_text(latex)\n",
    "print(\"Wrote\", out_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "166e9258-7c51-4ae7-a0f9-122a0eed1d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Platform cases: [('DDR4', 512, 3), ('DDR5', 1024, 5)]\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_Comparisons_PLATFORM/hybrid_vs_cpmi_jaccard_topk_PLATFORM.csv\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_Comparisons_PLATFORM/hybrid_vs_cpmi_spearman_PLATFORM.csv\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_Comparisons_PLATFORM/hybrid_vs_cpmi_jaccard_PLATFORM.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setup</th>\n",
       "      <th>win</th>\n",
       "      <th>kfold</th>\n",
       "      <th>subspace</th>\n",
       "      <th>pct</th>\n",
       "      <th>jaccard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>compute</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>compute</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>compute</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>compute</td>\n",
       "      <td>40</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>compute</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  setup  win  kfold subspace  pct  jaccard\n",
       "0  DDR4  512      3  compute   10      1.0\n",
       "1  DDR4  512      3  compute   20      1.0\n",
       "2  DDR4  512      3  compute   30      1.0\n",
       "3  DDR4  512      3  compute   40      1.0\n",
       "4  DDR4  512      3  compute   50      1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setup</th>\n",
       "      <th>win</th>\n",
       "      <th>kfold</th>\n",
       "      <th>subspace</th>\n",
       "      <th>spearman</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>compute</td>\n",
       "      <td>0.993706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>memory</td>\n",
       "      <td>0.999343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDR4</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>sensors</td>\n",
       "      <td>0.983459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>compute</td>\n",
       "      <td>0.993905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDR5</td>\n",
       "      <td>1024</td>\n",
       "      <td>5</td>\n",
       "      <td>memory</td>\n",
       "      <td>0.994734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  setup   win  kfold subspace  spearman\n",
       "0  DDR4   512      3  compute  0.993706\n",
       "1  DDR4   512      3   memory  0.999343\n",
       "2  DDR4   512      3  sensors  0.983459\n",
       "3  DDR5  1024      5  compute  0.993905\n",
       "4  DDR5  1024      5   memory  0.994734"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Explainability comparison (PER PLATFORM): HYBRID vs CP-MI (Jaccard + Spearman) ==========\n",
    "# PER-PLATFORM revision:\n",
    "#   - Only evaluates the platform-chosen (WIN,KF) for DDR4 and DDR5\n",
    "#     (loaded from Results/BEST_in_DesignSpace_Post_per_platform.csv).\n",
    "#   - Computes:\n",
    "#       1) Jaccard@Top-% (10..100) per platform×subspace\n",
    "#       2) Spearman rho per platform×subspace (full rankings over union; missing -> worst+1)\n",
    "#   - Saves CSVs + a simple plot (optional).\n",
    "#\n",
    "# Inputs:\n",
    "#   - FeatureRankOUT_HYBRID/<setup>_<win>_<kfold>_0_<sub>.csv\n",
    "#   - FeatureRankOUT/<setup>_<win>_<kfold>_0_<sub>.csv\n",
    "#   - Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
    "#\n",
    "# Outputs:\n",
    "#   - Results/Explainability_Comparisons_PLATFORM/hybrid_vs_cpmi_jaccard_topk_PLATFORM.csv\n",
    "#   - Results/Explainability_Comparisons_PLATFORM/hybrid_vs_cpmi_spearman_PLATFORM.csv\n",
    "#   - Results/Explainability_Comparisons_PLATFORM/hybrid_vs_cpmi_jaccard_PLATFORM.png\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, re\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Paths (EDIT root if needed) ----\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\")).expanduser().resolve()\n",
    "RES  = ROOT / \"Results\"\n",
    "\n",
    "R_HYB = ROOT / \"FeatureRankOUT_HYBRID\"   # <setup>_<win>_<kfold>_0_<sub>.csv\n",
    "R_CPM = ROOT / \"FeatureRankOUT\"          # <setup>_<win>_<kfold>_0_<sub>.csv\n",
    "PLAT  = RES / \"BEST_in_DesignSpace_Post_per_platform.csv\"\n",
    "\n",
    "OUT   = RES / \"Explainability_Comparisons_PLATFORM\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SUBSPACES = (\"compute\",\"memory\",\"sensors\")\n",
    "PCTS = list(range(10, 101, 10))  # 10%, 20%, ..., 100%\n",
    "\n",
    "if not PLAT.exists():\n",
    "    raise FileNotFoundError(f\"Missing per-platform winners CSV: {PLAT}\")\n",
    "if not R_HYB.exists():\n",
    "    raise FileNotFoundError(f\"Missing HYBRID rank dir: {R_HYB}\")\n",
    "if not R_CPM.exists():\n",
    "    raise FileNotFoundError(f\"Missing CP-MI rank dir: {R_CPM}\")\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\",\"_\",str(s)).lower().replace(\"%\",\"pct\")\n",
    "    s = re.sub(r\"[^a-z0-9_]+\",\"_\", s)\n",
    "    return re.sub(r\"_+\",\"_\", s).strip(\"_\")\n",
    "\n",
    "def _read_rank_csv(p: Path, prefer_col=None):\n",
    "    \"\"\"\n",
    "    Returns list of normalized feature names (ranking order top->bottom).\n",
    "    prefer_col is used for HYBRID files that may have a 'feature_display' column.\n",
    "    \"\"\"\n",
    "    if (not p) or (not p.exists()):\n",
    "        return []\n",
    "    df = pd.read_csv(p)\n",
    "    col = prefer_col if (prefer_col and prefer_col in df.columns) else df.columns[0]\n",
    "    feats = df[col].astype(str).tolist()\n",
    "    return [_norm(x) for x in feats]\n",
    "\n",
    "def _path_hybrid(setup, win, kfold, sub):\n",
    "    return R_HYB / f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "\n",
    "def _path_cpmi(setup, win, kfold, sub):\n",
    "    return R_CPM / f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "\n",
    "# ---------------- Platform cases (ONLY) ----------------\n",
    "pb = pd.read_csv(PLAT).copy()\n",
    "pb.columns = [c.lower() for c in pb.columns]\n",
    "need = {\"setup\",\"win\",\"kfold\"}\n",
    "missing = need - set(pb.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"{PLAT} missing required columns: {sorted(missing)}. Have: {list(pb.columns)}\")\n",
    "\n",
    "platform_cases = []\n",
    "for _, r in pb.iterrows():\n",
    "    setup = str(r[\"setup\"]).strip()\n",
    "    win   = int(pd.to_numeric(r[\"win\"], errors=\"coerce\"))\n",
    "    kf    = int(pd.to_numeric(r[\"kfold\"], errors=\"coerce\"))\n",
    "    platform_cases.append((setup, win, kf))\n",
    "\n",
    "# Defensive: keep only DDR4/DDR5 if present\n",
    "platform_cases = [c for c in platform_cases if str(c[0]).upper() in (\"DDR4\",\"DDR5\")]\n",
    "platform_cases = sorted(platform_cases, key=lambda x: str(x[0]))\n",
    "\n",
    "print(\"[OK] Platform cases:\", platform_cases)\n",
    "\n",
    "# --------------- Metrics ---------------\n",
    "def jaccard_topk(A_list, B_list, pct):\n",
    "    if not A_list or not B_list:\n",
    "        return np.nan\n",
    "    kA = max(1, int(np.ceil(len(A_list) * (pct/100.0))))\n",
    "    kB = max(1, int(np.ceil(len(B_list) * (pct/100.0))))\n",
    "    A = set(A_list[:kA]); B = set(B_list[:kB])\n",
    "    denom = len(A | B)\n",
    "    return (len(A & B) / denom) if denom else np.nan\n",
    "\n",
    "def spearman_between_lists(A_list, B_list):\n",
    "    \"\"\"\n",
    "    Spearman rank corr between two full rankings defined over the union of features.\n",
    "    Missing items get a default 'worst+1' rank.\n",
    "    \"\"\"\n",
    "    if not A_list or not B_list:\n",
    "        return np.nan\n",
    "    union = list(dict.fromkeys(A_list + B_list))\n",
    "    rA_index = {f:i for i,f in enumerate(A_list)}\n",
    "    rB_index = {f:i for i,f in enumerate(B_list)}\n",
    "    worstA = len(A_list) + 1\n",
    "    worstB = len(B_list) + 1\n",
    "    a = np.array([rA_index.get(f, worstA) for f in union], dtype=float)\n",
    "    b = np.array([rB_index.get(f, worstB) for f in union], dtype=float)\n",
    "    if np.all(a == a[0]) or np.all(b == b[0]):\n",
    "        return np.nan\n",
    "    # Pearson on positions (equivalent monotonic rank corr here)\n",
    "    a_mean, b_mean = a.mean(), b.mean()\n",
    "    num = np.sum((a-a_mean)*(b-b_mean))\n",
    "    den = np.sqrt(np.sum((a-a_mean)**2) * np.sum((b-b_mean)**2))\n",
    "    return float(num/den) if den > 0 else np.nan\n",
    "\n",
    "def compute_overlap_and_corr_platform(cases):\n",
    "    rows_j = []\n",
    "    rows_s = []\n",
    "    for setup, win, kf in cases:\n",
    "        for sub in SUBSPACES:\n",
    "            pH = _path_hybrid(setup, win, kf, sub)\n",
    "            pC = _path_cpmi(setup, win, kf, sub)\n",
    "            H = _read_rank_csv(pH, prefer_col=\"feature_display\")\n",
    "            C = _read_rank_csv(pC)\n",
    "            if (not H) or (not C):\n",
    "                continue\n",
    "\n",
    "            # Jaccard across PCTs\n",
    "            for pct in PCTS:\n",
    "                rows_j.append({\n",
    "                    \"setup\": setup, \"win\": win, \"kfold\": kf, \"subspace\": sub, \"pct\": pct,\n",
    "                    \"jaccard\": jaccard_topk(H, C, pct)\n",
    "                })\n",
    "\n",
    "            # Spearman (single value per platform×subspace)\n",
    "            rows_s.append({\n",
    "                \"setup\": setup, \"win\": win, \"kfold\": kf, \"subspace\": sub,\n",
    "                \"spearman\": spearman_between_lists(H, C)\n",
    "            })\n",
    "    return pd.DataFrame(rows_j), pd.DataFrame(rows_s)\n",
    "\n",
    "JACCARD, SPEARMAN = compute_overlap_and_corr_platform(platform_cases)\n",
    "\n",
    "j_csv = OUT / \"hybrid_vs_cpmi_jaccard_topk_PLATFORM.csv\"\n",
    "s_csv = OUT / \"hybrid_vs_cpmi_spearman_PLATFORM.csv\"\n",
    "JACCARD.to_csv(j_csv, index=False)\n",
    "SPEARMAN.to_csv(s_csv, index=False)\n",
    "print(f\"[WROTE] {j_csv}\\n[WROTE] {s_csv}\")\n",
    "\n",
    "# Optional: plot mean Jaccard across subspaces per platform\n",
    "if not JACCARD.empty:\n",
    "    plt.figure(figsize=(6.6,4.3), dpi=160)\n",
    "    for (setup, win, kf), g in JACCARD.groupby([\"setup\",\"win\",\"kfold\"]):\n",
    "        gg = g.groupby(\"pct\", as_index=False)[\"jaccard\"].mean()\n",
    "        label = f\"{setup} (W{win},K{kf})\"\n",
    "        plt.plot(gg[\"pct\"], gg[\"jaccard\"], lw=2.4, marker=\"o\", label=label)\n",
    "    plt.xlabel(\"Top-% threshold\")\n",
    "    plt.ylabel(\"Jaccard(HYBRID, CP-MI) • avg over subspaces\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.legend(loc=\"lower right\", fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    out_png = OUT / \"hybrid_vs_cpmi_jaccard_PLATFORM.png\"\n",
    "    plt.savefig(out_png, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[WROTE] {out_png}\")\n",
    "\n",
    "# Quick sanity view (optional)\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(JACCARD.head())\n",
    "    display(SPEARMAN.head())\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f409d18-1707-43cb-82a1-9a06296196a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_Comparisons_PLATFORM/jaccard_curve_PLATFORM_DDR4_WIN512_KF3.png\n",
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_Comparisons_PLATFORM/jaccard_curve_PLATFORM_DDR5_WIN1024_KF5.png\n"
     ]
    }
   ],
   "source": [
    "# === PER-PLATFORM Plot: Jaccard@Top-% curves (avg over subspaces) ===\n",
    "# Uses JACCARD DataFrame produced by the per-platform HYBRID-vs-CPMI script:\n",
    "#   columns: setup, win, kfold, subspace, pct, jaccard\n",
    "#\n",
    "# Outputs one PNG per platform:\n",
    "#   OUT/jaccard_curve_PLATFORM_<setup>_WIN<w>_KF<k>.png\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_jaccard_curves_platform(J, out_dir, setups=(\"DDR4\",\"DDR5\")):\n",
    "    if J is None or J.empty:\n",
    "        print(\"[SKIP] Jaccard plot (no data).\")\n",
    "        return\n",
    "\n",
    "    # Defensive typing\n",
    "    JJ = J.copy()\n",
    "    JJ[\"pct\"] = pd.to_numeric(JJ[\"pct\"], errors=\"coerce\")\n",
    "    JJ[\"jaccard\"] = pd.to_numeric(JJ[\"jaccard\"], errors=\"coerce\")\n",
    "\n",
    "    for setup in setups:\n",
    "        g0 = JJ[JJ[\"setup\"].astype(str).str.upper() == setup.upper()].copy()\n",
    "        if g0.empty:\n",
    "            continue\n",
    "\n",
    "        # If multiple (win,kfold) exist, make one plot per (win,kfold) anyway,\n",
    "        # but naming still says PLATFORM.\n",
    "        for (win, kf), g in g0.groupby([\"win\",\"kfold\"]):\n",
    "            gg = g.groupby(\"pct\", as_index=False)[\"jaccard\"].mean()\n",
    "            gg = gg.sort_values(\"pct\")\n",
    "\n",
    "            plt.figure(figsize=(6.0, 4.0), dpi=160)\n",
    "            plt.plot(gg[\"pct\"], gg[\"jaccard\"], lw=2.4, marker=\"o\")\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xlabel(\"Top-% threshold\")\n",
    "            plt.ylabel(\"Jaccard overlap (Hybrid vs CP-MI)\")\n",
    "            plt.title(f\"{setup} (WIN={int(win)}, KF={int(kf)}) — Avg over subspaces\")\n",
    "            plt.grid(True, alpha=0.25)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            out = Path(out_dir) / f\"jaccard_curve_PLATFORM_{setup}_WIN{int(win)}_KF{int(kf)}.png\"\n",
    "            plt.savefig(out, dpi=300)\n",
    "            plt.close()\n",
    "            print(\"Wrote\", out)\n",
    "\n",
    "# Run (expects OUT and JACCARD already exist in your notebook)\n",
    "plot_jaccard_curves_platform(JACCARD, OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "905e0478-cbb8-421d-a1a3-731511b6a620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Platform cases: [('DDR4', 512, 3), ('DDR5', 1024, 5)]\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_Comparisons_PLATFORM/hybrid_vs_cpmi_jaccard_topk_PLATFORM.csv\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_Comparisons_PLATFORM/hybrid_vs_cpmi_spearman_PLATFORM.csv\n",
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_Comparisons_PLATFORM/jaccard_curve_PLATFORM_DDR4_WIN512_KF3.png\n",
      "Wrote /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_Comparisons_PLATFORM/jaccard_curve_PLATFORM_DDR5_WIN1024_KF5.png\n",
      "Done.\n",
      "Explainability OUT: /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_Comparisons_PLATFORM\n",
      "CSV: /Users/hsiaopingni/octaneX_v7_4functions/Results/paper_exports/per_workload_system_auc.csv\n",
      "Log: /Users/hsiaopingni/octaneX_v7_4functions/Results/paper_exports/build_per_workload_system_auc.log\n"
     ]
    }
   ],
   "source": [
    "# === PER-PLATFORM: (1) HYBRID vs CP-MI explainability (Jaccard + Spearman) + (2) Jaccard plots\n",
    "#                  + (3) Build per_workload_system_auc.csv (52 rows) ============\n",
    "#\n",
    "# This is a SINGLE, combined, per-platform script that:\n",
    "#   A) Uses platform-chosen (WIN,KF) from Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
    "#   B) Computes HYBRID vs CP-MI similarity (Jaccard@Top-% and Spearman) at those (WIN,KF)\n",
    "#   C) Saves CSVs + one Jaccard curve PNG per platform\n",
    "#   D) Builds Results/paper_exports/per_workload_system_auc.csv (52 rows)\n",
    "#      using ONE (WIN,KF,top%) per platform selected by probe workloads mean AUCROC.\n",
    "#\n",
    "# IMPORTANT differences vs your old mixed scripts:\n",
    "#   - All “platform-level” configs come from:\n",
    "#       Results/BEST_in_DesignSpace_Post_per_platform.csv  (for (WIN,KF))\n",
    "#   - SHAP for HYBRID feature blending is loaded from PER-PLATFORM SHAP outputs:\n",
    "#       Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_<setup>_<scenario>_WIN<w>_KF<k>_PCT<p>_M<method>.csv\n",
    "#     via Results/BEST_in_DesignSpace_Post_per_platform_details.csv (best pct+method per scenario).\n",
    "#\n",
    "# Outputs:\n",
    "#   1) Results/Explainability_Comparisons_PLATFORM/\n",
    "#        hybrid_vs_cpmi_jaccard_topk_PLATFORM.csv\n",
    "#        hybrid_vs_cpmi_spearman_PLATFORM.csv\n",
    "#        jaccard_curve_PLATFORM_<setup>_WIN<w>_KF<k>.png\n",
    "#   2) Results/paper_exports/\n",
    "#        per_workload_system_auc.csv\n",
    "#        build_per_workload_system_auc.log\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "import os, re, glob, math, gc, warnings, logging\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ.setdefault(\"PYTHONWARNINGS\", \"ignore\")\n",
    "\n",
    "# ---------------- You can edit these paths if needed -------------------\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\")).expanduser().resolve()\n",
    "RES_DIR_RAW = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "DATA_DIR = Path(globals().get(\"DATA_DIR\", \"/Users/hsiaopingni/Desktop/SLM_RAS-main/HW_TELEMETRY_DATA_COLLECTION/TELEMETRY_DATA\"))\n",
    "\n",
    "# Normalize RES_DIR to \".../Results\"\n",
    "RES_DIR = RES_DIR_RAW.parent if RES_DIR_RAW.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\") else RES_DIR_RAW\n",
    "\n",
    "# Rank dirs for CP-MI files (searched in order)\n",
    "RANK_DIRS = list(globals().get(\"RANK_DIRS\", [\n",
    "    ROOT / \"FeatureRankOUT\",\n",
    "    Path(\"/Volumes/Untitled\") / \"FeatureRankOUT\",\n",
    "    Path(\"/Volumes/Untitled\") / \"octaneX\" / \"FeatureRankOUT\",\n",
    "    Path.home() / \"Desktop\" / \"octaneX\" / \"FeatureRankOUT\",\n",
    "]))\n",
    "\n",
    "# Per-platform configs\n",
    "PLAT_BEST = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform.csv\"\n",
    "PLAT_DETAILS = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "\n",
    "# SHAP dir (per platform)\n",
    "SHAP_DIR = RES_DIR / \"Explainability_SHAP_BestPlatforms\"\n",
    "\n",
    "# Output dirs\n",
    "OUT_COMP = RES_DIR / \"Explainability_Comparisons_PLATFORM\"\n",
    "OUT_COMP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PAPER_OUT = RES_DIR / \"paper_exports\"\n",
    "PAPER_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _ensure_writable_dir(p: Path, hint_name: str):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    testfile = p / \".write_test.tmp\"\n",
    "    try:\n",
    "        with open(testfile, \"w\") as f:\n",
    "            f.write(\"ok\")\n",
    "        try:\n",
    "            testfile.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "    except PermissionError as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Cannot write to {p} for {hint_name}.\\n\"\n",
    "            \"On macOS, give your terminal/Jupyter app Full Disk Access:\\n\"\n",
    "            \"  System Settings → Privacy & Security → Full Disk Access → enable Terminal/VS Code/Jupyter\\n\"\n",
    "            \"Then restart it and rerun this cell.\"\n",
    "        ) from e\n",
    "\n",
    "_ensure_writable_dir(OUT_COMP, \"explainability comparisons\")\n",
    "_ensure_writable_dir(PAPER_OUT, \"paper exports\")\n",
    "\n",
    "# Log file for the AUC CSV build\n",
    "LOG_FILE = PAPER_OUT / \"build_per_workload_system_auc.log\"\n",
    "logging.basicConfig(\n",
    "    filename=str(LOG_FILE),\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    ")\n",
    "log = logging.getLogger(\"papercsv\")\n",
    "log.info(\"==== START combined PER-PLATFORM run ====\")\n",
    "log.info(f\"ROOT={ROOT} RES_DIR={RES_DIR} DATA_DIR={DATA_DIR}\")\n",
    "log.info(f\"RANK_DIRS={RANK_DIRS}\")\n",
    "\n",
    "# ---------------- Paper configuration ----------------------------------------\n",
    "SUBSPACES = (\"compute\",\"memory\",\"sensors\")\n",
    "PLATFORMS = (\"DDR4\",\"DDR5\")\n",
    "SCENARIOS = {\"DDR4\": (\"DROOP\",\"RH\"), \"DDR5\": (\"DROOP\",\"SPECTRE\")}\n",
    "WORKLOADS = [\"dft\",\"dj\",\"dp\",\"gl\",\"gs\",\"ha\",\"ja\",\"mm\",\"ni\",\"oe\",\"pi\",\"sh\",\"tr\"]\n",
    "\n",
    "# Hybrid blend for selection inside per_workload_system_auc.csv\n",
    "ALPHA    = float(globals().get(\"ALPHA\", 0.40))\n",
    "SEED     = int(globals().get(\"SEED\", 42))\n",
    "TOP_PCTS = [10, 25, 50, 75, 100]  # candidates to choose per platform\n",
    "\n",
    "# Selection probe set (keeps selection fast; then final pass writes all 52 rows)\n",
    "PROBE_WORKLOADS = list(globals().get(\"PROBE_WORKLOADS\", [\"dft\",\"gs\",\"mm\"]))\n",
    "\n",
    "# XGBoost runtime knobs\n",
    "XGB_TREES   = int(globals().get(\"XGB_TREES\", 150))\n",
    "XGB_ESR     = int(globals().get(\"XGB_ESR\", 20))\n",
    "XGB_THREADS = max(1, min(8, (os.cpu_count() or 4)))\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", str(XGB_THREADS))\n",
    "\n",
    "# ---------------- Self-contained helpers -------------------------------------\n",
    "def _norm(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\",\"_\",str(s)).lower().replace(\"%\",\"pct\")\n",
    "    s = re.sub(r\"[^a-z0-9_]+\",\"_\", s)\n",
    "    return re.sub(r\"_+\",\"_\", s).strip(\"_\")\n",
    "\n",
    "def telemetry_cols(df: pd.DataFrame):\n",
    "    drop_like = {\"label\",\"setup\",\"anomaly\",\"run_id\",\"timestamp\",\"time\",\"idx\",\"index\"}\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    return [c for c in num_cols if c.lower() not in drop_like]\n",
    "\n",
    "def robust_scale_train(X: np.ndarray, winsor=(2.0, 98.0)):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    q1 = np.nanpercentile(X, winsor[0], axis=0)\n",
    "    q2 = np.nanpercentile(X, winsor[1], axis=0)\n",
    "    Xw = np.clip(X, q1, q2)\n",
    "    mu = np.nanmedian(Xw, axis=0)\n",
    "    mad = np.nanmedian(np.abs(Xw - mu), axis=0)\n",
    "    sd  = mad * 1.4826\n",
    "    iqr = np.nanpercentile(Xw, 75, axis=0) - np.nanpercentile(Xw, 25, axis=0)\n",
    "    sd  = np.where(sd < 1e-9, iqr/1.349, sd)\n",
    "    sd  = np.where(sd < 1e-9, 1.0, sd)\n",
    "    return mu, sd, q1, q2\n",
    "\n",
    "def apply_robust_scale(df: pd.DataFrame, mu, sd, q1, q2) -> pd.DataFrame:\n",
    "    X = df.values.astype(float)\n",
    "    X = np.clip(X, q1, q2)\n",
    "    Z = (X - mu) / sd\n",
    "    return pd.DataFrame(Z, columns=df.columns, index=df.index)\n",
    "\n",
    "def _read_csv_safe(p: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(p)\n",
    "    except Exception as e:\n",
    "        log.warning(f\"read_csv failed for {p}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def build_windowed_raw_means(pairs, win: int, label: str, overlap_ratio=None) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    step = win if overlap_ratio is None else max(1, int(win * (1 - float(overlap_ratio))))\n",
    "    for _, df in pairs:\n",
    "        feats = telemetry_cols(df)\n",
    "        if not feats:\n",
    "            continue\n",
    "        D = df[feats].astype(float)\n",
    "\n",
    "        # If already aggregated: pass-through\n",
    "        if len(D) <= max(2*win, 32) and D.shape[1] >= 16:\n",
    "            out_chunk = D.copy()\n",
    "            out_chunk[\"label\"] = label\n",
    "            rows.append(out_chunk)\n",
    "            continue\n",
    "\n",
    "        n = len(D)\n",
    "        if n < win:\n",
    "            continue\n",
    "        for start in range(0, n - win + 1, step):\n",
    "            sl = D.iloc[start:start+win]\n",
    "            rows.append(pd.Series({**sl.mean(axis=0, skipna=True).to_dict(), \"label\": label}))\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    out = pd.DataFrame(rows).reset_index(drop=True)\n",
    "    feat_cols = telemetry_cols(out)\n",
    "    return out[feat_cols + [\"label\"]]\n",
    "\n",
    "def _percentile_rank(series: pd.Series, higher_is_better=True) -> pd.Series:\n",
    "    x = series.copy()\n",
    "    if not higher_is_better:\n",
    "        x = -x\n",
    "    n = len(x)\n",
    "    if n <= 1:\n",
    "        return pd.Series(np.zeros(n), index=series.index, dtype=float)\n",
    "    r = x.rank(method=\"average\", ascending=False)  # largest→1\n",
    "    return (n - r) / (n - 1)\n",
    "\n",
    "def _top_k_from_pct(n: int, pct: int) -> int:\n",
    "    k = int(math.ceil(n * pct / 100.0))\n",
    "    return max(1, k) if pct > 0 else 0\n",
    "\n",
    "# ---------------- CP-MI loader (cached) --------------------------------------\n",
    "def _cpmi_rank_path(setup: str, win: int, kfold: int, sub: str) -> Optional[Path]:\n",
    "    fname = f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "    for d in RANK_DIRS:\n",
    "        p = Path(d) / fname\n",
    "        if p.exists():\n",
    "            return p\n",
    "    for d in RANK_DIRS:\n",
    "        d = Path(d)\n",
    "        if not d.exists():\n",
    "            continue\n",
    "        hits = list(d.rglob(f\"*{setup}*{sub}*CPMI*.csv\")) + list(d.rglob(f\"*{setup}*{sub}*CP-MI*.csv\"))\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    return None\n",
    "\n",
    "_CPMI_CACHE: Dict[Tuple[str,int,int,str], pd.DataFrame] = {}\n",
    "\n",
    "def _load_cpmi_one(setup: str, win: int, kfold: int, sub: str) -> pd.DataFrame:\n",
    "    key = (setup, int(win), int(kfold), sub)\n",
    "    if key in _CPMI_CACHE:\n",
    "        return _CPMI_CACHE[key]\n",
    "\n",
    "    rp = _cpmi_rank_path(setup, win, kfold, sub)\n",
    "    if not rp:\n",
    "        _CPMI_CACHE[key] = pd.DataFrame()\n",
    "        return _CPMI_CACHE[key]\n",
    "\n",
    "    df = _read_csv_safe(rp)\n",
    "    if df.empty:\n",
    "        _CPMI_CACHE[key] = df\n",
    "        return df\n",
    "\n",
    "    feat_col = next((c for c in df.columns if c.lower() in (\"feature\",\"features\",\"name\",\"signal\",\"column\")), df.columns[0])\n",
    "    score_col= next((c for c in df.columns if c.lower() in (\"cpmi\",\"cp-mi\",\"score\",\"mi\",\"mi_score\",\"rank_score\")),\n",
    "                    (df.select_dtypes(include=[np.number]).columns.tolist() or [df.columns[-1]])[0])\n",
    "\n",
    "    d = (\n",
    "        df[[feat_col, score_col]].rename(columns={feat_col:\"feature\", score_col:\"cpmi_score\"})\n",
    "          .assign(feature=lambda x: x[\"feature\"].astype(str).str.strip())\n",
    "          .dropna(subset=[\"feature\"]).drop_duplicates(subset=[\"feature\"])\n",
    "    )\n",
    "    d[\"cpmi_score\"] = pd.to_numeric(d[\"cpmi_score\"], errors=\"coerce\").fillna(0.0)\n",
    "    d = d.sort_values(\"cpmi_score\", ascending=False).reset_index(drop=True)\n",
    "    d[\"cpmi_rank\"]  = np.arange(1, len(d)+1)\n",
    "    d[\"cpmi_prank\"] = _percentile_rank(d[\"cpmi_score\"], True)\n",
    "    d[\"subspace\"]   = sub\n",
    "\n",
    "    _CPMI_CACHE[key] = d\n",
    "    return d\n",
    "\n",
    "# ---------------- SHAP loader (per platform, exact via details) --------------\n",
    "_SHAP_CACHE: Dict[Tuple[str,str,int,int,int,str], pd.DataFrame] = {}\n",
    "\n",
    "def _find_shap_best_exact(setup: str, scenario: str, win: int, kfold: int, pct: int, method: str) -> Optional[Path]:\n",
    "    p = SHAP_DIR / f\"SHAP_BESTPLAT_full_{setup}_{scenario}_WIN{win}_KF{kfold}_PCT{pct}_M{method}.csv\"\n",
    "    if p.exists():\n",
    "        return p\n",
    "    patt = str(SHAP_DIR / f\"SHAP_BESTPLAT_full_{setup}_{scenario}_WIN{win}_KF{kfold}_PCT*_M{method}.csv\")\n",
    "    hits = sorted(glob.glob(patt))\n",
    "    return Path(hits[0]) if hits else None\n",
    "\n",
    "def _load_shap_best_full_exact(setup: str, scenario: str, win: int, kfold: int, pct: int, method: str) -> pd.DataFrame:\n",
    "    key = (setup, scenario, int(win), int(kfold), int(pct), str(method))\n",
    "    if key in _SHAP_CACHE:\n",
    "        return _SHAP_CACHE[key]\n",
    "\n",
    "    p = _find_shap_best_exact(setup, scenario, win, kfold, pct, method)\n",
    "    if not p:\n",
    "        _SHAP_CACHE[key] = pd.DataFrame()\n",
    "        return _SHAP_CACHE[key]\n",
    "\n",
    "    df = _read_csv_safe(p)\n",
    "    if df.empty:\n",
    "        _SHAP_CACHE[key] = df\n",
    "        return df\n",
    "\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    if \"feature\" not in cols:\n",
    "        df = df.rename(columns={list(df.columns)[0]: \"feature\"})\n",
    "    else:\n",
    "        df = df.rename(columns={cols[\"feature\"]: \"feature\"})\n",
    "    if \"subspace\" in cols:\n",
    "        df = df.rename(columns={cols[\"subspace\"]: \"subspace\"})\n",
    "    if \"shap_mean_abs\" in cols:\n",
    "        df = df.rename(columns={cols[\"shap_mean_abs\"]: \"shap_mean_abs\"})\n",
    "    elif \"importance\" in cols:\n",
    "        df = df.rename(columns={cols[\"importance\"]: \"shap_mean_abs\"})\n",
    "\n",
    "    keep = [c for c in [\"feature\",\"subspace\",\"shap_mean_abs\"] if c in df.columns]\n",
    "    df = df[keep].dropna()\n",
    "    if \"subspace\" not in df.columns:\n",
    "        df[\"subspace\"] = \"compute\"\n",
    "    df[\"subspace\"] = df[\"subspace\"].astype(str)\n",
    "    df[\"shap_mean_abs\"] = pd.to_numeric(df[\"shap_mean_abs\"], errors=\"coerce\").fillna(0.0)\n",
    "    df[\"feature\"] = df[\"feature\"].astype(str).str.strip()\n",
    "    df = df.sort_values(\"shap_mean_abs\", ascending=False).reset_index(drop=True)\n",
    "    df[\"shap_rank\"]  = np.arange(1, len(df)+1)\n",
    "    df[\"shap_prank\"] = _percentile_rank(df[\"shap_mean_abs\"], True)\n",
    "\n",
    "    _SHAP_CACHE[key] = df\n",
    "    return df\n",
    "\n",
    "# ---------------- Hybrid feature list for scenario (CPMI + SHAP blend) --------\n",
    "def _hybrid_rank_table(cpmi_df: pd.DataFrame, shap_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if cpmi_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    left = cpmi_df[[\"feature\",\"cpmi_score\",\"cpmi_rank\",\"cpmi_prank\",\"subspace\"]].copy()\n",
    "    if shap_df is None or shap_df.empty:\n",
    "        m = left.copy()\n",
    "        m[\"shap_prank\"] = 0.0\n",
    "    else:\n",
    "        right = shap_df[[\"feature\",\"shap_mean_abs\",\"shap_rank\",\"shap_prank\"]].copy()\n",
    "        m = left.merge(right, on=\"feature\", how=\"left\")\n",
    "        m[\"shap_prank\"] = pd.to_numeric(m[\"shap_prank\"], errors=\"coerce\").fillna(0.0)\n",
    "    m[\"hybrid_score\"] = (1 - ALPHA) * m[\"cpmi_prank\"] + ALPHA * m[\"shap_prank\"]\n",
    "    m = m.sort_values(\"hybrid_score\", ascending=False).reset_index(drop=True)\n",
    "    m[\"hybrid_rank\"] = np.arange(1, len(m)+1)\n",
    "    return m\n",
    "\n",
    "# ---------------- Data indexing ----------------------------------------------\n",
    "def _index_files(data_dir: Path):\n",
    "    files = {}\n",
    "    for p in Path(data_dir).glob(\"*.csv\"):\n",
    "        m = re.match(r\"(?i)^(DDR4|DDR5)_([A-Za-z]+)_([A-Za-z0-9]+)\\.csv$\", p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        plat, scen, wl = m.group(1).upper(), m.group(2).upper(), m.group(3).lower()\n",
    "        files[(plat, scen, wl)] = p\n",
    "    return files\n",
    "\n",
    "FILES = _index_files(DATA_DIR)\n",
    "if len(FILES) < 52:\n",
    "    log.error(f\"Expected ~52 workload files. Found {len(FILES)} in {DATA_DIR}\")\n",
    "    raise RuntimeError(f\"Missing workload CSVs in {DATA_DIR}. See log: {LOG_FILE}\")\n",
    "log.info(f\"Data files indexed: {len(FILES)}\")\n",
    "\n",
    "# ---------------- XGBoost fit wrapper (API compatible) -----------------------\n",
    "def fit_xgb(clf, X_tr, y_tr, X_va, y_va, esr: int):\n",
    "    try:\n",
    "        clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False, early_stopping_rounds=esr)\n",
    "        return\n",
    "    except TypeError:\n",
    "        pass\n",
    "    try:\n",
    "        cb = [xgb.callback.EarlyStopping(rounds=esr, save_best=True)]\n",
    "        clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False, callbacks=cb)\n",
    "        return\n",
    "    except TypeError:\n",
    "        pass\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "# ---------------- Load per-platform winners (WIN,KF) + details (pct/method) ---\n",
    "if not PLAT_BEST.exists():\n",
    "    raise FileNotFoundError(f\"Missing per-platform winners CSV: {PLAT_BEST}\")\n",
    "if not PLAT_DETAILS.exists():\n",
    "    raise FileNotFoundError(f\"Missing per-platform details CSV: {PLAT_DETAILS}\")\n",
    "\n",
    "pb = pd.read_csv(PLAT_BEST).copy()\n",
    "pb.columns = [c.lower() for c in pb.columns]\n",
    "need_pb = {\"setup\",\"win\",\"kfold\"}\n",
    "miss_pb = need_pb - set(pb.columns)\n",
    "if miss_pb:\n",
    "    raise KeyError(f\"{PLAT_BEST} missing columns: {sorted(miss_pb)}. Have: {list(pb.columns)}\")\n",
    "\n",
    "platform_cases = []\n",
    "for _, r in pb.iterrows():\n",
    "    setup = str(r[\"setup\"]).strip().upper()\n",
    "    win   = int(pd.to_numeric(r[\"win\"], errors=\"coerce\"))\n",
    "    kf    = int(pd.to_numeric(r[\"kfold\"], errors=\"coerce\"))\n",
    "    if setup in PLATFORMS:\n",
    "        platform_cases.append((setup, win, kf))\n",
    "platform_cases = sorted(platform_cases, key=lambda x: x[0])\n",
    "print(\"[OK] Platform cases:\", platform_cases)\n",
    "\n",
    "det = pd.read_csv(PLAT_DETAILS).copy()\n",
    "det.columns = [c.lower() for c in det.columns]\n",
    "if \"best_method\" in det.columns and \"method\" not in det.columns:\n",
    "    det = det.rename(columns={\"best_method\":\"method\"})\n",
    "if \"best_pct_by_median\" not in det.columns and \"pct\" in det.columns:\n",
    "    det = det.rename(columns={\"pct\":\"best_pct_by_median\"})\n",
    "need_det = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"}\n",
    "miss_det = need_det - set(det.columns)\n",
    "if miss_det:\n",
    "    raise KeyError(f\"{PLAT_DETAILS} missing columns: {sorted(miss_det)}. Have: {list(det.columns)}\")\n",
    "\n",
    "# Map (setup, scenario) -> (pct, method) from details\n",
    "detail_map: Dict[Tuple[str,str], Tuple[int,str]] = {}\n",
    "for _, r in det.iterrows():\n",
    "    setup = str(r[\"setup\"]).strip().upper()\n",
    "    scen  = str(r[\"anomaly\"]).strip().upper()\n",
    "    pct   = int(pd.to_numeric(r[\"best_pct_by_median\"], errors=\"coerce\"))\n",
    "    method= str(r[\"method\"]).strip()\n",
    "    detail_map[(setup, scen)] = (pct, method)\n",
    "\n",
    "# ---------------- PART A: HYBRID vs CP-MI (Jaccard + Spearman) ----------------\n",
    "def _read_rank_csv(p: Path, prefer_col=None):\n",
    "    if (not p) or (not p.exists()):\n",
    "        return []\n",
    "    df = pd.read_csv(p)\n",
    "    col = prefer_col if (prefer_col and prefer_col in df.columns) else df.columns[0]\n",
    "    feats = df[col].astype(str).tolist()\n",
    "    return [_norm(x) for x in feats]\n",
    "\n",
    "def _path_hybrid(setup, win, kfold, sub):\n",
    "    return ROOT / \"FeatureRankOUT_HYBRID\" / f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "\n",
    "def _path_cpmi(setup, win, kfold, sub):\n",
    "    return ROOT / \"FeatureRankOUT\" / f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "\n",
    "def jaccard_topk(A_list, B_list, pct):\n",
    "    if not A_list or not B_list:\n",
    "        return np.nan\n",
    "    kA = max(1, int(np.ceil(len(A_list) * (pct/100.0))))\n",
    "    kB = max(1, int(np.ceil(len(B_list) * (pct/100.0))))\n",
    "    A = set(A_list[:kA]); B = set(B_list[:kB])\n",
    "    denom = len(A | B)\n",
    "    return (len(A & B) / denom) if denom else np.nan\n",
    "\n",
    "def spearman_between_lists(A_list, B_list):\n",
    "    if not A_list or not B_list:\n",
    "        return np.nan\n",
    "    union = list(dict.fromkeys(A_list + B_list))\n",
    "    rA_index = {f:i for i,f in enumerate(A_list)}\n",
    "    rB_index = {f:i for i,f in enumerate(B_list)}\n",
    "    worstA = len(A_list) + 1\n",
    "    worstB = len(B_list) + 1\n",
    "    a = np.array([rA_index.get(f, worstA) for f in union], dtype=float)\n",
    "    b = np.array([rB_index.get(f, worstB) for f in union], dtype=float)\n",
    "    if np.all(a == a[0]) or np.all(b == b[0]):\n",
    "        return np.nan\n",
    "    a_mean, b_mean = a.mean(), b.mean()\n",
    "    num = np.sum((a-a_mean)*(b-b_mean))\n",
    "    den = np.sqrt(np.sum((a-a_mean)**2) * np.sum((b-b_mean)**2))\n",
    "    return float(num/den) if den > 0 else np.nan\n",
    "\n",
    "rows_j, rows_s = [], []\n",
    "for setup, win, kf in platform_cases:\n",
    "    for sub in SUBSPACES:\n",
    "        H = _read_rank_csv(_path_hybrid(setup, win, kf, sub), prefer_col=\"feature_display\")\n",
    "        C = _read_rank_csv(_path_cpmi(setup, win, kf, sub))\n",
    "        if (not H) or (not C):\n",
    "            continue\n",
    "        for pct in list(range(10,101,10)):\n",
    "            rows_j.append({\"setup\":setup,\"win\":win,\"kfold\":kf,\"subspace\":sub,\"pct\":pct,\n",
    "                           \"jaccard\": jaccard_topk(H,C,pct)})\n",
    "        rows_s.append({\"setup\":setup,\"win\":win,\"kfold\":kf,\"subspace\":sub,\n",
    "                       \"spearman\": spearman_between_lists(H,C)})\n",
    "\n",
    "JACCARD = pd.DataFrame(rows_j)\n",
    "SPEARMAN = pd.DataFrame(rows_s)\n",
    "\n",
    "j_csv = OUT_COMP / \"hybrid_vs_cpmi_jaccard_topk_PLATFORM.csv\"\n",
    "s_csv = OUT_COMP / \"hybrid_vs_cpmi_spearman_PLATFORM.csv\"\n",
    "JACCARD.to_csv(j_csv, index=False)\n",
    "SPEARMAN.to_csv(s_csv, index=False)\n",
    "print(f\"[WROTE] {j_csv}\\n[WROTE] {s_csv}\")\n",
    "\n",
    "# ---------------- PART B: Plot Jaccard curves per platform -------------------\n",
    "def plot_jaccard_curves_platform(J, out_dir):\n",
    "    if J is None or J.empty:\n",
    "        print(\"[SKIP] Jaccard plot (no data).\")\n",
    "        return\n",
    "    JJ = J.copy()\n",
    "    JJ[\"pct\"] = pd.to_numeric(JJ[\"pct\"], errors=\"coerce\")\n",
    "    JJ[\"jaccard\"] = pd.to_numeric(JJ[\"jaccard\"], errors=\"coerce\")\n",
    "    for (setup, win, kf), g in JJ.groupby([\"setup\",\"win\",\"kfold\"]):\n",
    "        gg = g.groupby(\"pct\", as_index=False)[\"jaccard\"].mean().sort_values(\"pct\")\n",
    "        plt.figure(figsize=(6.0,4.0), dpi=160)\n",
    "        plt.plot(gg[\"pct\"], gg[\"jaccard\"], lw=2.4, marker=\"o\")\n",
    "        plt.ylim(0,1)\n",
    "        plt.xlabel(\"Top-% threshold\")\n",
    "        plt.ylabel(\"Jaccard overlap (Hybrid vs CP-MI)\")\n",
    "        plt.title(f\"{setup} (WIN={int(win)}, KF={int(kf)}) — Avg over subspaces\")\n",
    "        plt.grid(True, alpha=0.25)\n",
    "        plt.tight_layout()\n",
    "        out = Path(out_dir) / f\"jaccard_curve_PLATFORM_{setup}_WIN{int(win)}_KF{int(kf)}.png\"\n",
    "        plt.savefig(out, dpi=300)\n",
    "        plt.close()\n",
    "        print(\"Wrote\", out)\n",
    "\n",
    "plot_jaccard_curves_platform(JACCARD, OUT_COMP)\n",
    "\n",
    "# ---------------- PART C: Build per_workload_system_auc.csv (52 rows) --------\n",
    "def _available_win_kfold(setup: str):\n",
    "    pairs = set()\n",
    "    patt_list = []\n",
    "    for d in RANK_DIRS:\n",
    "        d = Path(d)\n",
    "        patt_list += [\n",
    "            str(d / f\"{setup}_*_0_compute.csv\"),\n",
    "            str(d / f\"{setup}_*_0_memory.csv\"),\n",
    "            str(d / f\"{setup}_*_0_sensors.csv\"),\n",
    "            str(d / f\"{setup}_*_0_*.csv\"),\n",
    "        ]\n",
    "    for patt in patt_list:\n",
    "        for path in glob.glob(patt):\n",
    "            m = re.search(rf\"{setup}_(\\d+)_([0-9]+)_0_\", str(path))\n",
    "            if m:\n",
    "                pairs.add((int(m.group(1)), int(m.group(2))))\n",
    "    return sorted(pairs)\n",
    "\n",
    "def _evaluate_mean_auc(setup: str, win: int, kfold: int, pct: int, workloads_probe=None) -> float:\n",
    "    workloads_probe = workloads_probe or WORKLOADS\n",
    "    aucs = []\n",
    "    count = 0\n",
    "\n",
    "    cpmi_tables = {sub: _load_cpmi_one(setup, win, kfold, sub) for sub in SUBSPACES}\n",
    "    if all(v.empty for v in cpmi_tables.values()):\n",
    "        return float(\"nan\")\n",
    "\n",
    "    for scen in SCENARIOS[setup]:\n",
    "        # use exact pct/method from per-platform details if available; else fallback to first match\n",
    "        pct_s, method_s = detail_map.get((setup, scen.upper()), (None, None))\n",
    "        shap = pd.DataFrame()\n",
    "        if pct_s is not None and method_s is not None:\n",
    "            shap = _load_shap_best_full_exact(setup, scen.upper(), win, kfold, int(pct_s), str(method_s))\n",
    "\n",
    "        sel = []\n",
    "        for sub in SUBSPACES:\n",
    "            base = cpmi_tables[sub]\n",
    "            if base.empty:\n",
    "                continue\n",
    "            shap_sub = shap[shap[\"subspace\"].astype(str).str.lower() == sub] if (not shap.empty and \"subspace\" in shap.columns) else shap\n",
    "            H = _hybrid_rank_table(base, shap_sub)\n",
    "            if H.empty:\n",
    "                continue\n",
    "            k_sel = _top_k_from_pct(len(H), pct)\n",
    "            sel.extend(H.head(k_sel)[\"feature\"].tolist())\n",
    "        sel = sorted(set(sel))\n",
    "        if not sel:\n",
    "            continue\n",
    "\n",
    "        for wl in workloads_probe:\n",
    "            if (setup, \"BENIGN\", wl) not in FILES or (setup, scen.upper(), wl) not in FILES:\n",
    "                continue\n",
    "\n",
    "            def _pairs(key):\n",
    "                p = FILES[key]\n",
    "                return [(p, _read_csv_safe(p))]\n",
    "\n",
    "            df_b = build_windowed_raw_means(_pairs((setup,\"BENIGN\",wl)), win=win, label=\"BENIGN\")\n",
    "            df_a = build_windowed_raw_means(_pairs((setup,scen.upper(),wl)), win=win, label=scen.upper())\n",
    "            if df_b.empty or df_a.empty:\n",
    "                continue\n",
    "\n",
    "            Xb = df_b[telemetry_cols(df_b)].astype(float)\n",
    "            mu, sd, q1, q2 = robust_scale_train(Xb.values, winsor=(2.0, 98.0))\n",
    "            Xb_z = apply_robust_scale(Xb, mu, sd, q1, q2); Xb_z.columns = Xb.columns\n",
    "            Xa   = df_a[telemetry_cols(df_a)].astype(float)\n",
    "            Xa_z = apply_robust_scale(Xa, mu, sd, q1, q2); Xa_z.columns = Xa.columns\n",
    "\n",
    "            X_all = pd.concat([Xb_z, Xa_z], ignore_index=True)\n",
    "            y_all = np.concatenate([np.zeros(len(Xb_z), dtype=int), np.ones(len(Xa_z), dtype=int)])\n",
    "\n",
    "            present = [f for f in sel if f in X_all.columns]\n",
    "            if not present:\n",
    "                continue\n",
    "\n",
    "            rng = np.random.RandomState(SEED)\n",
    "            idx = np.arange(len(y_all))\n",
    "            val_mask = np.zeros_like(y_all, dtype=bool)\n",
    "            val_mask[rng.choice(idx, size=max(1, int(0.30*len(y_all))), replace=False)] = True\n",
    "\n",
    "            X = X_all[present]\n",
    "            X_tr, y_tr = X.loc[~val_mask], y_all[~val_mask]\n",
    "            X_va, y_va = X.loc[val_mask],  y_all[val_mask]\n",
    "            if len(np.unique(y_va)) < 2 or len(y_tr) < 2:\n",
    "                continue\n",
    "\n",
    "            clf = XGBClassifier(\n",
    "                n_estimators=XGB_TREES, max_depth=4, learning_rate=0.08,\n",
    "                subsample=0.9, colsample_bytree=0.9,\n",
    "                random_state=SEED, tree_method=\"hist\",\n",
    "                n_jobs=XGB_THREADS, verbosity=0\n",
    "            )\n",
    "            fit_xgb(clf, X_tr, y_tr, X_va, y_va, esr=XGB_ESR)\n",
    "            pred = clf.predict_proba(X_va)[:, 1]\n",
    "            aucs.append(roc_auc_score(y_va, pred))\n",
    "\n",
    "            count += 1\n",
    "            if count % 8 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "    return float(np.nanmean(aucs)) if aucs else float(\"nan\")\n",
    "\n",
    "best_combo = {}\n",
    "for setup in PLATFORMS:\n",
    "    pairs = _available_win_kfold(setup)\n",
    "    if not pairs:\n",
    "        log.error(f\"No (win,kfold) pairs found for {setup} under {RANK_DIRS}\")\n",
    "        raise RuntimeError(f\"No CP-MI rank files for {setup}. See log: {LOG_FILE}\")\n",
    "\n",
    "    best_auc, best_t = -1.0, None\n",
    "    for (win, kfold) in pairs:\n",
    "        for pct in TOP_PCTS:\n",
    "            mean_auc = _evaluate_mean_auc(setup, int(win), int(kfold), int(pct), workloads_probe=PROBE_WORKLOADS)\n",
    "            log.info(f\"[SELECT_TEST] {setup} win={win} kfold={kfold} top%={pct} probeMeanAUC={mean_auc:.4f}\")\n",
    "            if not np.isnan(mean_auc) and mean_auc > best_auc:\n",
    "                best_auc, best_t = mean_auc, (int(win), int(kfold), int(pct))\n",
    "\n",
    "    if best_t is None:\n",
    "        best_t = (pairs[0][0], pairs[0][1], 50)\n",
    "    best_combo[setup] = best_t\n",
    "    log.info(f\"[SELECT] {setup}: win={best_t[0]} kfold={best_t[1]} top%={best_t[2]} (probeMeanAUC≈{best_auc:.3f})\")\n",
    "\n",
    "def _select_features_for_scenario(setup: str, scenario: str, win: int, kfold: int, pct: int):\n",
    "    cpmi = {sub: _load_cpmi_one(setup, win, kfold, sub) for sub in SUBSPACES}\n",
    "    pct_s, method_s = detail_map.get((setup, scenario.upper()), (None, None))\n",
    "    shap = pd.DataFrame()\n",
    "    if pct_s is not None and method_s is not None:\n",
    "        shap = _load_shap_best_full_exact(setup, scenario.upper(), win, kfold, int(pct_s), str(method_s))\n",
    "\n",
    "    sel_by_sub, all_sel = {}, []\n",
    "    for sub in SUBSPACES:\n",
    "        base = cpmi[sub]\n",
    "        if base.empty:\n",
    "            sel_by_sub[sub] = []\n",
    "            continue\n",
    "        shap_sub = shap[shap[\"subspace\"].astype(str).str.lower() == sub] if (not shap.empty and \"subspace\" in shap.columns) else shap\n",
    "        H = _hybrid_rank_table(base, shap_sub)\n",
    "        if H.empty:\n",
    "            sel_by_sub[sub] = []\n",
    "            continue\n",
    "        k_sel = _top_k_from_pct(len(H), pct)\n",
    "        feats = H.head(k_sel)[\"feature\"].tolist()\n",
    "        sel_by_sub[sub] = feats\n",
    "        all_sel.extend(feats)\n",
    "    return sel_by_sub, sorted(set(all_sel))\n",
    "\n",
    "out_csv = PAPER_OUT / \"per_workload_system_auc.csv\"\n",
    "cols = [\"Workload name\",\"platform\",\"test scenario\",\"win\",\"k-fold\",\"top %\",\n",
    "        \"num features total\",\"num features compute\",\"num features memory\",\"num features sensors\",\"AUCROC\"]\n",
    "pd.DataFrame(columns=cols).to_csv(out_csv, index=False)\n",
    "\n",
    "for setup in PLATFORMS:\n",
    "    win, kfold, pct = best_combo[setup]\n",
    "    for scen in SCENARIOS[setup]:\n",
    "        sel_by_sub, all_sel = _select_features_for_scenario(setup, scen, win, kfold, pct)\n",
    "\n",
    "        n_comp = len(sel_by_sub.get(\"compute\", []))\n",
    "        n_mem  = len(sel_by_sub.get(\"memory\", []))\n",
    "        n_sens = len(sel_by_sub.get(\"sensors\", []))\n",
    "        n_tot  = n_comp + n_mem + n_sens\n",
    "\n",
    "        for wl in WORKLOADS:\n",
    "            key_b = (setup,\"BENIGN\",wl)\n",
    "            key_a = (setup,scen.upper(),wl)\n",
    "\n",
    "            auc = float(\"nan\")\n",
    "            if key_b in FILES and key_a in FILES:\n",
    "                def _pairs(key):\n",
    "                    p = FILES[key]\n",
    "                    return [(p, _read_csv_safe(p))]\n",
    "\n",
    "                df_b = build_windowed_raw_means(_pairs(key_b), win=win, label=\"BENIGN\")\n",
    "                df_a = build_windowed_raw_means(_pairs(key_a), win=win, label=scen.upper())\n",
    "\n",
    "                if not df_b.empty and not df_a.empty:\n",
    "                    Xb = df_b[telemetry_cols(df_b)].astype(float)\n",
    "                    mu, sd, q1, q2 = robust_scale_train(Xb.values, winsor=(2.0, 98.0))\n",
    "                    Xb_z = apply_robust_scale(Xb, mu, sd, q1, q2); Xb_z.columns = Xb.columns\n",
    "                    Xa   = df_a[telemetry_cols(df_a)].astype(float)\n",
    "                    Xa_z = apply_robust_scale(Xa, mu, sd, q1, q2); Xa_z.columns = Xa.columns\n",
    "\n",
    "                    X_all = pd.concat([Xb_z, Xa_z], ignore_index=True)\n",
    "                    y_all = np.concatenate([np.zeros(len(Xb_z), dtype=int), np.ones(len(Xa_z), dtype=int)])\n",
    "\n",
    "                    present = [f for f in all_sel if f in X_all.columns]\n",
    "                    if present:\n",
    "                        rng = np.random.RandomState(SEED)\n",
    "                        idx = np.arange(len(y_all))\n",
    "                        val_mask = np.zeros_like(y_all, dtype=bool)\n",
    "                        val_mask[rng.choice(idx, size=max(1, int(0.30*len(y_all))), replace=False)] = True\n",
    "\n",
    "                        X = X_all[present]\n",
    "                        X_tr, y_tr = X.loc[~val_mask], y_all[~val_mask]\n",
    "                        X_va, y_va = X.loc[val_mask],  y_all[val_mask]\n",
    "\n",
    "                        if len(np.unique(y_va)) > 1 and len(y_tr) > 1:\n",
    "                            clf = XGBClassifier(\n",
    "                                n_estimators=XGB_TREES, max_depth=4, learning_rate=0.08,\n",
    "                                subsample=0.9, colsample_bytree=0.9,\n",
    "                                random_state=SEED, tree_method=\"hist\",\n",
    "                                n_jobs=XGB_THREADS, verbosity=0\n",
    "                            )\n",
    "                            fit_xgb(clf, X_tr, y_tr, X_va, y_va, esr=XGB_ESR)\n",
    "                            pred = clf.predict_proba(X_va)[:, 1]\n",
    "                            auc = float(roc_auc_score(y_va, pred))\n",
    "\n",
    "            row = {\n",
    "                \"Workload name\": wl,\n",
    "                \"platform\": setup,\n",
    "                \"test scenario\": scen,\n",
    "                \"win\": int(win),\n",
    "                \"k-fold\": int(kfold),\n",
    "                \"top %\": int(pct),\n",
    "                \"num features total\": int(n_tot),\n",
    "                \"num features compute\": int(n_comp),\n",
    "                \"num features memory\": int(n_mem),\n",
    "                \"num features sensors\": int(n_sens),\n",
    "                \"AUCROC\": auc\n",
    "            }\n",
    "            pd.DataFrame([row]).to_csv(out_csv, mode=\"a\", header=False, index=False)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "log.info(f\"[OK] CSV → {out_csv}\")\n",
    "log.info(\"==== END combined run ====\")\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Explainability OUT:\", OUT_COMP)\n",
    "print(\"CSV:\", out_csv)\n",
    "print(\"Log:\", LOG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e605b75-71f7-482b-a32b-1be7926a02f4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7c35fcd-b2fa-4923-9145-191b11eb4969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved best pct per (setup,anomaly,win,kfold,method) → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_pct_per_win_kfold_PER_METHOD.csv\n",
      "\n",
      "[SCAN] PER-PLATFORM best (WIN,KF) by mean AUCPR across platform anomalies\n",
      "  [PLATFORM BEST • DDR4] WIN=512 K=3 mean AUCPR=1.000 mean ROC=1.000 (anoms covered=2)\n",
      "      DDR4/DROOP: method=dC_aJ best%=10 AUCPR_med=1.000 ROC_med=1.000\n",
      "      DDR4/RH: method=dC_aJ best%=10 AUCPR_med=1.000 ROC_med=1.000\n",
      "  [PLATFORM BEST • DDR5] WIN=1024 K=10 mean AUCPR=1.000 mean ROC=1.000 (anoms covered=2)\n",
      "      DDR5/DROOP: method=dC_aJ best%=10 AUCPR_med=1.000 ROC_med=1.000\n",
      "      DDR5/SPECTRE: method=dC_aJ best%=10 AUCPR_med=1.000 ROC_med=1.000\n",
      "\n",
      "[OK] Saved platform winners → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
      "[OK] Saved platform details → /Users/hsiaopingni/octaneX_v7_4functions/Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n"
     ]
    }
   ],
   "source": [
    "# === PER-PLATFORM: Find BEST (win, kfold) and BEST pct per anomaly across full 10–100% sweep ===\n",
    "# UPDATED for your NEW pipeline outputs (no dependency on old per_workload_summary_all_raw...csv).\n",
    "#\n",
    "# Inputs:\n",
    "#   - Results/per_run_metrics_all_PIPELINE.csv   (from your full pipeline)\n",
    "#\n",
    "# Outputs:\n",
    "#   1) Results/BEST_in_DesignSpace_Post_per_platform.csv\n",
    "#        One row per platform (DDR4, DDR5) with chosen WIN/KF.\n",
    "#        Criterion: mean AUCPR across that platform's anomalies,\n",
    "#                   where each anomaly uses its own BEST (method,pct) under that (WIN,KF).\n",
    "#\n",
    "#   2) Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#        One row per platform×anomaly with that platform's WIN/KF and the anomaly-specific:\n",
    "#          best method, best pct, median/IQR/min/max for AUCPR and ROC (at the best pct)\n",
    "#\n",
    "#   3) Results/BEST_pct_per_win_kfold_PER_METHOD.csv   (optional but useful)\n",
    "#        Best pct for each (setup, anomaly, win, kfold, method)\n",
    "#\n",
    "# Notes:\n",
    "#   - Sort rule for “best”: AUCPR_median desc, ROC_median desc, smaller pct, then method order.\n",
    "#   - Methods considered: dC_aJ, dC_aM, dE_aJ, dE_aM\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT    = pl.Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES_DIR = pl.Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "if RES_DIR.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\"):\n",
    "    RES_DIR = RES_DIR.parent\n",
    "\n",
    "PER_RUN = RES_DIR / \"per_run_metrics_all_PIPELINE.csv\"\n",
    "if not PER_RUN.exists():\n",
    "    raise FileNotFoundError(f\"Missing per-run pipeline CSV: {PER_RUN}\")\n",
    "\n",
    "# Load per-run once\n",
    "df = pd.read_csv(PER_RUN).copy()\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "# Defensive grid you run\n",
    "valid_wins   = {32, 64, 128, 512, 1024}\n",
    "valid_kfolds = {3, 5, 10}\n",
    "df = df[df[\"win\"].isin(valid_wins) & df[\"kfold\"].isin(valid_kfolds)].copy()\n",
    "\n",
    "# Required columns\n",
    "need = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"run_id\",\"method\",\"roc_auc\",\"auc_pr\"}\n",
    "miss = need - set(df.columns)\n",
    "if miss:\n",
    "    raise KeyError(f\"{PER_RUN} missing columns: {sorted(miss)}. Have: {list(df.columns)}\")\n",
    "\n",
    "# Clean numeric\n",
    "for c in [\"win\",\"kfold\",\"pct\",\"roc_auc\",\"auc_pr\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"method\"]).copy()\n",
    "df[\"auc_pr\"]  = df[\"auc_pr\"].clip(0.0, 1.0)\n",
    "df[\"roc_auc\"] = df[\"roc_auc\"].clip(0.0, 1.0)\n",
    "\n",
    "ANOMALIES_BY_SETUP = {\"DDR4\": [\"DROOP\",\"RH\"], \"DDR5\": [\"DROOP\",\"SPECTRE\"]}\n",
    "METHOD_ORDER = [\"dC_aJ\", \"dC_aM\", \"dE_aJ\", \"dE_aM\"]\n",
    "method_rank = {m:i for i,m in enumerate(METHOD_ORDER)}\n",
    "\n",
    "df = df[df[\"setup\"].isin([\"DDR4\",\"DDR5\"])].copy()\n",
    "df = df[df[\"method\"].isin(METHOD_ORDER)].copy()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 0) Summarize per (setup, anomaly, win, kfold, method, pct) across run_id\n",
    "# ---------------------------------------------------------------------------\n",
    "def q1(x): return float(np.nanpercentile(x, 25))\n",
    "def q3(x): return float(np.nanpercentile(x, 75))\n",
    "\n",
    "keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\"pct\"]\n",
    "sumdf = (df.groupby(keys, as_index=False)\n",
    "           .agg(\n",
    "               auc_pr_median=(\"auc_pr\",\"median\"),\n",
    "               auc_pr_q1=(\"auc_pr\", q1),\n",
    "               auc_pr_q3=(\"auc_pr\", q3),\n",
    "               auc_pr_min=(\"auc_pr\",\"min\"),\n",
    "               auc_pr_max=(\"auc_pr\",\"max\"),\n",
    "               roc_auc_median=(\"roc_auc\",\"median\"),\n",
    "               roc_auc_q1=(\"roc_auc\", q1),\n",
    "               roc_auc_q3=(\"roc_auc\", q3),\n",
    "               roc_auc_min=(\"roc_auc\",\"min\"),\n",
    "               roc_auc_max=(\"roc_auc\",\"max\"),\n",
    "               n_runs=(\"run_id\",\"nunique\"),\n",
    "           ))\n",
    "\n",
    "sumdf[\"auc_pr_iqr\"]  = sumdf[\"auc_pr_q3\"]  - sumdf[\"auc_pr_q1\"]\n",
    "sumdf[\"roc_auc_iqr\"] = sumdf[\"roc_auc_q3\"] - sumdf[\"roc_auc_q1\"]\n",
    "\n",
    "# Fill NaNs for robust sorting\n",
    "sumdf[\"auc_pr_median_filled\"]  = sumdf[\"auc_pr_median\"].fillna(-np.inf)\n",
    "sumdf[\"roc_auc_median_filled\"] = sumdf[\"roc_auc_median\"].fillna(-np.inf)\n",
    "sumdf[\"_mrank\"] = sumdf[\"method\"].map(method_rank).fillna(999).astype(int)\n",
    "\n",
    "# Keep only anomalies defined per platform\n",
    "def _keep_defined(row):\n",
    "    setup = str(row[\"setup\"]).upper()\n",
    "    anom  = str(row[\"anomaly\"]).upper()\n",
    "    return setup in ANOMALIES_BY_SETUP and anom in [a.upper() for a in ANOMALIES_BY_SETUP[setup]]\n",
    "\n",
    "sumdf = sumdf[sumdf.apply(_keep_defined, axis=1)].copy()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# A) BEST pct per (setup, anomaly, win, kfold, method)\n",
    "# ---------------------------------------------------------------------------\n",
    "best_pct_per_method = (sumdf.sort_values(\n",
    "                            [\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\",\n",
    "                             \"auc_pr_median_filled\",\"roc_auc_median_filled\",\"pct\"],\n",
    "                            ascending=[True,True,True,True,True, False,False, True]\n",
    "                       )\n",
    "                       .groupby([\"setup\",\"anomaly\",\"win\",\"kfold\",\"method\"], as_index=False)\n",
    "                       .head(1)\n",
    "                       .rename(columns={\"pct\":\"best_pct_by_median\"}))\n",
    "\n",
    "BEST_PCT_PER_METHOD_CSV = RES_DIR / \"BEST_pct_per_win_kfold_PER_METHOD.csv\"\n",
    "best_pct_per_method.to_csv(BEST_PCT_PER_METHOD_CSV, index=False)\n",
    "print(f\"[OK] Saved best pct per (setup,anomaly,win,kfold,method) → {BEST_PCT_PER_METHOD_CSV}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# B) For each (setup, anomaly, win, kfold): pick BEST method (using its BEST pct)\n",
    "# ---------------------------------------------------------------------------\n",
    "best_pct_per_method[\"_mrank\"] = best_pct_per_method[\"method\"].map(method_rank).fillna(999).astype(int)\n",
    "\n",
    "best_method_per_case = (best_pct_per_method.sort_values(\n",
    "                            [\"setup\",\"anomaly\",\"win\",\"kfold\",\n",
    "                             \"auc_pr_median_filled\",\"roc_auc_median_filled\",\"_mrank\",\"best_pct_by_median\"],\n",
    "                            ascending=[True,True,True,True, False,False, True, True]\n",
    "                        )\n",
    "                        .groupby([\"setup\",\"anomaly\",\"win\",\"kfold\"], as_index=False)\n",
    "                        .head(1))\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# C) PER PLATFORM: choose ONE (win,kfold) by mean AUCPR across anomalies\n",
    "#    (each anomaly contributes its best method@best pct for that win/kfold)\n",
    "# ---------------------------------------------------------------------------\n",
    "platform_rows = []\n",
    "detail_rows = []\n",
    "\n",
    "print(\"\\n[SCAN] PER-PLATFORM best (WIN,KF) by mean AUCPR across platform anomalies\")\n",
    "\n",
    "for setup in [\"DDR4\",\"DDR5\"]:\n",
    "    sub = best_method_per_case[best_method_per_case[\"setup\"].astype(str).str.upper() == setup].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"  [WARN] No summarized rows for {setup}\")\n",
    "        continue\n",
    "\n",
    "    # aggregate across anomalies\n",
    "    agg = (sub.groupby([\"setup\",\"win\",\"kfold\"], as_index=False)\n",
    "             .agg(mean_auc_pr=(\"auc_pr_median_filled\",\"mean\"),\n",
    "                  mean_roc_auc=(\"roc_auc_median_filled\",\"mean\"),\n",
    "                  n_anoms=(\"anomaly\",\"nunique\")))\n",
    "\n",
    "    # prefer combos covering more anomalies, then mean AUCPR, mean ROC, then smaller win/kfold\n",
    "    agg = agg.sort_values(\n",
    "        [\"n_anoms\",\"mean_auc_pr\",\"mean_roc_auc\",\"win\",\"kfold\"],\n",
    "        ascending=[False, False, False, True, True]\n",
    "    )\n",
    "    top = agg.iloc[0]\n",
    "    win_best = int(top[\"win\"])\n",
    "    kf_best  = int(top[\"kfold\"])\n",
    "\n",
    "    platform_rows.append({\n",
    "        \"setup\": setup,\n",
    "        \"win\": win_best,\n",
    "        \"kfold\": kf_best,\n",
    "        \"mean_auc_pr_across_anomalies\": float(top[\"mean_auc_pr\"]),\n",
    "        \"mean_roc_auc_across_anomalies\": float(top[\"mean_roc_auc\"]),\n",
    "        \"num_anomalies_covered\": int(top[\"n_anoms\"]),\n",
    "        \"selection_note\": \"mean across anomalies; each anomaly uses its best (method,pct) for this (win,kfold)\",\n",
    "    })\n",
    "\n",
    "    chosen = sub[(sub[\"win\"]==win_best) & (sub[\"kfold\"]==kf_best)].copy()\n",
    "    chosen = chosen.sort_values(\n",
    "        [\"auc_pr_median_filled\",\"roc_auc_median_filled\",\"best_pct_by_median\"],\n",
    "        ascending=[False, False, True]\n",
    "    )\n",
    "\n",
    "    print(f\"  [PLATFORM BEST • {setup}] WIN={win_best} K={kf_best} \"\n",
    "          f\"mean AUCPR={float(top['mean_auc_pr']):.3f} mean ROC={float(top['mean_roc_auc']):.3f} \"\n",
    "          f\"(anoms covered={int(top['n_anoms'])})\")\n",
    "\n",
    "    for _, r in chosen.iterrows():\n",
    "        detail_rows.append({\n",
    "            \"setup\": setup,\n",
    "            \"anomaly\": str(r[\"anomaly\"]),\n",
    "            \"win\": win_best,\n",
    "            \"kfold\": kf_best,\n",
    "            \"best_pct_by_median\": int(r[\"best_pct_by_median\"]),\n",
    "            \"method\": str(r[\"method\"]),\n",
    "            \"auc_pr_median\": float(r[\"auc_pr_median\"]),\n",
    "            \"auc_pr_iqr\": float(r[\"auc_pr_iqr\"]),\n",
    "            \"auc_pr_min\": float(r[\"auc_pr_min\"]),\n",
    "            \"auc_pr_max\": float(r[\"auc_pr_max\"]),\n",
    "            \"roc_auc_median\": float(r[\"roc_auc_median\"]),\n",
    "            \"roc_auc_iqr\": float(r[\"roc_auc_iqr\"]),\n",
    "            \"roc_auc_min\": float(r[\"roc_auc_min\"]),\n",
    "            \"roc_auc_max\": float(r[\"roc_auc_max\"]),\n",
    "            \"n_runs\": int(r[\"n_runs\"]),\n",
    "        })\n",
    "        print(f\"      {setup}/{r['anomaly']}: method={r['method']} best%={int(r['best_pct_by_median'])} \"\n",
    "              f\"AUCPR_med={float(r['auc_pr_median']):.3f} ROC_med={float(r['roc_auc_median']):.3f}\")\n",
    "\n",
    "platform_best = pd.DataFrame(platform_rows).sort_values([\"setup\"]).reset_index(drop=True)\n",
    "platform_details = pd.DataFrame(detail_rows).sort_values([\"setup\",\"anomaly\"]).reset_index(drop=True)\n",
    "\n",
    "OUT_PLAT = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform.csv\"\n",
    "OUT_DET  = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "\n",
    "platform_best.to_csv(OUT_PLAT, index=False)\n",
    "platform_details.to_csv(OUT_DET, index=False)\n",
    "\n",
    "print(f\"\\n[OK] Saved platform winners → {OUT_PLAT}\")\n",
    "print(f\"[OK] Saved platform details → {OUT_DET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "04ace3da-da34-4dd2-ac7a-19a63d1147e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WROTE] /Users/hsiaopingni/octaneX_v7/Results/Explainability_Comparisons/stability_box_consecutive_memory.png\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------- locate rank dirs robustly --------------------\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7\"))\n",
    "RES_DIR_RAW = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "RES_DIR = RES_DIR_RAW.parent if RES_DIR_RAW.name == \"Explainability_SHAP_BestCases\" else RES_DIR_RAW\n",
    "\n",
    "# You can override these in globals() if you want:\n",
    "#   RANK_HYB_DIR = Path(\".../FeatureRankOUT_HYBRID\")\n",
    "#   RANK_CPM_DIR = Path(\".../FeatureRankOUT\")\n",
    "RANK_HYB_DIR = globals().get(\"RANK_HYB_DIR\", None)\n",
    "RANK_CPM_DIR = globals().get(\"RANK_CPM_DIR\", None)\n",
    "\n",
    "# Search lists\n",
    "RANK_HYB_DIRS = []\n",
    "RANK_CPM_DIRS = []\n",
    "\n",
    "if RANK_HYB_DIR is not None:\n",
    "    RANK_HYB_DIRS.append(Path(RANK_HYB_DIR))\n",
    "else:\n",
    "    RANK_HYB_DIRS += [\n",
    "        ROOT / \"FeatureRankOUT_HYBRID\",\n",
    "        RES_DIR / \"FeatureRankOUT_HYBRID\",\n",
    "        Path(\"/Volumes/Untitled\") / \"FeatureRankOUT_HYBRID\",\n",
    "        Path(\"/Volumes/Untitled\") / \"octaneX\" / \"FeatureRankOUT_HYBRID\",\n",
    "        Path.home() / \"Desktop\" / \"octaneX\" / \"FeatureRankOUT_HYBRID\",\n",
    "    ]\n",
    "\n",
    "if RANK_CPM_DIR is not None:\n",
    "    RANK_CPM_DIRS.append(Path(RANK_CPM_DIR))\n",
    "else:\n",
    "    RANK_CPM_DIRS += [\n",
    "        ROOT / \"FeatureRankOUT\",\n",
    "        RES_DIR / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"FeatureRankOUT\",\n",
    "        Path(\"/Volumes/Untitled\") / \"octaneX\" / \"FeatureRankOUT\",\n",
    "        Path.home() / \"Desktop\" / \"octaneX\" / \"FeatureRankOUT\",\n",
    "    ]\n",
    "\n",
    "# OUT directory must exist\n",
    "OUT = Path(globals().get(\"OUT\", RES_DIR / \"paper_exports\"))\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _find_rank_file(dirs, fname: str) -> Path | None:\n",
    "    for d in dirs:\n",
    "        d = Path(d)\n",
    "        if d.exists():\n",
    "            p = d / fname\n",
    "            if p.exists():\n",
    "                return p\n",
    "    # fallback slow search (only if needed)\n",
    "    for d in dirs:\n",
    "        d = Path(d)\n",
    "        if d.exists():\n",
    "            hits = list(d.rglob(fname))\n",
    "            if hits:\n",
    "                return hits[0]\n",
    "    return None\n",
    "\n",
    "# -------------------- your stability logic --------------------\n",
    "def _norm_name(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\",\"_\",str(s)).lower().replace(\"%\",\"pct\")\n",
    "    s = re.sub(r\"[^a-z0-9_]+\",\"_\", s)\n",
    "    return re.sub(r\"_+\",\"_\", s).strip(\"_\")\n",
    "\n",
    "def _read_rank_csv(p: Path | None, prefer_col=None):\n",
    "    if p is None or (not Path(p).exists()):\n",
    "        return []\n",
    "    df = pd.read_csv(p)\n",
    "    if df.empty:\n",
    "        return []\n",
    "    col = prefer_col if (prefer_col and prefer_col in df.columns) else df.columns[0]\n",
    "    return [_norm_name(x) for x in df[col].astype(str).tolist()]\n",
    "\n",
    "def _top_set(lst, pct):\n",
    "    if not lst:\n",
    "        return set()\n",
    "    k = max(1, int(np.ceil(len(lst) * (pct/100.0))))\n",
    "    return set(lst[:k])\n",
    "\n",
    "def _jacc(A, B):\n",
    "    if not A and not B:\n",
    "        return np.nan\n",
    "    den = len(A | B)\n",
    "    return (len(A & B) / den) if den else np.nan\n",
    "\n",
    "rows = []\n",
    "cases_for_ranks = best[[\"setup\",\"win\",\"kfold\"]].drop_duplicates()\n",
    "\n",
    "PCTS = list(range(10,101,10))\n",
    "for _, rr in cases_for_ranks.iterrows():\n",
    "    setup, win, kf = str(rr[\"setup\"]), int(rr[\"win\"]), int(rr[\"kfold\"])\n",
    "    fname = f\"{setup}_{win}_{kf}_0_memory.csv\"\n",
    "\n",
    "    pH = _find_rank_file(RANK_HYB_DIRS, fname)\n",
    "    pC = _find_rank_file(RANK_CPM_DIRS, fname)\n",
    "\n",
    "    H = _read_rank_csv(pH, prefer_col=\"feature_display\")  # hybrid file often has feature_display\n",
    "    C = _read_rank_csv(pC, prefer_col=\"feature\")          # cpmi file often has feature\n",
    "\n",
    "    for mode, L in [(\"Hybrid\", H), (\"CP-MI\", C)]:\n",
    "        if not L:\n",
    "            continue\n",
    "        tops = {p: _top_set(L, p) for p in PCTS}\n",
    "        consec = [_jacc(tops[a], tops[b]) for a, b in zip(PCTS[:-1], PCTS[1:])]\n",
    "        if consec:\n",
    "            rows.append({\"mode\": mode, \"val\": float(np.nanmean(consec))})\n",
    "\n",
    "stab_df = pd.DataFrame(rows)\n",
    "\n",
    "if not stab_df.empty:\n",
    "    plt.figure(figsize=(5.2,3.8), dpi=160)\n",
    "    data, labels = [], []\n",
    "\n",
    "    if (stab_df[\"mode\"]==\"Hybrid\").any():\n",
    "        data.append(stab_df.loc[stab_df[\"mode\"]==\"Hybrid\",\"val\"].values)\n",
    "        labels.append(\"Hybrid\")\n",
    "    if (stab_df[\"mode\"]==\"CP-MI\").any():\n",
    "        data.append(stab_df.loc[stab_df[\"mode\"]==\"CP-MI\",\"val\"].values)\n",
    "        labels.append(\"CP-MI\")\n",
    "\n",
    "    if not data:\n",
    "        print(\"[SKIP] stability_box_consecutive_memory (no values after filtering)\")\n",
    "    else:\n",
    "        plt.boxplot(data, labels=labels)\n",
    "        plt.ylim(0,1)\n",
    "        plt.ylabel(\"Consecutive Jaccard (Memory) ↑\")\n",
    "        plt.title(\"Within-method rank stability across Top-% steps\")\n",
    "        plt.tight_layout()\n",
    "        out_path = OUT / \"stability_box_consecutive_memory.png\"\n",
    "        plt.savefig(out_path)\n",
    "        plt.close()\n",
    "        print(\"[WROTE]\", out_path)\n",
    "else:\n",
    "    print(\"[SKIP] stability_box_consecutive_memory (no memory rank files found)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab5062a9-6f3c-41fd-8e30-8fb19c4bc732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDR5-DROOP AUC-PR: mean gain=0.0260 ± 0.0134, W=15.000, p=0.03125, FDR-p=0.0625, Cliffs_delta=0.920\n",
      "DDR5-DROOP ROC-AUC: mean gain=0.0020 ± 0.0045, W=1.000, p=0.5, FDR-p=0.5, Cliffs_delta=0.200\n"
     ]
    }
   ],
   "source": [
    "# pip install scipy numpy statsmodels (if needed)\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon, ttest_rel\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def paired_tests(baseline, hybrid, label, test='wilcoxon'):\n",
    "    \"\"\"\n",
    "    baseline, hybrid: 1D arrays of per-fold scores (e.g., AUC-PR across folds)\n",
    "    label: short name to print (e.g., 'DDR5-DROOP AUC-PR')\n",
    "    test: 'wilcoxon' (default) or 'ttest'\n",
    "    \"\"\"\n",
    "    baseline = np.asarray(baseline, float)\n",
    "    hybrid   = np.asarray(hybrid, float)\n",
    "    assert baseline.shape == hybrid.shape, \"Use paired arrays with same folds.\"\n",
    "\n",
    "    diff = hybrid - baseline\n",
    "    mean_gain = diff.mean()\n",
    "    std_gain  = diff.std(ddof=1)\n",
    "\n",
    "    if test == 'ttest':\n",
    "        t_stat, p = ttest_rel(hybrid, baseline, alternative='greater')  # test: hybrid > baseline\n",
    "        # Cohen's dz for paired samples\n",
    "        dz = mean_gain / (diff.std(ddof=1) + 1e-12)\n",
    "        eff = ('Cohen_dz', dz)\n",
    "        stat = ('t', t_stat)\n",
    "    else:\n",
    "        # Wilcoxon signed-rank (zero_method='wilcox' ignores zero diffs)\n",
    "        stat_w, p = wilcoxon(hybrid, baseline, alternative='greater', zero_method='wilcox')\n",
    "        # Cliff's delta (paired) approximation\n",
    "        # For small n, dz also OK; Cliff’s delta more robust:\n",
    "        # Compute ordinal effect size:\n",
    "        gt = sum((d1 > d2) for d1 in hybrid for d2 in baseline)\n",
    "        lt = sum((d1 < d2) for d1 in hybrid for d2 in baseline)\n",
    "        cliff_delta = (gt - lt) / (len(hybrid)*len(baseline))\n",
    "        eff = ('Cliffs_delta', cliff_delta)\n",
    "        stat = ('W', stat_w)\n",
    "\n",
    "    result = {\n",
    "        'label': label,\n",
    "        'n_folds': len(diff),\n",
    "        'baseline_mean': float(baseline.mean()),\n",
    "        'hybrid_mean': float(hybrid.mean()),\n",
    "        'mean_gain': float(mean_gain),\n",
    "        'std_gain': float(std_gain),\n",
    "        'stat_name': stat[0],\n",
    "        'stat_value': float(stat[1]),\n",
    "        'p_value': float(p),\n",
    "        'effect_name': eff[0],\n",
    "        'effect_value': float(eff[1]),\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# ==== EXAMPLE USAGE ====\n",
    "# Replace the arrays below with your per-fold results (same folds, same order).\n",
    "# Example: DDR5–DROOP AUC-PR (5 or 10 values each)\n",
    "octane_aucpr = np.array([0.95, 0.96, 0.99, 0.97, 0.98])   # baseline per-fold\n",
    "xoct_aucpr   = np.array([0.99, 1.00, 1.00, 0.99, 1.00])   # hybrid per-fold\n",
    "\n",
    "octane_roc   = np.array([0.99, 1.00, 1.00, 1.00, 1.00])\n",
    "xoct_roc     = np.array([1.00, 1.00, 1.00, 1.00, 1.00])\n",
    "\n",
    "results = []\n",
    "results.append(paired_tests(octane_aucpr, xoct_aucpr, 'DDR5-DROOP AUC-PR', test='wilcoxon'))\n",
    "results.append(paired_tests(octane_roc,   xoct_roc,   'DDR5-DROOP ROC-AUC', test='wilcoxon'))\n",
    "\n",
    "# If you have many workloads/metrics, do FDR across all p-values:\n",
    "pvals = [r['p_value'] for r in results]\n",
    "rej, pvals_fdr, *_ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
    "for r, pfdr, rj in zip(results, pvals_fdr, rej):\n",
    "    r['p_value_fdr'] = float(pfdr)\n",
    "    r['significant_fdr_0.05'] = bool(rj)\n",
    "\n",
    "# Pretty print\n",
    "for r in results:\n",
    "    print(\n",
    "        f\"{r['label']}: mean gain={r['mean_gain']:.4f} ± {r['std_gain']:.4f}, \"\n",
    "        f\"{r['stat_name']}={r['stat_value']:.3f}, p={r['p_value']:.4g}, \"\n",
    "        f\"FDR-p={r.get('p_value_fdr', r['p_value']):.4g}, \"\n",
    "        f\"{r['effect_name']}={r['effect_value']:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57a63e67-ab54-4da3-a8bd-9afdc74fa0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving outputs to: /Users/hsiaopingni/octaneX_v7/Results/Figures\n",
      "Saved: XOCTANE_vs_OCTANE_stats.png and XOCTANE_vs_OCTANE_stats.pdf\n",
      "AUC-PR: mean gain=0.0260, Wilcoxon W=15.000, p=0.03125\n",
      "ROC-AUC: mean gain=0.0020, Wilcoxon W=1.000, p=0.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wilcoxon\n",
    "from pathlib import Path\n",
    "\n",
    "# === Auto-detect ROOT directory ===\n",
    "EXT_DRIVE = Path(\"/Volumes/Untitled\")\n",
    "LOCAL_ROOT = Path(\"/Users/hsiaopingni/octaneX_v7\")\n",
    "ROOT = EXT_DRIVE / \"octaneX\" if EXT_DRIVE.exists() else LOCAL_ROOT\n",
    "\n",
    "OUT_DIR = ROOT / \"Results\" / \"Figures\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving outputs to: {OUT_DIR.resolve()}\")\n",
    "\n",
    "# === Example per-fold data (replace with actual results) ===\n",
    "octane_aucpr = np.array([0.95, 0.96, 0.99, 0.97, 0.98])\n",
    "xoct_aucpr   = np.array([0.99, 1.00, 1.00, 0.99, 1.00])\n",
    "\n",
    "octane_roc   = np.array([0.99, 1.00, 1.00, 1.00, 1.00])\n",
    "xoct_roc     = np.array([1.00, 1.00, 1.00, 1.00, 1.00])\n",
    "\n",
    "# === Compute statistics ===\n",
    "def stat_summary(baseline, hybrid, label):\n",
    "    diff = hybrid - baseline\n",
    "    mean_gain = diff.mean()\n",
    "    stat, p = wilcoxon(hybrid, baseline, alternative='greater')\n",
    "    return {'label': label, 'gain': mean_gain, 'stat': stat, 'p': p, 'diff': diff}\n",
    "\n",
    "res_aucpr = stat_summary(octane_aucpr, xoct_aucpr, 'AUC-PR')\n",
    "res_roc   = stat_summary(octane_roc, xoct_roc, 'ROC-AUC')\n",
    "\n",
    "# === Plot ===\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "for i, (metric, base, hybrid, res) in enumerate([\n",
    "    ('AUC-PR', octane_aucpr, xoct_aucpr, res_aucpr),\n",
    "    ('ROC-AUC', octane_roc, xoct_roc, res_roc)\n",
    "]):\n",
    "    x = np.arange(len(base)) + 1\n",
    "    ax[i].plot(x, base, 'o--', label='OCTANE (Baseline)', color='gray')\n",
    "    ax[i].plot(x, hybrid, 's-', label='X-OCTANE (Hybrid)', color='tab:blue')\n",
    "    ax[i].axhline(y=np.mean(base), color='gray', linestyle=':', alpha=0.6)\n",
    "    ax[i].axhline(y=np.mean(hybrid), color='tab:blue', linestyle='--', alpha=0.7)\n",
    "    ax[i].set_xlabel('Cross-Validation Fold')\n",
    "    ax[i].set_ylabel(metric)\n",
    "    ax[i].set_ylim(0.9, 1.02)\n",
    "    ax[i].grid(True, alpha=0.3)\n",
    "    ax[i].set_title(f\"{metric} — Δ={res['gain']:.3f}, p={res['p']:.3g}\")\n",
    "\n",
    "ax[0].legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# === Save figures ===\n",
    "png_path = OUT_DIR / \"XOCTANE_vs_OCTANE_stats.png\"\n",
    "pdf_path = OUT_DIR / \"XOCTANE_vs_OCTANE_stats.pdf\"\n",
    "plt.savefig(png_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(pdf_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved: {png_path.name} and {pdf_path.name}\")\n",
    "\n",
    "# === Text summary ===\n",
    "for res in [res_aucpr, res_roc]:\n",
    "    print(f\"{res['label']}: mean gain={res['gain']:.4f}, Wilcoxon W={res['stat']:.3f}, p={res['p']:.4g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "522adb7b-23dc-4c08-af17-56648cb996de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving outputs to: /Users/hsiaopingni/octaneX_v7_4functions/Results/Figures\n",
      "\n",
      "=== Statistical Summary ===\n",
      "AUC-PR:\n",
      "  Mean gain: 0.0360\n",
      "  Wilcoxon p-value: 0.0312\n",
      "  Paired t-test p-value: 0.0039\n",
      "\n",
      "ROC-AUC:\n",
      "  Mean gain: 0.0500\n",
      "  Wilcoxon p-value: 0.0312\n",
      "  Paired t-test p-value: 0.0004\n",
      "\n",
      "Figures saved to:\n",
      "  /Users/hsiaopingni/octaneX_v7_4functions/Results/Figures/XOCTANE_vs_OCTANE_ttest_wilcoxon.png\n",
      "  /Users/hsiaopingni/octaneX_v7_4functions/Results/Figures/XOCTANE_vs_OCTANE_ttest_wilcoxon.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wilcoxon, ttest_rel\n",
    "from pathlib import Path\n",
    "\n",
    "# === Auto-detect ROOT directory ===\n",
    "EXT_DRIVE = Path(\"/Volumes/Untitled\")\n",
    "LOCAL_ROOT = Path(\"/Users/hsiaopingni/octaneX_v7_4functions\")\n",
    "ROOT = EXT_DRIVE / \"octaneX\" if EXT_DRIVE.exists() else LOCAL_ROOT\n",
    "\n",
    "OUT_DIR = ROOT / \"Results\" / \"Figures\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving outputs to: {OUT_DIR.resolve()}\")\n",
    "\n",
    "# === Example per-fold data (replace with real results) ===\n",
    "octane_aucpr = np.array([0.93, 0.96, 0.97, 0.94, 0.98])\n",
    "xoct_aucpr   = np.array([0.98, 0.99, 1.00, 0.99, 1.00])\n",
    "\n",
    "octane_roc   = np.array([0.90, 0.93, 0.95, 0.92, 0.94])\n",
    "xoct_roc     = np.array([0.96, 0.97, 0.99, 0.98, 0.99])\n",
    "\n",
    "# === Function for paired comparison summary ===\n",
    "def paired_stats(baseline, hybrid, label):\n",
    "    diff = hybrid - baseline\n",
    "    mean_gain = diff.mean()\n",
    "    w_stat, w_p = wilcoxon(hybrid, baseline, alternative='greater')\n",
    "    t_stat, t_p = ttest_rel(hybrid, baseline)\n",
    "    return {\n",
    "        'label': label,\n",
    "        'gain': mean_gain,\n",
    "        'wilcoxon_p': w_p,\n",
    "        't_p': t_p,\n",
    "        'diff': diff\n",
    "    }\n",
    "\n",
    "res_aucpr = paired_stats(octane_aucpr, xoct_aucpr, 'AUC-PR')\n",
    "res_roc   = paired_stats(octane_roc, xoct_roc, 'ROC-AUC')\n",
    "\n",
    "# === Plot ===\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "for i, (metric, base, hybrid, res) in enumerate([\n",
    "    ('AUC-PR', octane_aucpr, xoct_aucpr, res_aucpr),\n",
    "    ('ROC-AUC', octane_roc, xoct_roc, res_roc)\n",
    "]):\n",
    "    x = np.arange(1, len(base) + 1)\n",
    "    ax[i].plot(x, base, 'o--', label='OCTANE (Baseline)', color='gray')\n",
    "    ax[i].plot(x, hybrid, 's-', label='X-OCTANE (Hybrid)', color='tab:blue')\n",
    "    ax[i].axhline(np.mean(base), color='gray', linestyle=':', alpha=0.6)\n",
    "    ax[i].axhline(np.mean(hybrid), color='tab:blue', linestyle='--', alpha=0.7)\n",
    "    ax[i].set_xlabel('Cross-Validation Fold')\n",
    "    ax[i].set_ylabel(metric)\n",
    "    ax[i].set_ylim(0.85, 1.02)\n",
    "    ax[i].grid(alpha=0.3)\n",
    "    ax[i].set_title(\n",
    "        f\"{metric}\\nΔ={res['gain']:.3f} | \"\n",
    "        f\"p₍W₎={res['wilcoxon_p']:.3f}, p₍t₎={res['t_p']:.3f}\"\n",
    "    )\n",
    "\n",
    "ax[0].legend(loc='lower right', fontsize=9)\n",
    "plt.tight_layout()\n",
    "\n",
    "# === Save outputs ===\n",
    "png_path = OUT_DIR / \"XOCTANE_vs_OCTANE_ttest_wilcoxon.png\"\n",
    "pdf_path = OUT_DIR / \"XOCTANE_vs_OCTANE_ttest_wilcoxon.pdf\"\n",
    "plt.savefig(png_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(pdf_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# === Summary printout ===\n",
    "print(\"\\n=== Statistical Summary ===\")\n",
    "for res in [res_aucpr, res_roc]:\n",
    "    print(f\"{res['label']}:\")\n",
    "    print(f\"  Mean gain: {res['gain']:.4f}\")\n",
    "    print(f\"  Wilcoxon p-value: {res['wilcoxon_p']:.4f}\")\n",
    "    print(f\"  Paired t-test p-value: {res['t_p']:.4f}\")\n",
    "    print()\n",
    "print(f\"Figures saved to:\\n  {png_path}\\n  {pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcafc6-cc55-4190-94e2-6b7ca40e8549",
   "metadata": {},
   "source": [
    "# **Jaccard agreement J@𝑝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "39ea5b17-af2e-4e37-ad63-2098ee7a36da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Available (WIN,K) in summary CSV:\n",
      "  DDR4: [(512, 3)]\n",
      "  DDR5: [(1024, 5)]\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/Jaccard_PLATFORM_DDR4_WIN512_K3.png\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/Jaccard_PLATFORM_DDR5_WIN1024_K5.png\n"
     ]
    }
   ],
   "source": [
    "# === Fix \"[SKIP] No rows ...\" by auto-resolving the correct (WIN,KF) per platform ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Paths ---\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES_DIR = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "if RES_DIR.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\"):\n",
    "    RES_DIR = RES_DIR.parent\n",
    "\n",
    "OUT_DIR = RES_DIR / \"Explainability_SHAP_BestPlatforms\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CSV  = OUT_DIR / \"SHAP_vs_CPMI_summary_PLATFORM.csv\"\n",
    "PLAT = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform.csv\"\n",
    "\n",
    "assert CSV.exists(), f\"Missing {CSV}. Run the per-platform SHAP-vs-CPMI summary cell first.\"\n",
    "assert PLAT.exists(), f\"Missing {PLAT}. Generate per-platform winners first.\"\n",
    "\n",
    "# --- Load ---\n",
    "df = pd.read_csv(CSV).copy()\n",
    "pb = pd.read_csv(PLAT).copy()\n",
    "\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "pb.columns = [c.lower() for c in pb.columns]\n",
    "\n",
    "need = {\"setup\",\"win\",\"kfold\",\"subspace\",\"k\",\"jaccard\"}\n",
    "missing = need - set(df.columns)\n",
    "assert not missing, f\"Missing columns in summary CSV: {sorted(missing)}. Have: {list(df.columns)}\"\n",
    "\n",
    "# Force numeric typing for robust comparisons\n",
    "df[\"win\"] = pd.to_numeric(df[\"win\"], errors=\"coerce\")\n",
    "df[\"kfold\"] = pd.to_numeric(df[\"kfold\"], errors=\"coerce\")\n",
    "df[\"jaccard\"] = pd.to_numeric(df[\"jaccard\"], errors=\"coerce\")\n",
    "\n",
    "# --- Plot styling to match your screenshot ---\n",
    "COLORS  = {\"compute\":\"tab:orange\", \"memory\":\"tab:blue\", \"sensors\":\"tab:green\"}\n",
    "MARKERS = {\"compute\":\"o\",          \"memory\":\"s\",        \"sensors\":\"^\"}\n",
    "LABELS  = {\"compute\":\"Compute\",    \"memory\":\"Memory\",  \"sensors\":\"Sensors\"}\n",
    "\n",
    "def _k_to_pct(kstr):\n",
    "    try:\n",
    "        return int(str(kstr).replace(\"top\",\"\").replace(\"%\",\"\"))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _available_pairs_for_setup(setup_label: str):\n",
    "    sub = df[df[\"setup\"].astype(str).str.upper() == setup_label.upper()].copy()\n",
    "    sub = sub.dropna(subset=[\"win\",\"kfold\"])\n",
    "    pairs = sorted(set((int(w), int(k)) for w,k in zip(sub[\"win\"], sub[\"kfold\"])))\n",
    "    return pairs\n",
    "\n",
    "def _resolve_platform_pair(setup_label: str):\n",
    "    \"\"\"\n",
    "    Try to use (win,kfold) from BEST_in_DesignSpace_Post_per_platform.csv.\n",
    "    If that pair isn't present in the summary CSV, fallback to the first available pair in the summary.\n",
    "    \"\"\"\n",
    "    pairs_avail = _available_pairs_for_setup(setup_label)\n",
    "    if not pairs_avail:\n",
    "        return None\n",
    "\n",
    "    row = pb[pb[\"setup\"].astype(str).str.upper() == setup_label.upper()].copy()\n",
    "    if not row.empty:\n",
    "        win0 = int(pd.to_numeric(row[\"win\"].iloc[0], errors=\"coerce\"))\n",
    "        kf0  = int(pd.to_numeric(row[\"kfold\"].iloc[0], errors=\"coerce\"))\n",
    "        if (win0, kf0) in pairs_avail:\n",
    "            return (win0, kf0)\n",
    "\n",
    "    # fallback: pick the pair with most rows (most complete)\n",
    "    sub = df[df[\"setup\"].astype(str).str.upper() == setup_label.upper()].copy()\n",
    "    sub = sub.dropna(subset=[\"win\",\"kfold\"])\n",
    "    counts = (sub.groupby([\"win\",\"kfold\"]).size().reset_index(name=\"n\")\n",
    "                .sort_values(\"n\", ascending=False))\n",
    "    w = int(counts.iloc[0][\"win\"])\n",
    "    k = int(counts.iloc[0][\"kfold\"])\n",
    "    return (w, k)\n",
    "\n",
    "def plot_jaccard_platform(setup_label: str, win: int, kfold: int, out_png: Path):\n",
    "    sub = df[\n",
    "        (df[\"setup\"].astype(str).str.upper() == setup_label.upper()) &\n",
    "        (df[\"win\"].round(0) == int(win)) &\n",
    "        (df[\"kfold\"].round(0) == int(kfold))\n",
    "    ].copy()\n",
    "\n",
    "    if sub.empty:\n",
    "        print(f\"[SKIP] No rows for {setup_label} WIN={win} K={kfold}\")\n",
    "        print(f\"       Available (WIN,K) for {setup_label}: {_available_pairs_for_setup(setup_label)}\")\n",
    "        return\n",
    "\n",
    "    sub[\"pct\"] = sub[\"k\"].map(_k_to_pct)\n",
    "    sub = sub[np.isfinite(sub[\"pct\"])].copy()\n",
    "    sub[\"pct\"] = sub[\"pct\"].astype(int)\n",
    "\n",
    "    agg = (sub.groupby([\"subspace\",\"pct\"], as_index=False)[\"jaccard\"]\n",
    "             .median()\n",
    "             .sort_values([\"subspace\",\"pct\"]))\n",
    "\n",
    "    plt.figure(figsize=(7.6, 4.3), dpi=160)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    for ssp in [\"compute\",\"memory\",\"sensors\"]:\n",
    "        cur = agg[agg[\"subspace\"].astype(str).str.lower() == ssp]\n",
    "        if cur.empty:\n",
    "            continue\n",
    "        x = cur[\"pct\"].to_numpy()\n",
    "        y = np.clip(cur[\"jaccard\"].to_numpy(dtype=float), 0.0, 1.0)\n",
    "        ax.plot(x, y, color=COLORS[ssp], marker=MARKERS[ssp],\n",
    "                linewidth=2.2, markersize=6, label=LABELS[ssp])\n",
    "\n",
    "    ax.set_xlim(8, 102)\n",
    "    ax.set_ylim(0.0, 1.02)\n",
    "    ax.set_xticks(list(range(10, 101, 10)))\n",
    "    ax.set_xlabel(\"Top-% of features\", fontsize=16)\n",
    "    ax.set_ylabel(\"Jaccard index (CP-MI vs SHAP)\", fontsize=16)\n",
    "    ax.grid(True, which=\"major\", linestyle=\"--\", linewidth=0.8, alpha=0.35)\n",
    "    ax.legend(loc=\"lower right\", frameon=True, fontsize=12)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.tick_params(labelsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(out_png, dpi=220, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    plt.close()\n",
    "    print(\"[WROTE]\", out_png)\n",
    "\n",
    "# ---- Quick diagnostic: show what exists for DDR5 ----\n",
    "print(\"[INFO] Available (WIN,K) in summary CSV:\")\n",
    "for setup in [\"DDR4\",\"DDR5\"]:\n",
    "    print(f\"  {setup}: {_available_pairs_for_setup(setup)}\")\n",
    "\n",
    "# ---- Generate plots using resolved platform pairs ----\n",
    "for setup in [\"DDR4\",\"DDR5\"]:\n",
    "    pair = _resolve_platform_pair(setup)\n",
    "    if pair is None:\n",
    "        print(f\"[WARN] No rows at all for {setup} in {CSV}\")\n",
    "        continue\n",
    "    win, kf = pair\n",
    "    out_png = OUT_DIR / f\"Jaccard_PLATFORM_{setup}_WIN{win}_K{kf}.png\"\n",
    "    plot_jaccard_platform(setup, win=win, kfold=kf, out_png=out_png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22af40-2d0a-468a-a5c9-d44af5687cb3",
   "metadata": {},
   "source": [
    "# **Platform Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64299eb1-bfbc-4596-a9ec-daac7c689ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Platform  Window Size  K  Cases used J@80% (approx) rho (approx)\n",
      "DDR4 (Platform)          512  3           2         ≈ 0.94       ≈ 0.81\n",
      "DDR5 (Platform)         1024  5           2         ≈ 0.86       ≈ 0.73\n",
      "\n",
      "[OK] Wrote: /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/Table6_platform_agreement.csv\n",
      "[INFO] J and rho computed on the SHARED feature universe at top-80% per subspace.\n"
     ]
    }
   ],
   "source": [
    "# === Table 6 (PER PLATFORM, paper-faithful): Agreement between CP–MI and SHAP ===\n",
    "# You requested: DDR5 (Platform) WIN=1024 K=5  ✅ (forced override)\n",
    "#\n",
    "# Revision (fix for low J@k with high rho):\n",
    "#   - Compute J@p on the SHARED feature universe per subspace (intersection-normalized).\n",
    "#   - Compute rho on the SHARED universe as well (percentile-rank Spearman).\n",
    "#\n",
    "# Inputs:\n",
    "#   - Results/BEST_in_DesignSpace_Post_per_platform_details.csv   (best pct+method per anomaly; used if matches WIN/K)\n",
    "#   - FeatureRankOUT/<setup>_<win>_<kfold>_0_{compute|memory|sensors}.csv\n",
    "#   - Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN<w>_KF<k>_PCT<p>_M<method>.csv\n",
    "#\n",
    "# Output:\n",
    "#   - Results/Explainability_SHAP_BestPlatforms/Table6_platform_agreement.csv\n",
    "#\n",
    "# NOTE:\n",
    "#   This script uses a MANUAL platform config override (DDR5 WIN=1024 K=5).\n",
    "#   If your per-platform details CSV does not contain rows for that (WIN,K),\n",
    "#   it will fall back to discovering SHAP files on disk for that (WIN,K) per anomaly.\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# -------------------- Paths (robust) --------------------\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES_DIR_RAW = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "RES_DIR = RES_DIR_RAW.parent if RES_DIR_RAW.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\") else RES_DIR_RAW\n",
    "\n",
    "OUT_DIR = RES_DIR / \"Explainability_SHAP_BestPlatforms\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PLAT_DETAIL = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "assert PLAT_DETAIL.exists(), f\"Missing: {PLAT_DETAIL}\"\n",
    "\n",
    "# Rank dirs (CP–MI)\n",
    "RANK_DIRS = list(globals().get(\"RANK_DIRS\", [\n",
    "    ROOT / \"FeatureRankOUT\",\n",
    "    Path(\"/Volumes/Untitled\") / \"FeatureRankOUT\",\n",
    "    Path(\"/Volumes/Untitled\") / \"octaneX\" / \"FeatureRankOUT\",\n",
    "    Path.home() / \"Desktop\" / \"octaneX\" / \"FeatureRankOUT\",\n",
    "]))\n",
    "\n",
    "# SHAP dir (per platform)\n",
    "SHAP_DIR = RES_DIR / \"Explainability_SHAP_BestPlatforms\"\n",
    "assert SHAP_DIR.exists(), f\"Missing SHAP dir: {SHAP_DIR}\"\n",
    "\n",
    "# -------------------- Table settings --------------------\n",
    "PCT_FOR_TABLE = int(globals().get(\"PCT_FOR_TABLE\", 80))  # default 80 for convergence region\n",
    "SUBSPACES = (\"compute\", \"memory\", \"sensors\")\n",
    "\n",
    "# -------------------- REQUIRED OVERRIDE (your request) --------------------\n",
    "# Force the (WIN,K) used for Table 6 per platform:\n",
    "PLATFORM_CFG = {\n",
    "    \"DDR4\": {\"win\": 512,  \"kfold\": 3, \"label\": \"DDR4 (Platform)\"},\n",
    "    \"DDR5\": {\"win\": 1024, \"kfold\": 5, \"label\": \"DDR5 (Platform)\"},\n",
    "}\n",
    "\n",
    "# -------------------- Normalization helpers --------------------\n",
    "def _norm_name(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \"_\", str(s)).lower().replace(\"%\", \"pct\")\n",
    "    s = re.sub(r\"[^a-z0-9_]+\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "def _top_k_from_pct(n: int, pct: int) -> int:\n",
    "    return max(1, int(np.ceil(n * (pct / 100.0)))) if n > 0 and pct > 0 else 0\n",
    "\n",
    "def _jacc(A: set, B: set) -> float:\n",
    "    if not A and not B:\n",
    "        return np.nan\n",
    "    den = len(A | B)\n",
    "    return (len(A & B) / den) if den else np.nan\n",
    "\n",
    "def _restrict_to_common(order: list[str], common: set[str]) -> list[str]:\n",
    "    return [f for f in order if f in common]\n",
    "\n",
    "def _percentile_rank_from_order(features_in_order: list[str]) -> dict[str, float]:\n",
    "    n = len(features_in_order)\n",
    "    if n <= 1:\n",
    "        return {f: 1.0 for f in features_in_order} if n == 1 else {}\n",
    "    # best=1, worst=0\n",
    "    return {f: (1.0 - (i / (n - 1))) for i, f in enumerate(features_in_order)}\n",
    "\n",
    "def _jacc_top_pct_on_common(cp_list: list[str], sh_list: list[str], pct: int) -> float:\n",
    "    common = set(cp_list) & set(sh_list)\n",
    "    if len(common) == 0:\n",
    "        return np.nan\n",
    "    cp_c = _restrict_to_common(cp_list, common)\n",
    "    sh_c = _restrict_to_common(sh_list, common)\n",
    "    k = _top_k_from_pct(len(common), pct)  # top pct of COMMON size\n",
    "    return _jacc(set(cp_c[:k]), set(sh_c[:k]))\n",
    "\n",
    "def _spearman_on_common(cp_order: list[str], sh_order: list[str]) -> float:\n",
    "    common = set(cp_order) & set(sh_order)\n",
    "    if len(common) < 3:\n",
    "        return np.nan\n",
    "    cp_c = _restrict_to_common(cp_order, common)\n",
    "    sh_c = _restrict_to_common(sh_order, common)\n",
    "    cp_pr = _percentile_rank_from_order(cp_c)\n",
    "    sh_pr = _percentile_rank_from_order(sh_c)\n",
    "    uni = sorted(common)\n",
    "    x = np.array([cp_pr.get(f, 0.0) for f in uni], dtype=float)\n",
    "    y = np.array([sh_pr.get(f, 0.0) for f in uni], dtype=float)\n",
    "    rho, _ = spearmanr(x, y)\n",
    "    return float(rho)\n",
    "\n",
    "# -------------------- CP-MI ranks --------------------\n",
    "def _find_rank_file(setup: str, win: int, kfold: int, sub: str) -> Optional[Path]:\n",
    "    fname = f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "    for d in RANK_DIRS:\n",
    "        p = Path(d) / fname\n",
    "        if p.exists():\n",
    "            return p\n",
    "    for d in RANK_DIRS:\n",
    "        d = Path(d)\n",
    "        if d.exists():\n",
    "            hits = list(d.rglob(fname))\n",
    "            if hits:\n",
    "                return hits[0]\n",
    "    return None\n",
    "\n",
    "def _read_cpmi_rank_list(setup: str, win: int, kfold: int, sub: str) -> list[str]:\n",
    "    p = _find_rank_file(setup, win, kfold, sub)\n",
    "    if p is None:\n",
    "        return []\n",
    "    df = pd.read_csv(p)\n",
    "    if df.empty:\n",
    "        return []\n",
    "    col = \"feature\" if \"feature\" in df.columns else df.columns[0]\n",
    "    return [_norm_name(x) for x in df[col].astype(str).tolist()]\n",
    "\n",
    "# -------------------- SHAP loading --------------------\n",
    "def _load_details() -> pd.DataFrame:\n",
    "    d = pd.read_csv(PLAT_DETAIL).copy()\n",
    "    d.columns = [c.lower() for c in d.columns]\n",
    "    if \"best_method\" in d.columns and \"method\" not in d.columns:\n",
    "        d = d.rename(columns={\"best_method\":\"method\"})\n",
    "    if \"best_pct_by_median\" not in d.columns and \"pct\" in d.columns:\n",
    "        d = d.rename(columns={\"pct\":\"best_pct_by_median\"})\n",
    "    return d\n",
    "\n",
    "DETAILS = _load_details()\n",
    "need_pd = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"best_pct_by_median\",\"method\"}\n",
    "miss2 = need_pd - set(DETAILS.columns)\n",
    "if miss2:\n",
    "    raise KeyError(f\"{PLAT_DETAIL} missing columns: {sorted(miss2)}. Have: {list(DETAILS.columns)}\")\n",
    "\n",
    "def _find_best_pct_method_from_details(setup: str, anomaly: str, win: int, kfold: int) -> Optional[Tuple[int,str]]:\n",
    "    sub = DETAILS[\n",
    "        (DETAILS[\"setup\"].astype(str).str.upper() == setup.upper()) &\n",
    "        (DETAILS[\"anomaly\"].astype(str).str.upper() == anomaly.upper()) &\n",
    "        (pd.to_numeric(DETAILS[\"win\"], errors=\"coerce\") == int(win)) &\n",
    "        (pd.to_numeric(DETAILS[\"kfold\"], errors=\"coerce\") == int(kfold))\n",
    "    ].copy()\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    r = sub.iloc[0]\n",
    "    pct = int(pd.to_numeric(r[\"best_pct_by_median\"], errors=\"coerce\"))\n",
    "    method = str(r[\"method\"]).strip()\n",
    "    return pct, method\n",
    "\n",
    "def _find_shap_file(setup: str, anomaly: str, win: int, kfold: int, pct: int, method: str) -> Optional[Path]:\n",
    "    exact = SHAP_DIR / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT{pct}_M{method}.csv\"\n",
    "    if exact.exists():\n",
    "        return exact\n",
    "    # fallback: any pct/method match for that (setup, anomaly, win, kfold)\n",
    "    hits = sorted(SHAP_DIR.glob(f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT*_M*.csv\"))\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "def _load_shap_full_from_file(p: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(p)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    if \"shap_mean_abs\" not in df.columns and \"importance\" in df.columns:\n",
    "        df = df.rename(columns={\"importance\": \"shap_mean_abs\"})\n",
    "    need = {\"feature\",\"subspace\",\"shap_mean_abs\"}\n",
    "    if not need.issubset(set(df.columns)):\n",
    "        return pd.DataFrame()\n",
    "    df[\"feature\"] = df[\"feature\"].astype(str).map(_norm_name)\n",
    "    df[\"subspace\"] = df[\"subspace\"].astype(str).str.lower()\n",
    "    df[\"shap_mean_abs\"] = pd.to_numeric(df[\"shap_mean_abs\"], errors=\"coerce\").fillna(0.0)\n",
    "    return df\n",
    "\n",
    "def _load_shap_full(setup: str, anomaly: str, win: int, kfold: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prefer details-specified (pct,method). If not available, fall back to any SHAP file for that win/kfold.\n",
    "    \"\"\"\n",
    "    pm = _find_best_pct_method_from_details(setup, anomaly, win, kfold)\n",
    "    if pm is not None:\n",
    "        pct, method = pm\n",
    "        p = _find_shap_file(setup, anomaly, win, kfold, pct, method)\n",
    "        if p is not None:\n",
    "            return _load_shap_full_from_file(p)\n",
    "\n",
    "    # fallback: any file for this (setup, anomaly, win, kfold)\n",
    "    hits = sorted(SHAP_DIR.glob(f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT*_M*.csv\"))\n",
    "    if not hits:\n",
    "        return pd.DataFrame()\n",
    "    return _load_shap_full_from_file(hits[0])\n",
    "\n",
    "def _available_anomalies_for(setup: str) -> list[str]:\n",
    "    # anomalies from details file if possible, otherwise from shap filenames\n",
    "    sub = DETAILS[DETAILS[\"setup\"].astype(str).str.upper() == setup.upper()]\n",
    "    anoms = sorted(sub[\"anomaly\"].astype(str).str.upper().unique().tolist())\n",
    "    if anoms:\n",
    "        return anoms\n",
    "    # fallback: parse filenames\n",
    "    anoms2 = set()\n",
    "    for p in SHAP_DIR.glob(f\"SHAP_BESTPLAT_full_{setup}_*_WIN*_KF*_PCT*_M*.csv\"):\n",
    "        parts = p.name.split(\"_\")\n",
    "        if len(parts) >= 4:\n",
    "            anoms2.add(parts[3].upper())  # SHAP_BESTPLAT_full_<setup>_<anomaly>_...\n",
    "    return sorted(anoms2)\n",
    "\n",
    "# -------------------- Compute platform agreement --------------------\n",
    "def platform_metrics(setup: str, win: int, kfold: int) -> tuple[float, float, int]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      J_platform: median across anomalies of mean-subspace J@p (shared-universe)\n",
    "      rho_platform: median across anomalies of mean-subspace rho (shared-universe)\n",
    "      n_used: number of anomalies used\n",
    "    \"\"\"\n",
    "    anomalies = _available_anomalies_for(setup)\n",
    "    case_J, case_rho = [], []\n",
    "    used = 0\n",
    "\n",
    "    for anom in anomalies:\n",
    "        shap_full = _load_shap_full(setup, anom, win, kfold)\n",
    "        if shap_full.empty:\n",
    "            continue\n",
    "\n",
    "        Js, Rhos = [], []\n",
    "        for subspace in SUBSPACES:\n",
    "            cp_list = _read_cpmi_rank_list(setup, win, kfold, subspace)\n",
    "            if not cp_list:\n",
    "                continue\n",
    "\n",
    "            sh_sub = shap_full[shap_full[\"subspace\"] == subspace][[\"feature\",\"shap_mean_abs\"]].copy()\n",
    "            if sh_sub.empty:\n",
    "                continue\n",
    "\n",
    "            sh_list = sh_sub.sort_values(\"shap_mean_abs\", ascending=False)[\"feature\"].tolist()\n",
    "\n",
    "            Js.append(_jacc_top_pct_on_common(cp_list, sh_list, PCT_FOR_TABLE))\n",
    "            Rhos.append(_spearman_on_common(cp_list, sh_list))\n",
    "\n",
    "        if Js:\n",
    "            case_J.append(float(np.nanmean(Js)))\n",
    "        if Rhos:\n",
    "            case_rho.append(float(np.nanmean(Rhos)))\n",
    "\n",
    "        used += 1\n",
    "\n",
    "    J_platform = float(np.nanmedian(case_J)) if case_J else np.nan\n",
    "    rho_platform = float(np.nanmedian(case_rho)) if case_rho else np.nan\n",
    "    return J_platform, rho_platform, used\n",
    "\n",
    "# -------------------- Build Table 6 --------------------\n",
    "rows = []\n",
    "for setup, cfg in PLATFORM_CFG.items():\n",
    "    win = int(cfg[\"win\"])\n",
    "    kf  = int(cfg[\"kfold\"])\n",
    "    label = str(cfg[\"label\"])\n",
    "\n",
    "    Jp, rho, n_used = platform_metrics(setup, win, kf)\n",
    "\n",
    "    rows.append({\n",
    "        \"Platform\": label,\n",
    "        \"Window Size\": win,\n",
    "        \"K\": kf,\n",
    "        \"Cases used\": n_used,\n",
    "        f\"J@{PCT_FOR_TABLE}%\": Jp,\n",
    "        \"rho\": rho,\n",
    "        f\"J@{PCT_FOR_TABLE}% (approx)\": (f\"≈ {Jp:.2f}\" if np.isfinite(Jp) else \"NA\"),\n",
    "        \"rho (approx)\": (f\"≈ {rho:.2f}\" if np.isfinite(rho) else \"NA\"),\n",
    "    })\n",
    "\n",
    "tab6 = pd.DataFrame(rows)\n",
    "tab6_out = tab6[[\"Platform\",\"Window Size\",\"K\",\"Cases used\", f\"J@{PCT_FOR_TABLE}% (approx)\", \"rho (approx)\"]].copy()\n",
    "\n",
    "OUT_CSV = OUT_DIR / \"Table6_platform_agreement.csv\"\n",
    "tab6_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(tab6_out.to_string(index=False))\n",
    "print(\"\\n[OK] Wrote:\", OUT_CSV)\n",
    "print(f\"[INFO] J and rho computed on the SHARED feature universe at top-{PCT_FOR_TABLE}% per subspace.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4822c1-e967-4222-bbce-302cba16d652",
   "metadata": {},
   "source": [
    "# **Percentile Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85cd3f4b-2cdb-4d79-8171-df3a69f6b6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/Fig9_percentile_concordance_DDR4_WIN512_K3.png (best%=10 | M=dC_aJ | SHAP_BESTPLAT_full_DDR4_DROOP_WIN512_KF3_PCT10_MdC_aJ.csv)\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/Fig9_percentile_concordance_DDR5_WIN1024_K5.png (SHAP_BESTPLAT_full_DDR5_DROOP_WIN1024_KF5_PCT20_MdC_aJ.csv)\n"
     ]
    }
   ],
   "source": [
    "# === Fig. 9 style (PER PLATFORM): Percentile-rank concordance (CP–MI vs SHAP) ===\n",
    "# You requested: DDR5 (Platform) WIN=1024 and K=5 ✅\n",
    "#\n",
    "# CONSISTENT with revised Table 6:\n",
    "#   - same _norm_name()\n",
    "#   - restrict to SHARED feature universe per subspace (intersection)\n",
    "#   - percentile ranks computed within the shared universe\n",
    "#\n",
    "# UPDATED (visual quality):\n",
    "#   - legend moved OUTSIDE (never blocks points)\n",
    "#   - smaller points + semi-transparency + white edges (better overlap readability)\n",
    "#   - diagonal behind points\n",
    "#   - Spearman ρ shown as in-axes annotation (cleaner than a large title)\n",
    "#   - optional TOPK per subspace to reduce clutter (default None = keep all)\n",
    "#\n",
    "# Inputs:\n",
    "#   - Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#   - FeatureRankOUT/<setup>_<win>_<kfold>_0_{compute|memory|sensors}.csv\n",
    "#   - Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN<w>_KF<k>_PCT<p>_M<method>.csv\n",
    "#\n",
    "# Outputs:\n",
    "#   - Results/Explainability_SHAP_BestPlatforms/Fig9_percentile_concordance_DDR4_WIN512_K3.png\n",
    "#   - Results/Explainability_SHAP_BestPlatforms/Fig9_percentile_concordance_DDR5_WIN1024_K5.png\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# -------------------- Paths (robust) --------------------\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES_DIR_RAW = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "RES_DIR = RES_DIR_RAW.parent if RES_DIR_RAW.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\") else RES_DIR_RAW\n",
    "\n",
    "OUT_DIR = RES_DIR / \"Explainability_SHAP_BestPlatforms\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DETAILS_CSV = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "assert DETAILS_CSV.exists(), f\"Missing: {DETAILS_CSV}\"\n",
    "\n",
    "# Rank dirs (CP–MI)\n",
    "RANK_DIRS = list(globals().get(\"RANK_DIRS\", [\n",
    "    ROOT / \"FeatureRankOUT\",\n",
    "    Path(\"/Volumes/Untitled\") / \"FeatureRankOUT\",\n",
    "    Path(\"/Volumes/Untitled\") / \"octaneX\" / \"FeatureRankOUT\",\n",
    "    Path.home() / \"Desktop\" / \"octaneX\" / \"FeatureRankOUT\",\n",
    "]))\n",
    "\n",
    "# SHAP dir (same as output dir by your naming)\n",
    "SHAP_DIR = RES_DIR / \"Explainability_SHAP_BestPlatforms\"\n",
    "assert SHAP_DIR.exists(), f\"Missing SHAP dir: {SHAP_DIR}\"\n",
    "\n",
    "SUBSPACES = (\"compute\", \"memory\", \"sensors\")\n",
    "\n",
    "# -------------------- Case configs (forced to your request) --------------------\n",
    "SETUP_A = dict(setup=\"DDR4\", anomaly=\"DROOP\",   win=512,  kfold=3, tag=\"DDR4_WIN512_K3\")\n",
    "SETUP_B = dict(setup=\"DDR5\", anomaly=\"DROOP\",   win=1024, kfold=5, tag=\"DDR5_WIN1024_K5\")\n",
    "# If Fig.9 DDR5 should be SPECTRE instead of DROOP:\n",
    "# SETUP_B[\"anomaly\"] = \"SPECTRE\"\n",
    "\n",
    "# -------------------- Plot controls --------------------\n",
    "# If DDR5 looks too cluttered, set TOPK_PER_SUBSPACE to 25 (or 20/30).\n",
    "# None = keep all common features.\n",
    "TOPK_PER_SUBSPACE: Optional[int] = 25  # e.g., 25\n",
    "\n",
    "# -------------------- Shared helpers (match Table 6) --------------------\n",
    "def _norm_name(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \"_\", str(s)).lower().replace(\"%\", \"pct\")\n",
    "    s = re.sub(r\"[^a-z0-9_]+\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "def _find_rank_file(setup: str, win: int, kfold: int, sub: str) -> Optional[Path]:\n",
    "    fname = f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "    for d in RANK_DIRS:\n",
    "        p = Path(d) / fname\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # fallback: recursive search\n",
    "    for d in RANK_DIRS:\n",
    "        d = Path(d)\n",
    "        if d.exists():\n",
    "            hits = list(d.rglob(fname))\n",
    "            if hits:\n",
    "                return hits[0]\n",
    "    return None\n",
    "\n",
    "def _read_cpmi_rank_list(setup: str, win: int, kfold: int, sub: str) -> list[str]:\n",
    "    p = _find_rank_file(setup, win, kfold, sub)\n",
    "    if p is None:\n",
    "        return []\n",
    "    df = pd.read_csv(p)\n",
    "    if df.empty:\n",
    "        return []\n",
    "    col = \"feature\" if \"feature\" in df.columns else df.columns[0]\n",
    "    return [_norm_name(x) for x in df[col].astype(str).tolist()]\n",
    "\n",
    "# ---- details: best pct + method (if present for this win/kfold) ----\n",
    "DETAILS = pd.read_csv(DETAILS_CSV).copy()\n",
    "DETAILS.columns = [c.lower() for c in DETAILS.columns]\n",
    "if \"best_method\" in DETAILS.columns and \"method\" not in DETAILS.columns:\n",
    "    DETAILS = DETAILS.rename(columns={\"best_method\": \"method\"})\n",
    "if \"best_pct_by_median\" not in DETAILS.columns and \"pct\" in DETAILS.columns:\n",
    "    DETAILS = DETAILS.rename(columns={\"pct\": \"best_pct_by_median\"})\n",
    "\n",
    "def _best_pct_method_from_details(setup: str, anomaly: str, win: int, kfold: int) -> Optional[Tuple[int, str]]:\n",
    "    sub = DETAILS[\n",
    "        (DETAILS[\"setup\"].astype(str).str.upper() == setup.upper()) &\n",
    "        (DETAILS[\"anomaly\"].astype(str).str.upper() == anomaly.upper()) &\n",
    "        (pd.to_numeric(DETAILS[\"win\"], errors=\"coerce\") == int(win)) &\n",
    "        (pd.to_numeric(DETAILS[\"kfold\"], errors=\"coerce\") == int(kfold))\n",
    "    ].copy()\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    r = sub.iloc[0]\n",
    "    pct = int(pd.to_numeric(r[\"best_pct_by_median\"], errors=\"coerce\"))\n",
    "    method = str(r[\"method\"]).strip()\n",
    "    return pct, method\n",
    "\n",
    "def _find_shap_file(setup: str, anomaly: str, win: int, kfold: int,\n",
    "                    pct: Optional[int], method: Optional[str]) -> Optional[Path]:\n",
    "    # 1) exact (best if pct+method known)\n",
    "    if pct is not None and method is not None:\n",
    "        exact = SHAP_DIR / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT{pct}_M{method}.csv\"\n",
    "        if exact.exists():\n",
    "            return exact\n",
    "        hits = sorted(SHAP_DIR.glob(\n",
    "            f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT*_M{method}.csv\"\n",
    "        ))\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    # 2) fallback: any method/pct for that (setup, anomaly, win, kfold)\n",
    "    hits2 = sorted(SHAP_DIR.glob(\n",
    "        f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT*_M*.csv\"\n",
    "    ))\n",
    "    return hits2[0] if hits2 else None\n",
    "\n",
    "def _load_shap_full(setup: str, anomaly: str, win: int, kfold: int) -> Tuple[pd.DataFrame, Optional[int], Optional[str], Optional[Path]]:\n",
    "    pm = _best_pct_method_from_details(setup, anomaly, win, kfold)\n",
    "    pct, method = (pm if pm is not None else (None, None))\n",
    "    p = _find_shap_file(setup, anomaly, win, kfold, pct, method)\n",
    "    if p is None or not p.exists():\n",
    "        return pd.DataFrame(), pct, method, None\n",
    "\n",
    "    df = pd.read_csv(p)\n",
    "    if df.empty:\n",
    "        return df, pct, method, p\n",
    "\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    if \"shap_mean_abs\" not in df.columns and \"importance\" in df.columns:\n",
    "        df = df.rename(columns={\"importance\": \"shap_mean_abs\"})\n",
    "\n",
    "    need = {\"feature\", \"subspace\", \"shap_mean_abs\"}\n",
    "    if not need.issubset(set(df.columns)):\n",
    "        return pd.DataFrame(), pct, method, p\n",
    "\n",
    "    df[\"feature\"] = df[\"feature\"].astype(str).map(_norm_name)\n",
    "    df[\"subspace\"] = df[\"subspace\"].astype(str).str.lower()\n",
    "    df[\"shap_mean_abs\"] = pd.to_numeric(df[\"shap_mean_abs\"], errors=\"coerce\").fillna(0.0)\n",
    "    return df, pct, method, p\n",
    "\n",
    "def _restrict_to_common(order: list[str], common: set[str]) -> list[str]:\n",
    "    return [f for f in order if f in common]\n",
    "\n",
    "def _percentile_rank_from_order(features_in_order: list[str]) -> dict[str, float]:\n",
    "    n = len(features_in_order)\n",
    "    if n <= 1:\n",
    "        return {features_in_order[0]: 1.0} if n == 1 else {}\n",
    "    # best=1, worst=0\n",
    "    return {f: (1.0 - (i / (n - 1))) for i, f in enumerate(features_in_order)}\n",
    "\n",
    "# -------------------- Plot function --------------------\n",
    "def plot_percentile_concordance_one(setup: str, anomaly: str, win: int, kfold: int, out_png: Path):\n",
    "    shap, pct_best, method_best, shap_path = _load_shap_full(setup, anomaly, win, kfold)\n",
    "    if shap.empty:\n",
    "        print(f\"[SKIP] Missing SHAP full for {setup}/{anomaly} WIN={win} KF={kfold}\")\n",
    "        return\n",
    "\n",
    "    pts = []\n",
    "    for sub in SUBSPACES:\n",
    "        cp_order = _read_cpmi_rank_list(setup, win, kfold, sub)\n",
    "        sh_sub = shap[shap[\"subspace\"] == sub][[\"feature\", \"shap_mean_abs\"]].copy()\n",
    "        if (not cp_order) or sh_sub.empty:\n",
    "            continue\n",
    "\n",
    "        # SHAP order (desc)\n",
    "        sh_order = sh_sub.sort_values(\"shap_mean_abs\", ascending=False)[\"feature\"].tolist()\n",
    "\n",
    "        # shared universe\n",
    "        common = set(cp_order) & set(sh_order)\n",
    "        if len(common) < 3:\n",
    "            continue\n",
    "\n",
    "        # preserve each method's order, restricted to common\n",
    "        cp_c = _restrict_to_common(cp_order, common)\n",
    "        sh_c = _restrict_to_common(sh_order, common)\n",
    "\n",
    "        # OPTIONAL: reduce clutter by plotting only top-K from each list, then re-intersect\n",
    "        if TOPK_PER_SUBSPACE is not None:\n",
    "            cp_c = cp_c[:int(TOPK_PER_SUBSPACE)]\n",
    "            sh_c = sh_c[:int(TOPK_PER_SUBSPACE)]\n",
    "            common = set(cp_c) & set(sh_c)\n",
    "            if len(common) < 3:\n",
    "                continue\n",
    "            cp_c = _restrict_to_common(cp_c, common)\n",
    "            sh_c = _restrict_to_common(sh_c, common)\n",
    "\n",
    "        # percentile ranks within shared universe\n",
    "        cp_pr = _percentile_rank_from_order(cp_c)\n",
    "        sh_pr = _percentile_rank_from_order(sh_c)\n",
    "\n",
    "        for f in common:\n",
    "            pts.append({\"subspace\": sub, \"x\": float(cp_pr.get(f, 0.0)), \"y\": float(sh_pr.get(f, 0.0))})\n",
    "\n",
    "    if not pts:\n",
    "        print(f\"[SKIP] No aligned features for {setup}/{anomaly} WIN={win} KF={kfold}\")\n",
    "        return\n",
    "\n",
    "    P = pd.DataFrame(pts)\n",
    "    rho, _ = spearmanr(P[\"x\"].to_numpy(dtype=float), P[\"y\"].to_numpy(dtype=float))\n",
    "    rho = float(rho) if np.isfinite(rho) else np.nan\n",
    "\n",
    "    # --- Style (paper-ready) ---\n",
    "    fig, ax = plt.subplots(figsize=(7.6, 4.3), dpi=160)\n",
    "\n",
    "    colors  = {\"compute\": \"tab:orange\", \"memory\": \"tab:blue\", \"sensors\": \"tab:green\"}\n",
    "    markers = {\"compute\": \"o\",          \"memory\": \"s\",        \"sensors\": \"^\"}\n",
    "    labels  = {\"compute\": \"Compute\",    \"memory\": \"Memory\",  \"sensors\": \"Sensors\"}\n",
    "\n",
    "    # diagonal behind points\n",
    "    ax.plot([0, 1], [0, 1], \"--\", color=\"gray\", linewidth=1.4, label=\"y = x\", zorder=1)\n",
    "\n",
    "    # points\n",
    "    for sub in [\"compute\", \"memory\", \"sensors\"]:\n",
    "        Q = P[P[\"subspace\"] == sub]\n",
    "        if Q.empty:\n",
    "            continue\n",
    "        ax.scatter(\n",
    "            Q[\"x\"], Q[\"y\"],\n",
    "            s=26, alpha=0.70,\n",
    "            marker=markers[sub],\n",
    "            color=colors[sub],\n",
    "            label=labels[sub],\n",
    "            edgecolors=\"white\", linewidths=0.4,\n",
    "            zorder=3\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "    ax.set_xlabel(\"CP-MI percentile rank\", fontsize=16)\n",
    "    ax.set_ylabel(\"SHAP percentile rank\", fontsize=16)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.8, alpha=0.30)\n",
    "\n",
    "    ax.tick_params(labelsize=11)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # Spearman text (cleaner than big title)\n",
    "    ax.text(\n",
    "        0.02, 0.98, f\"Spearman ρ = {rho:.2f}\",\n",
    "        transform=ax.transAxes, ha=\"left\", va=\"top\", fontsize=14\n",
    "    )\n",
    "\n",
    "    # Legend OUTSIDE (never blocks data)\n",
    "    ax.legend(\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        frameon=False,\n",
    "        fontsize=12,\n",
    "        borderaxespad=0.0\n",
    "    )\n",
    "\n",
    "    extra = []\n",
    "    if pct_best is not None: extra.append(f\"best%={pct_best}\")\n",
    "    if method_best is not None: extra.append(f\"M={method_best}\")\n",
    "    if shap_path is not None: extra.append(shap_path.name)\n",
    "    extra_txt = \" | \".join(extra)\n",
    "\n",
    "    # leave room for legend on right\n",
    "    fig.tight_layout(rect=[0, 0, 0.82, 1])\n",
    "\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_png, dpi=220, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(\"[WROTE]\", out_png, f\"({extra_txt})\")\n",
    "\n",
    "# -------------------- Generate Setup A and Setup B --------------------\n",
    "outA = OUT_DIR / f\"Fig9_percentile_concordance_{SETUP_A['tag']}.png\"\n",
    "outB = OUT_DIR / f\"Fig9_percentile_concordance_{SETUP_B['tag']}.png\"\n",
    "\n",
    "plot_percentile_concordance_one(SETUP_A[\"setup\"], SETUP_A[\"anomaly\"], SETUP_A[\"win\"], SETUP_A[\"kfold\"], outA)\n",
    "plot_percentile_concordance_one(SETUP_B[\"setup\"], SETUP_B[\"anomaly\"], SETUP_B[\"win\"], SETUP_B[\"kfold\"], outB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9282526-e218-4cb8-8d03-39e9adcfcbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/Fig9_percentile_concordance_DDR4_WIN512_K3.png (best%=10 | M=dC_aJ | SHAP_BESTPLAT_full_DDR4_DROOP_WIN512_KF3_PCT10_MdC_aJ.csv)\n",
      "[WROTE] /Users/hsiaopingni/octaneX_v7_4functions/Results/Explainability_SHAP_BestPlatforms/Fig9_percentile_concordance_DDR5_WIN1024_K5.png (SHAP_BESTPLAT_full_DDR5_DROOP_WIN1024_KF5_PCT20_MdC_aJ.csv)\n"
     ]
    }
   ],
   "source": [
    "# === Fig. 9 style (PER PLATFORM): Percentile-rank concordance (CP–MI vs SHAP) ===\n",
    "# You requested: DDR5 (Platform) WIN=1024 and K=5 ✅\n",
    "#\n",
    "# CONSISTENT with revised Table 6:\n",
    "#   - same _norm_name()\n",
    "#   - restrict to SHARED feature universe per subspace (intersection)\n",
    "#   - percentile ranks computed within the shared universe\n",
    "#\n",
    "# UPDATED (visual quality):\n",
    "#   - legend moved OUTSIDE (never blocks points)\n",
    "#   - smaller points + semi-transparency + white edges (better overlap readability)\n",
    "#   - diagonal behind points\n",
    "#   - (REMOVED) Spearman ρ text annotation in-axes (per your request)\n",
    "#   - optional TOPK per subspace to reduce clutter (default 25; set None to keep all)\n",
    "#\n",
    "# Inputs:\n",
    "#   - Results/BEST_in_DesignSpace_Post_per_platform_details.csv\n",
    "#   - FeatureRankOUT/<setup>_<win>_<kfold>_0_{compute|memory|sensors}.csv\n",
    "#   - Results/Explainability_SHAP_BestPlatforms/SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN<w>_KF<k>_PCT<p>_M<method>.csv\n",
    "#\n",
    "# Outputs:\n",
    "#   - Results/Explainability_SHAP_BestPlatforms/Fig9_percentile_concordance_DDR4_WIN512_K3.png\n",
    "#   - Results/Explainability_SHAP_BestPlatforms/Fig9_percentile_concordance_DDR5_WIN1024_K5.png\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# -------------------- Paths (robust) --------------------\n",
    "ROOT = Path(globals().get(\"ROOT\", \"/Users/hsiaopingni/octaneX_v7_4functions\"))\n",
    "RES_DIR_RAW = Path(globals().get(\"RES_DIR\", ROOT / \"Results\"))\n",
    "RES_DIR = RES_DIR_RAW.parent if RES_DIR_RAW.name in (\"Explainability_SHAP_BestCases\", \"Explainability_SHAP_BestPlatforms\") else RES_DIR_RAW\n",
    "\n",
    "OUT_DIR = RES_DIR / \"Explainability_SHAP_BestPlatforms\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DETAILS_CSV = RES_DIR / \"BEST_in_DesignSpace_Post_per_platform_details.csv\"\n",
    "assert DETAILS_CSV.exists(), f\"Missing: {DETAILS_CSV}\"\n",
    "\n",
    "# Rank dirs (CP–MI)\n",
    "RANK_DIRS = list(globals().get(\"RANK_DIRS\", [\n",
    "    ROOT / \"FeatureRankOUT\",\n",
    "    Path(\"/Volumes/Untitled\") / \"FeatureRankOUT\",\n",
    "    Path(\"/Volumes/Untitled\") / \"octaneX\" / \"FeatureRankOUT\",\n",
    "    Path.home() / \"Desktop\" / \"octaneX\" / \"FeatureRankOUT\",\n",
    "]))\n",
    "\n",
    "# SHAP dir (same as output dir by your naming)\n",
    "SHAP_DIR = RES_DIR / \"Explainability_SHAP_BestPlatforms\"\n",
    "assert SHAP_DIR.exists(), f\"Missing SHAP dir: {SHAP_DIR}\"\n",
    "\n",
    "SUBSPACES = (\"compute\", \"memory\", \"sensors\")\n",
    "\n",
    "# -------------------- Case configs (forced to your request) --------------------\n",
    "SETUP_A = dict(setup=\"DDR4\", anomaly=\"DROOP\",   win=512,  kfold=3, tag=\"DDR4_WIN512_K3\")\n",
    "SETUP_B = dict(setup=\"DDR5\", anomaly=\"DROOP\",   win=1024, kfold=5, tag=\"DDR5_WIN1024_K5\")\n",
    "# If Fig.9 DDR5 should be SPECTRE instead of DROOP:\n",
    "# SETUP_B[\"anomaly\"] = \"SPECTRE\"\n",
    "\n",
    "# -------------------- Plot controls --------------------\n",
    "# If DDR5 looks too cluttered, set TOPK_PER_SUBSPACE to 25 (or 20/30).\n",
    "# None = keep all common features.\n",
    "TOPK_PER_SUBSPACE: Optional[int] = 25  # e.g., 25\n",
    "\n",
    "# -------------------- Shared helpers (match Table 6) --------------------\n",
    "def _norm_name(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \"_\", str(s)).lower().replace(\"%\", \"pct\")\n",
    "    s = re.sub(r\"[^a-z0-9_]+\", \"_\", s)\n",
    "    return re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "def _find_rank_file(setup: str, win: int, kfold: int, sub: str) -> Optional[Path]:\n",
    "    fname = f\"{setup}_{win}_{kfold}_0_{sub}.csv\"\n",
    "    for d in RANK_DIRS:\n",
    "        p = Path(d) / fname\n",
    "        if p.exists():\n",
    "            return p\n",
    "    # fallback: recursive search\n",
    "    for d in RANK_DIRS:\n",
    "        d = Path(d)\n",
    "        if d.exists():\n",
    "            hits = list(d.rglob(fname))\n",
    "            if hits:\n",
    "                return hits[0]\n",
    "    return None\n",
    "\n",
    "def _read_cpmi_rank_list(setup: str, win: int, kfold: int, sub: str) -> list[str]:\n",
    "    p = _find_rank_file(setup, win, kfold, sub)\n",
    "    if p is None:\n",
    "        return []\n",
    "    df = pd.read_csv(p)\n",
    "    if df.empty:\n",
    "        return []\n",
    "    col = \"feature\" if \"feature\" in df.columns else df.columns[0]\n",
    "    return [_norm_name(x) for x in df[col].astype(str).tolist()]\n",
    "\n",
    "# ---- details: best pct + method (if present for this win/kfold) ----\n",
    "DETAILS = pd.read_csv(DETAILS_CSV).copy()\n",
    "DETAILS.columns = [c.lower() for c in DETAILS.columns]\n",
    "if \"best_method\" in DETAILS.columns and \"method\" not in DETAILS.columns:\n",
    "    DETAILS = DETAILS.rename(columns={\"best_method\": \"method\"})\n",
    "if \"best_pct_by_median\" not in DETAILS.columns and \"pct\" in DETAILS.columns:\n",
    "    DETAILS = DETAILS.rename(columns={\"pct\": \"best_pct_by_median\"})\n",
    "\n",
    "def _best_pct_method_from_details(setup: str, anomaly: str, win: int, kfold: int) -> Optional[Tuple[int, str]]:\n",
    "    sub = DETAILS[\n",
    "        (DETAILS[\"setup\"].astype(str).str.upper() == setup.upper()) &\n",
    "        (DETAILS[\"anomaly\"].astype(str).str.upper() == anomaly.upper()) &\n",
    "        (pd.to_numeric(DETAILS[\"win\"], errors=\"coerce\") == int(win)) &\n",
    "        (pd.to_numeric(DETAILS[\"kfold\"], errors=\"coerce\") == int(kfold))\n",
    "    ].copy()\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    r = sub.iloc[0]\n",
    "    pct = int(pd.to_numeric(r[\"best_pct_by_median\"], errors=\"coerce\"))\n",
    "    method = str(r[\"method\"]).strip()\n",
    "    return pct, method\n",
    "\n",
    "def _find_shap_file(setup: str, anomaly: str, win: int, kfold: int,\n",
    "                    pct: Optional[int], method: Optional[str]) -> Optional[Path]:\n",
    "    # 1) exact (best if pct+method known)\n",
    "    if pct is not None and method is not None:\n",
    "        exact = SHAP_DIR / f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT{pct}_M{method}.csv\"\n",
    "        if exact.exists():\n",
    "            return exact\n",
    "        hits = sorted(SHAP_DIR.glob(\n",
    "            f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT*_M{method}.csv\"\n",
    "        ))\n",
    "        if hits:\n",
    "            return hits[0]\n",
    "    # 2) fallback: any method/pct for that (setup, anomaly, win, kfold)\n",
    "    hits2 = sorted(SHAP_DIR.glob(\n",
    "        f\"SHAP_BESTPLAT_full_{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT*_M*.csv\"\n",
    "    ))\n",
    "    return hits2[0] if hits2 else None\n",
    "\n",
    "def _load_shap_full(setup: str, anomaly: str, win: int, kfold: int) -> Tuple[pd.DataFrame, Optional[int], Optional[str], Optional[Path]]:\n",
    "    pm = _best_pct_method_from_details(setup, anomaly, win, kfold)\n",
    "    pct, method = (pm if pm is not None else (None, None))\n",
    "    p = _find_shap_file(setup, anomaly, win, kfold, pct, method)\n",
    "    if p is None or not p.exists():\n",
    "        return pd.DataFrame(), pct, method, None\n",
    "\n",
    "    df = pd.read_csv(p)\n",
    "    if df.empty:\n",
    "        return df, pct, method, p\n",
    "\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "    if \"shap_mean_abs\" not in df.columns and \"importance\" in df.columns:\n",
    "        df = df.rename(columns={\"importance\": \"shap_mean_abs\"})\n",
    "\n",
    "    need = {\"feature\", \"subspace\", \"shap_mean_abs\"}\n",
    "    if not need.issubset(set(df.columns)):\n",
    "        return pd.DataFrame(), pct, method, p\n",
    "\n",
    "    df[\"feature\"] = df[\"feature\"].astype(str).map(_norm_name)\n",
    "    df[\"subspace\"] = df[\"subspace\"].astype(str).str.lower()\n",
    "    df[\"shap_mean_abs\"] = pd.to_numeric(df[\"shap_mean_abs\"], errors=\"coerce\").fillna(0.0)\n",
    "    return df, pct, method, p\n",
    "\n",
    "def _restrict_to_common(order: list[str], common: set[str]) -> list[str]:\n",
    "    return [f for f in order if f in common]\n",
    "\n",
    "def _percentile_rank_from_order(features_in_order: list[str]) -> dict[str, float]:\n",
    "    n = len(features_in_order)\n",
    "    if n <= 1:\n",
    "        return {features_in_order[0]: 1.0} if n == 1 else {}\n",
    "    # best=1, worst=0\n",
    "    return {f: (1.0 - (i / (n - 1))) for i, f in enumerate(features_in_order)}\n",
    "\n",
    "# -------------------- Plot function --------------------\n",
    "def plot_percentile_concordance_one(setup: str, anomaly: str, win: int, kfold: int, out_png: Path):\n",
    "    shap, pct_best, method_best, shap_path = _load_shap_full(setup, anomaly, win, kfold)\n",
    "    if shap.empty:\n",
    "        print(f\"[SKIP] Missing SHAP full for {setup}/{anomaly} WIN={win} KF={kfold}\")\n",
    "        return\n",
    "\n",
    "    pts = []\n",
    "    for sub in SUBSPACES:\n",
    "        cp_order = _read_cpmi_rank_list(setup, win, kfold, sub)\n",
    "        sh_sub = shap[shap[\"subspace\"] == sub][[\"feature\", \"shap_mean_abs\"]].copy()\n",
    "        if (not cp_order) or sh_sub.empty:\n",
    "            continue\n",
    "\n",
    "        # SHAP order (desc)\n",
    "        sh_order = sh_sub.sort_values(\"shap_mean_abs\", ascending=False)[\"feature\"].tolist()\n",
    "\n",
    "        # shared universe\n",
    "        common = set(cp_order) & set(sh_order)\n",
    "        if len(common) < 3:\n",
    "            continue\n",
    "\n",
    "        # preserve each method's order, restricted to common\n",
    "        cp_c = _restrict_to_common(cp_order, common)\n",
    "        sh_c = _restrict_to_common(sh_order, common)\n",
    "\n",
    "        # OPTIONAL: reduce clutter by plotting only top-K from each list, then re-intersect\n",
    "        if TOPK_PER_SUBSPACE is not None:\n",
    "            cp_c = cp_c[:int(TOPK_PER_SUBSPACE)]\n",
    "            sh_c = sh_c[:int(TOPK_PER_SUBSPACE)]\n",
    "            common = set(cp_c) & set(sh_c)\n",
    "            if len(common) < 3:\n",
    "                continue\n",
    "            cp_c = _restrict_to_common(cp_c, common)\n",
    "            sh_c = _restrict_to_common(sh_c, common)\n",
    "\n",
    "        # percentile ranks within shared universe\n",
    "        cp_pr = _percentile_rank_from_order(cp_c)\n",
    "        sh_pr = _percentile_rank_from_order(sh_c)\n",
    "\n",
    "        for f in common:\n",
    "            pts.append({\"subspace\": sub, \"x\": float(cp_pr.get(f, 0.0)), \"y\": float(sh_pr.get(f, 0.0))})\n",
    "\n",
    "    if not pts:\n",
    "        print(f\"[SKIP] No aligned features for {setup}/{anomaly} WIN={win} KF={kfold}\")\n",
    "        return\n",
    "\n",
    "    P = pd.DataFrame(pts)\n",
    "    # (still computed if you want it later, but NOT displayed on plot)\n",
    "    rho, _ = spearmanr(P[\"x\"].to_numpy(dtype=float), P[\"y\"].to_numpy(dtype=float))\n",
    "    rho = float(rho) if np.isfinite(rho) else np.nan\n",
    "\n",
    "    # --- Style (paper-ready) ---\n",
    "    fig, ax = plt.subplots(figsize=(7.6, 4.3), dpi=160)\n",
    "\n",
    "    colors  = {\"compute\": \"tab:orange\", \"memory\": \"tab:blue\", \"sensors\": \"tab:green\"}\n",
    "    markers = {\"compute\": \"o\",          \"memory\": \"s\",        \"sensors\": \"^\"}\n",
    "    labels  = {\"compute\": \"Compute\",    \"memory\": \"Memory\",  \"sensors\": \"Sensors\"}\n",
    "\n",
    "    # diagonal behind points\n",
    "    ax.plot([0, 1], [0, 1], \"--\", color=\"gray\", linewidth=1.4, label=\"y = x\", zorder=1)\n",
    "\n",
    "    # points\n",
    "    for sub in [\"compute\", \"memory\", \"sensors\"]:\n",
    "        Q = P[P[\"subspace\"] == sub]\n",
    "        if Q.empty:\n",
    "            continue\n",
    "        ax.scatter(\n",
    "            Q[\"x\"], Q[\"y\"],\n",
    "            s=26, alpha=0.70,\n",
    "            marker=markers[sub],\n",
    "            color=colors[sub],\n",
    "            label=labels[sub],\n",
    "            edgecolors=\"white\", linewidths=0.4,\n",
    "            zorder=3\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "    ax.set_xlabel(\"CP-MI percentile rank\", fontsize=16)\n",
    "    ax.set_ylabel(\"SHAP percentile rank\", fontsize=16)\n",
    "    ax.grid(True, linestyle=\"--\", linewidth=0.8, alpha=0.30)\n",
    "\n",
    "    ax.tick_params(labelsize=11)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # (REMOVED) Spearman text annotation per your request\n",
    "    # ax.text(0.02, 0.98, f\"Spearman ρ = {rho:.2f}\", transform=ax.transAxes, ha=\"left\", va=\"top\", fontsize=14)\n",
    "\n",
    "    # Legend OUTSIDE (never blocks data)\n",
    "    ax.legend(\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.02, 0.5),\n",
    "        frameon=False,\n",
    "        fontsize=12,\n",
    "        borderaxespad=0.0\n",
    "    )\n",
    "\n",
    "    extra = []\n",
    "    if pct_best is not None: extra.append(f\"best%={pct_best}\")\n",
    "    if method_best is not None: extra.append(f\"M={method_best}\")\n",
    "    if shap_path is not None: extra.append(shap_path.name)\n",
    "    extra_txt = \" | \".join(extra)\n",
    "\n",
    "    # leave room for legend on right\n",
    "    fig.tight_layout(rect=[0, 0, 0.82, 1])\n",
    "\n",
    "    out_png.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_png, dpi=220, bbox_inches=\"tight\", pad_inches=0.05)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(\"[WROTE]\", out_png, f\"({extra_txt})\")\n",
    "\n",
    "# -------------------- Generate Setup A and Setup B --------------------\n",
    "outA = OUT_DIR / f\"Fig9_percentile_concordance_{SETUP_A['tag']}.png\"\n",
    "outB = OUT_DIR / f\"Fig9_percentile_concordance_{SETUP_B['tag']}.png\"\n",
    "\n",
    "plot_percentile_concordance_one(SETUP_A[\"setup\"], SETUP_A[\"anomaly\"], SETUP_A[\"win\"], SETUP_A[\"kfold\"], outA)\n",
    "plot_percentile_concordance_one(SETUP_B[\"setup\"], SETUP_B[\"anomaly\"], SETUP_B[\"win\"], SETUP_B[\"kfold\"], outB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "812d5495-9e85-462d-b520-d6c807d8ce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[bp]\n",
      "    \\centering\n",
      "    \\caption{Median AUC--PR summary vs top $p$ features retained across setups.}\n",
      "    \\label{tab:aucpr_summary}\n",
      "    \\setlength{\\tabcolsep}{6pt}\n",
      "    \\renewcommand{\\arraystretch}{1.15}\n",
      "    \\footnotesize\n",
      "    \\begin{tabular}{||c|c|c|c|c|c||}\n",
      "        \\hline\n",
      "        \\multirow{2}{*}{\\textbf{Setup}} &\n",
      "        \\multirow{2}{*}{\\textbf{Anomaly}} &\n",
      "        \\multicolumn{4}{c||}{$p$} \\\\\n",
      "        \\cline{3-6}\n",
      "        & & \\textbf{10\\%} & \\textbf{30\\%} & \\textbf{70\\%} & \\textbf{100\\%} \\\\\n",
      "        \\hline\n",
      "        A & TRRespass & 1.000 & 1.000 & 1.000 & 1.000 \\\\\n",
      "        A & Droop & 1.000 & 1.000 & 0.996 & 1.000 \\\\\n",
      "        B & Droop & 0.978 & 0.959 & 0.975 & 0.973 \\\\\n",
      "        B & Spectre & 1.000 & 1.000 & 1.000 & 1.000 \\\\\n",
      "        \\hline\n",
      "    \\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "[OK] Saved LaTeX to: /Users/hsiaopingni/octaneX_v7_4functions/Results/aucpr_summary_table_from_perrun.tex\n"
     ]
    }
   ],
   "source": [
    "# === Build LaTeX Table: Median AUC-PR vs top-p (10/30/70/100) for specific (WIN,K) ===\n",
    "# Uses per-run CSV (source of truth):\n",
    "#   per_run_metrics_all_PIPELINE.csv\n",
    "#\n",
    "# Rule for each cell (setup, anomaly, win, kfold, pct):\n",
    "#   - compute auc_pr_median across run_id\n",
    "#   - if multiple methods exist, take the BEST (max) auc_pr_median across methods\n",
    "#\n",
    "# Targets (your request):\n",
    "#   - Setup A (DDR4): WIN=512,  K=3\n",
    "#   - Setup B (DDR5): WIN=1024, K=5\n",
    "#\n",
    "# Output:\n",
    "#   - prints LaTeX table ready to paste into Overleaf\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "TARGETS = {\n",
    "    \"A\": {\"setup\":\"DDR4\", \"win\":512,  \"kfold\":3, \"anomalies\":[\"RH\",\"DROOP\"]},       # RH -> TRRespass\n",
    "    \"B\": {\"setup\":\"DDR5\", \"win\":1024, \"kfold\":5, \"anomalies\":[\"SPECTRE\",\"DROOP\"]},\n",
    "}\n",
    "PCTS = [10, 30, 70, 100]\n",
    "\n",
    "def paper_anomaly_name(anom: str) -> str:\n",
    "    a = str(anom).strip().upper()\n",
    "    if a == \"RH\": return \"TRRespass\"\n",
    "    if a == \"DROOP\": return \"Droop\"\n",
    "    if a == \"SPECTRE\": return \"Spectre\"\n",
    "    return a.title()\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD per-run CSV (uploaded path first)\n",
    "# ----------------------------\n",
    "CANDS = [\n",
    "    Path(\"/Users/hsiaopingni/octaneX_v7_4functions/Results/per_run_metrics_all_PIPELINE.csv\"),\n",
    "    Path(\"/Users/hsiaopingni/octaneX_v7_4functions/Results/per_run_metrics_all_PIPELINE.csv\"),\n",
    "    Path(\"/Volumes/Untitled/octaneX_results/per_run_metrics_all_PIPELINE.csv\"),\n",
    "]\n",
    "PER_RUN = next((p for p in CANDS if p.exists()), None)\n",
    "if PER_RUN is None:\n",
    "    raise FileNotFoundError(\"Missing per_run_metrics_all_PIPELINE.csv. Looked for:\\n  - \" + \"\\n  - \".join(map(str, CANDS)))\n",
    "\n",
    "df = pd.read_csv(PER_RUN).copy()\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "need = {\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"run_id\",\"auc_pr\",\"method\"}\n",
    "missing = need - set(df.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"per_run_metrics_all_PIPELINE.csv missing columns: {sorted(missing)}. Have: {list(df.columns)}\")\n",
    "\n",
    "# Normalize types\n",
    "for c in [\"win\",\"kfold\",\"pct\",\"auc_pr\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df[\"setup\"] = df[\"setup\"].astype(str)\n",
    "df[\"anomaly\"] = df[\"anomaly\"].astype(str)\n",
    "df[\"method\"] = df[\"method\"].astype(str)\n",
    "df[\"run_id\"] = df[\"run_id\"].astype(str)\n",
    "\n",
    "df = df.dropna(subset=[\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"auc_pr\",\"method\"]).copy()\n",
    "df[\"auc_pr\"] = df[\"auc_pr\"].clip(0.0, 1.0)\n",
    "\n",
    "# ----------------------------\n",
    "# Compute per-(setup, anomaly, win, kfold, pct, method) median over run_id\n",
    "# then take max across method for each (setup, anomaly, win, kfold, pct)\n",
    "# ----------------------------\n",
    "med_by_method = (\n",
    "    df.groupby([\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"method\"], as_index=False)\n",
    "      .agg(auc_pr_median=(\"auc_pr\",\"median\"))\n",
    ")\n",
    "\n",
    "best_over_method = (\n",
    "    med_by_method.groupby([\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\"], as_index=False)\n",
    "                .agg(auc_pr_median=(\"auc_pr_median\",\"max\"))\n",
    ")\n",
    "\n",
    "# Helper: get cell value\n",
    "def get_aucpr_median(setup, anomaly, win, kfold, pct):\n",
    "    sub = best_over_method[\n",
    "        (best_over_method[\"setup\"].str.upper() == str(setup).upper()) &\n",
    "        (best_over_method[\"anomaly\"].str.upper() == str(anomaly).upper()) &\n",
    "        (best_over_method[\"win\"] == int(win)) &\n",
    "        (best_over_method[\"kfold\"] == int(kfold)) &\n",
    "        (best_over_method[\"pct\"] == int(pct))\n",
    "    ]\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    return float(sub[\"auc_pr_median\"].iloc[0])\n",
    "\n",
    "# ----------------------------\n",
    "# Build table rows\n",
    "# ----------------------------\n",
    "rows = []\n",
    "for setup_label, cfg in TARGETS.items():\n",
    "    setup = cfg[\"setup\"]\n",
    "    win = cfg[\"win\"]\n",
    "    kfold = cfg[\"kfold\"]\n",
    "    for anom in cfg[\"anomalies\"]:\n",
    "        row = {\"Setup\": setup_label, \"Anomaly\": paper_anomaly_name(anom)}\n",
    "        for p in PCTS:\n",
    "            v = get_aucpr_median(setup, anom, win, kfold, p)\n",
    "            row[f\"{p}%\"] = f\"{v:.3f}\" if v is not None and np.isfinite(v) else \"NA\"\n",
    "        rows.append(row)\n",
    "\n",
    "tab = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: order anomalies as your paper example\n",
    "order = [\"TRRespass\",\"Droop\",\"Spectre\"]\n",
    "tab[\"__ord\"] = tab[\"Anomaly\"].map(lambda x: order.index(x) if x in order else 999)\n",
    "tab = tab.sort_values([\"Setup\",\"__ord\",\"Anomaly\"]).drop(columns=[\"__ord\"]).reset_index(drop=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Emit LaTeX (paste into Overleaf)\n",
    "# ----------------------------\n",
    "latex_lines = []\n",
    "latex_lines.append(r\"\\begin{table}[bp]\")\n",
    "latex_lines.append(r\"    \\centering\")\n",
    "latex_lines.append(r\"    \\caption{Median AUC--PR summary vs top $p$ features retained across setups.}\")\n",
    "latex_lines.append(r\"    \\label{tab:aucpr_summary}\")\n",
    "latex_lines.append(r\"    \\setlength{\\tabcolsep}{6pt}\")\n",
    "latex_lines.append(r\"    \\renewcommand{\\arraystretch}{1.15}\")\n",
    "latex_lines.append(r\"    \\footnotesize\")\n",
    "latex_lines.append(r\"    \\begin{tabular}{||c|c|c|c|c|c||}\")\n",
    "latex_lines.append(r\"        \\hline\")\n",
    "latex_lines.append(r\"        \\multirow{2}{*}{\\textbf{Setup}} &\")\n",
    "latex_lines.append(r\"        \\multirow{2}{*}{\\textbf{Anomaly}} &\")\n",
    "latex_lines.append(r\"        \\multicolumn{4}{c||}{$p$} \\\\\")\n",
    "latex_lines.append(r\"        \\cline{3-6}\")\n",
    "latex_lines.append(r\"        & & \\textbf{10\\%} & \\textbf{30\\%} & \\textbf{70\\%} & \\textbf{100\\%} \\\\\")\n",
    "latex_lines.append(r\"        \\hline\")\n",
    "\n",
    "for _, r in tab.iterrows():\n",
    "    latex_lines.append(\n",
    "        f\"        {r['Setup']} & {r['Anomaly']} & {r['10%']} & {r['30%']} & {r['70%']} & {r['100%']} \\\\\\\\\"\n",
    "    )\n",
    "\n",
    "latex_lines.append(r\"        \\hline\")\n",
    "latex_lines.append(r\"    \\end{tabular}\")\n",
    "latex_lines.append(r\"\\end{table}\")\n",
    "\n",
    "latex = \"\\n\".join(latex_lines)\n",
    "print(latex)\n",
    "\n",
    "# Also save to a file in the current session (optional)\n",
    "out_tex = Path(\"/Users/hsiaopingni/octaneX_v7_4functions/Results/aucpr_summary_table_from_perrun.tex\")\n",
    "out_tex.write_text(latex)\n",
    "print(f\"\\n[OK] Saved LaTeX to: {out_tex}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "34f2a540-093e-427c-9a98-ad3e0b158972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saving outputs to: /Users/hsiaopingni/octaneX_v7_4functions/Results/Figures\n",
      "[OK] Using per-run CSV: /Users/hsiaopingni/octaneX_v7_4functions/Results/per_run_metrics_all_PIPELINE.csv\n",
      "\n",
      "[X-OCTANE (workload-ordered) selection]\n",
      "  DFT: AUC-PR=0.971168, ROC-AUC=0.966049\n",
      "  DJ: AUC-PR=0.958250, ROC-AUC=0.959877\n",
      "  DP: AUC-PR=1.000000, ROC-AUC=1.000000\n",
      "  GL: AUC-PR=0.907735, ROC-AUC=0.904321\n",
      "  GS: AUC-PR=1.000000, ROC-AUC=1.000000\n",
      "\n",
      "[CHECK vs baseline OCTANE arrays]\n",
      "  Fold1 (DFT): AUC-PR OK (0.971 vs 0.930), ROC OK (0.966 vs 0.900)\n",
      "  Fold2 (DJ): AUC-PR LOW (0.958 vs 0.960), ROC OK (0.960 vs 0.930)\n",
      "  Fold3 (DP): AUC-PR OK (1.000 vs 0.970), ROC OK (1.000 vs 0.950)\n",
      "  Fold4 (GL): AUC-PR LOW (0.908 vs 0.940), ROC LOW (0.904 vs 0.920)\n",
      "  Fold5 (GS): AUC-PR OK (1.000 vs 0.980), ROC OK (1.000 vs 0.940)\n",
      "\n",
      "=== Statistical Summary ===\n",
      "AUC-PR:\n",
      "  Mean gain: 0.0114\n",
      "  Wilcoxon p-value: 0.3125\n",
      "  Paired t-test p-value: 0.4292\n",
      "\n",
      "ROC-AUC:\n",
      "  Mean gain: 0.0380\n",
      "  Wilcoxon p-value: 0.0625\n",
      "  Paired t-test p-value: 0.0616\n",
      "\n",
      "Figures saved to:\n",
      "  /Users/hsiaopingni/octaneX_v7_4functions/Results/Figures/XOCTANE_vs_OCTANE_ttest_wilcoxon.png\n",
      "  /Users/hsiaopingni/octaneX_v7_4functions/Results/Figures/XOCTANE_vs_OCTANE_ttest_wilcoxon.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import wilcoxon, ttest_rel\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "EXT_DRIVE = Path(\"/Volumes/Untitled\")\n",
    "LOCAL_ROOT = Path(\"/Users/hsiaopingni/octaneX_v7_4functions\")\n",
    "ROOT = (EXT_DRIVE / \"octaneX\") if (EXT_DRIVE.exists() and (EXT_DRIVE / \"octaneX\").exists()) else LOCAL_ROOT\n",
    "\n",
    "OUT_DIR = ROOT / \"Results\" / \"Figures\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[OK] Saving outputs to: {OUT_DIR.resolve()}\")\n",
    "\n",
    "# Prefer uploaded file first (this session), then local paths\n",
    "CSV_CANDS = [\n",
    "    Path(\"/mnt/data/per_run_metrics_all_PIPELINE.csv\"),\n",
    "    ROOT / \"Results\" / \"per_run_metrics_all_PIPELINE.csv\",\n",
    "    Path(\"/Volumes/Untitled/octaneX_results/per_run_metrics_all_PIPELINE.csv\"),\n",
    "]\n",
    "PER_RUN = next((p for p in CSV_CANDS if p.exists()), None)\n",
    "if PER_RUN is None:\n",
    "    raise FileNotFoundError(\"Cannot find per_run_metrics_all_PIPELINE.csv. Tried:\\n  - \" + \"\\n  - \".join(map(str, CSV_CANDS)))\n",
    "print(f\"[OK] Using per-run CSV: {PER_RUN}\")\n",
    "\n",
    "# =========================\n",
    "# Baseline OCTANE arrays (unchanged)\n",
    "# =========================\n",
    "octane_aucpr = np.array([0.93, 0.96, 0.97, 0.94, 0.98])\n",
    "octane_roc   = np.array([0.90, 0.93, 0.95, 0.92, 0.94])\n",
    "\n",
    "# =========================\n",
    "# X-OCTANE selection (your exact request)\n",
    "# =========================\n",
    "XOCT_METHOD   = \"dE_aM\"   # your \"X-OCTANE\" proxy inside per_run_metrics_all_PIPELINE.csv\n",
    "TARGET_SETUP  = \"DDR5\"\n",
    "TARGET_ANOM   = \"DROOP\"\n",
    "TARGET_WIN    = 1024\n",
    "TARGET_KFOLD  = 5\n",
    "TARGET_PCT    = 80\n",
    "\n",
    "# workload order you use across the project\n",
    "WORKLOAD_ORDER = [\"DFT\",\"DJ\",\"DP\",\"GL\",\"GS\",\"HA\",\"JA\",\"MM\",\"NI\",\"OE\",\"PI\",\"SH\",\"TR\"]\n",
    "WL_RANK = {w:i for i,w in enumerate(WORKLOAD_ORDER)}\n",
    "\n",
    "# load only needed columns\n",
    "usecols = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"method\",\"workload\",\"auc_pr\",\"roc_auc\"]\n",
    "df = pd.read_csv(PER_RUN, usecols=usecols).copy()\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "for c in [\"win\",\"kfold\",\"pct\",\"auc_pr\",\"roc_auc\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df[\"setup\"]    = df[\"setup\"].astype(str).str.upper()\n",
    "df[\"anomaly\"]  = df[\"anomaly\"].astype(str).str.upper()\n",
    "df[\"method\"]   = df[\"method\"].astype(str)\n",
    "df[\"workload\"] = df[\"workload\"].astype(str).str.upper()\n",
    "\n",
    "mask = (\n",
    "    (df[\"setup\"] == TARGET_SETUP) &\n",
    "    (df[\"anomaly\"] == TARGET_ANOM) &\n",
    "    (df[\"win\"] == TARGET_WIN) &\n",
    "    (df[\"kfold\"] == TARGET_KFOLD) &\n",
    "    (df[\"pct\"] == TARGET_PCT) &\n",
    "    (df[\"method\"] == XOCT_METHOD)\n",
    ")\n",
    "\n",
    "dx = df[mask].dropna(subset=[\"auc_pr\",\"roc_auc\"]).copy()\n",
    "if dx.empty:\n",
    "    raise RuntimeError(\n",
    "        f\"No rows found for: {TARGET_SETUP} {TARGET_ANOM} WIN{TARGET_WIN} K{TARGET_KFOLD} PCT{TARGET_PCT} method={XOCT_METHOD}.\"\n",
    "    )\n",
    "\n",
    "# sort by workload order then take first 5\n",
    "dx[\"wl_ord\"] = dx[\"workload\"].map(WL_RANK).fillna(999).astype(int)\n",
    "dx = dx.sort_values([\"wl_ord\", \"workload\"]).reset_index(drop=True)\n",
    "\n",
    "dx5 = dx.head(5).copy()\n",
    "xoct_aucpr = np.clip(dx5[\"auc_pr\"].to_numpy(float), 0.0, 1.0)\n",
    "xoct_roc   = np.clip(dx5[\"roc_auc\"].to_numpy(float), 0.0, 1.0)\n",
    "xoct_wl    = dx5[\"workload\"].tolist()\n",
    "\n",
    "print(\"\\n[X-OCTANE (workload-ordered) selection]\")\n",
    "for wl, ap, rc in zip(xoct_wl, xoct_aucpr, xoct_roc):\n",
    "    print(f\"  {wl}: AUC-PR={ap:.6f}, ROC-AUC={rc:.6f}\")\n",
    "\n",
    "# Honest check vs baseline (cannot force these to be all above)\n",
    "print(\"\\n[CHECK vs baseline OCTANE arrays]\")\n",
    "for i, wl in enumerate(xoct_wl):\n",
    "    print(f\"  Fold{i+1} ({wl}): \"\n",
    "          f\"AUC-PR {'OK' if xoct_aucpr[i] >= octane_aucpr[i] else 'LOW'} \"\n",
    "          f\"({xoct_aucpr[i]:.3f} vs {octane_aucpr[i]:.3f}), \"\n",
    "          f\"ROC {'OK' if xoct_roc[i] >= octane_roc[i] else 'LOW'} \"\n",
    "          f\"({xoct_roc[i]:.3f} vs {octane_roc[i]:.3f})\")\n",
    "\n",
    "# =========================\n",
    "# Stats + plot (unchanged)\n",
    "# =========================\n",
    "def paired_stats(baseline, hybrid, label):\n",
    "    diff = hybrid - baseline\n",
    "    mean_gain = diff.mean()\n",
    "    # Wilcoxon can fail if all diffs are 0; handle safely\n",
    "    try:\n",
    "        w_stat, w_p = wilcoxon(hybrid, baseline, alternative='greater')\n",
    "    except Exception:\n",
    "        w_p = np.nan\n",
    "    try:\n",
    "        t_stat, t_p = ttest_rel(hybrid, baseline)\n",
    "    except Exception:\n",
    "        t_p = np.nan\n",
    "    return {'label': label, 'gain': mean_gain, 'wilcoxon_p': w_p, 't_p': t_p, 'diff': diff}\n",
    "\n",
    "res_aucpr = paired_stats(octane_aucpr, xoct_aucpr, 'AUC-PR')\n",
    "res_roc   = paired_stats(octane_roc, xoct_roc, 'ROC-AUC')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "for i, (metric, base, hybrid, res) in enumerate([\n",
    "    ('AUC-PR', octane_aucpr, xoct_aucpr, res_aucpr),\n",
    "    ('ROC-AUC', octane_roc, xoct_roc, res_roc)\n",
    "]):\n",
    "    x = np.arange(1, len(base) + 1)\n",
    "    ax[i].plot(x, base, 'o--', label='OCTANE', color='gray')\n",
    "    ax[i].plot(x, hybrid, 's-', label='X-OCTANE', color='tab:blue')\n",
    "    ax[i].axhline(np.mean(base), color='gray', linestyle=':', alpha=0.6)\n",
    "    ax[i].axhline(np.mean(hybrid), color='tab:blue', linestyle='--', alpha=0.7)\n",
    "    ax[i].set_xlabel('Cross-Validation Fold')\n",
    "    ax[i].set_ylabel(metric)\n",
    "    ax[i].set_ylim(0.85, 1.02)\n",
    "    ax[i].grid(alpha=0.3)\n",
    "    ax[i].set_title(\n",
    "        f\"{metric}\\nΔ={res['gain']:.3f} | \"\n",
    "        f\"p₍W₎={res['wilcoxon_p']:.3f}, p₍t₎={res['t_p']:.3f}\"\n",
    "    )\n",
    "ax[0].legend(loc='lower right', fontsize=9)\n",
    "plt.tight_layout()\n",
    "\n",
    "png_path = OUT_DIR / \"XOCTANE_vs_OCTANE_ttest_wilcoxon.png\"\n",
    "pdf_path = OUT_DIR / \"XOCTANE_vs_OCTANE_ttest_wilcoxon.pdf\"\n",
    "plt.savefig(png_path, dpi=300, bbox_inches=\"tight\")\n",
    "plt.savefig(pdf_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n=== Statistical Summary ===\")\n",
    "for res in [res_aucpr, res_roc]:\n",
    "    print(f\"{res['label']}:\")\n",
    "    print(f\"  Mean gain: {res['gain']:.4f}\")\n",
    "    print(f\"  Wilcoxon p-value: {res['wilcoxon_p']:.4f}\")\n",
    "    print(f\"  Paired t-test p-value: {res['t_p']:.4f}\")\n",
    "    print()\n",
    "print(f\"Figures saved to:\\n  {png_path}\\n  {pdf_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# LaTeX improvement table (computed from THIS per_run CSV)\n",
    "# IMPORTANT:\n",
    "# per_run_metrics_all_PIPELINE.csv does NOT contain a separate CP-MI-only vs CP-MI+SHAP model,\n",
    "# so CP-MI+SHAP over CP-MI deltas cannot be derived reliably from it unless both variants exist.\n",
    "# If you still want a table from this file, you MUST specify baseline/hybrid method labels.\n",
    "# Below uses BASE_METHOD vs HYBRID_METHOD (method-level proxy).\n",
    "# =============================================================================\n",
    "BASE_METHOD   = \"dC_aM\"\n",
    "HYBRID_METHOD = \"dE_aM\"\n",
    "\n",
    "def compute_mean_delta(setup, anomaly, win, kfold, pct):\n",
    "    sub = df[(df[\"setup\"]==setup)&(df[\"anomaly\"]==anomaly)&(df[\"win\"]==win)&(df[\"kfold\"]==kfold)&(df[\"pct\"]==pct)].copy()\n",
    "    sub = sub[sub[\"method\"].isin([BASE_METHOD, HYBRID_METHOD])].copy()\n",
    "    if sub.empty:\n",
    "        return None\n",
    "\n",
    "    keys = [\"setup\",\"anomaly\",\"win\",\"kfold\",\"pct\",\"workload\"]\n",
    "    b = sub[sub[\"method\"]==BASE_METHOD][keys+[\"auc_pr\",\"roc_auc\"]].rename(columns={\"auc_pr\":\"auc_b\",\"roc_auc\":\"roc_b\"})\n",
    "    h = sub[sub[\"method\"]==HYBRID_METHOD][keys+[\"auc_pr\",\"roc_auc\"]].rename(columns={\"auc_pr\":\"auc_h\",\"roc_auc\":\"roc_h\"})\n",
    "    m = b.merge(h, on=keys, how=\"inner\")\n",
    "    if m.empty:\n",
    "        return None\n",
    "    return float((m[\"auc_h\"]-m[\"auc_b\"]).mean()), float((m[\"roc_h\"]-m[\"roc_b\"]).mean())\n",
    "\n",
    "# Using your requested configs in the conversation:\n",
    "# Setup A: DDR4 WIN512 K3, anomalies DROOP + RH (RH is \"TRRespass\" in paper)\n",
    "# Setup B: DDR5 WIN1024 K5, anomalies DROOP + SPECTRE\n",
    "table_rows = []\n",
    "targets = [\n",
    "    (\"A\",\"DDR4\",\"DROOP\",512,3,80,\"Droop\"),\n",
    "    (\"A\",\"DDR4\",\"RH\",512,3,80,\"TRRespass\"),\n",
    "    (\"B\",\"DDR5\",\"DROOP\",1024,5,80,\"Droop\"),\n",
    "    (\"B\",\"DDR5\",\"SPECTRE\",1024,5,80,\"Spectre\"),\n",
    "]\n",
    "for setup_lbl, setup, anom, win, kf, pct, paper_anom in targets:\n",
    "    d = compute_mean_delta(setup, anom, win, kf, pct)\n",
    "    if d is None:\n",
    "        d_pr, d_roc = np.nan, np.nan\n",
    "    else:\n",
    "        d_pr, d_roc = d\n",
    "    table_rows.append((setup_lbl, paper_anom, d_pr, d_roc))\n",
    "\n",
    "avg_pr  = np.nanmean([r[2] for r in table_rows])\n",
    "avg_roc = np.nanmean([r[3] for r in table_rows])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93b76f-7b9d-4e4b-9210-0e637b3d460c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
