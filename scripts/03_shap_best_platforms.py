# Auto-generated from XOCTANE.ipynb (extracted).
# Patched for GitHub reproducibility: uses env vars instead of local absolute paths.
import os
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[1]
ROOT = Path(os.environ.get('XOCTANE_ROOT', str(REPO_ROOT))).resolve()
DATA_DIR_ENV = os.environ.get('XOCTANE_DATA_DIR', '')
if not DATA_DIR_ENV:
    raise SystemExit('Please set XOCTANE_DATA_DIR to the dataset root directory (see README).')
DATA_DIR = Path(DATA_DIR_ENV).resolve()

# Default output directories (created if missing)
RES_DIR = ROOT / 'Results'; RES_DIR.mkdir(parents=True, exist_ok=True)
FIG_DIR = ROOT / 'figures'; FIG_DIR.mkdir(parents=True, exist_ok=True)

# === SHAP explainability for BEST cases only (PER PLATFORM) ==========================
# Platform = DDR4, DDR5
#
# Reads winners from:
#   Results/BEST_in_DesignSpace_Post_per_platform_details.csv
#   (generated by your per-platform selector; contains one row per platform×anomaly,
#    with chosen WIN/K plus anomaly-specific best METHOD and best PCT)
#
# Uses your pipeline helpers if present; otherwise uses local rank-reader + simple selection.
#
# Outputs:
#   Results/Explainability_SHAP_BestPlatforms/
#       SHAP_BESTPLAT_full_<setup>_<anomaly>_WIN..._KF..._PCT..._METHOD....csv
#       SHAP_BESTPLAT_<subspace>_...csv
#       figs/BAR_BESTPLAT_<subspace>_...png
#       figs/BEE_BESTPLAT_...png
#       SHAP_BESTPLAT_master_all.csv
# =====================================================================================

import os, gc, re, unicodedata
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# ---- Paths ----

DETAILS_CSV = RES_DIR / "BEST_in_DesignSpace_Post_per_platform_details.csv"
OUT_DIR     = RES_DIR / "Explainability_SHAP_BestPlatforms"
FIG_DIR     = OUT_DIR / "figs"
OUT_DIR.mkdir(parents=True, exist_ok=True)
FIG_DIR.mkdir(parents=True, exist_ok=True)

SUBSPACES = ("compute","memory","sensors")
SEED      = int(globals().get("SEED", 42))

# ---- Require helpers from your pipeline (recommended) ----
_required = [
    "collect_raw_pairs_by_setup","build_windowed_raw_means","telemetry_cols",
    "robust_scale_train","apply_robust_scale",
    "slice_by_percent","intersect_selection_with_columns_robust",
]
_missing = [r for r in _required if r not in globals()]
if _missing:
    raise RuntimeError(
        f"Missing required helpers from your pipeline: {_missing}\n"
        f"Run the pipeline script first (or import it) so these functions exist in globals()."
    )

# ---- Rank roots (use your existing RANK_DIRS if present) ----
if "RANK_DIRS" not in globals():
    RANK_DIRS = [
        ROOT / "FeatureRankOUT",
        Path("/Volumes/Untitled") / "FeatureRankOUT",
        Path("/Volumes/Untitled") / "octaneX" / "FeatureRankOUT",
    ]

def _read_rank_list(rank_dirs, setup, win, kfold, sub):
    """Return ordered list of features from MMI FeatureRankOUT (first hit wins)."""
    fname = f"{setup}_{win}_{kfold}_0_{sub}.csv"
    for d in rank_dirs:
        p = Path(d) / fname
        if p.exists():
            try:
                df = pd.read_csv(p)
                col_f = "feature" if "feature" in df.columns else df.columns[0]
                return df[col_f].dropna().astype(str).tolist()
            except Exception as e:
                print(f"[WARN] cannot read {p}: {e}")
    return []

def _train_xgb_simple(X_tr, y_tr, seed=SEED):
    from xgboost import XGBClassifier
    clf = XGBClassifier(
        n_estimators=250, max_depth=4, learning_rate=0.06,
        subsample=0.9, colsample_bytree=0.9,
        random_state=seed, tree_method="hist", n_jobs=os.cpu_count()
    )
    clf.fit(X_tr, y_tr)
    return clf

def _compute_shap(clf, X_ref):
    """Return mean|SHAP| per feature (Series). Fall back to XGB gain if shap not installed."""
    try:
        import shap
        explainer = shap.TreeExplainer(clf, feature_perturbation="interventional")
        sv = explainer.shap_values(X_ref)
        if isinstance(sv, list):  # binary -> [neg,pos]
            vals = np.abs(sv[1]).mean(axis=0)
        else:
            vals = np.abs(sv).mean(axis=0)
        return pd.Series(vals, index=X_ref.columns, name="shap_mean_abs")
    except Exception:
        try:
            booster = clf.get_booster()
            gain = booster.get_score(importance_type="gain")
            s = pd.Series(gain, dtype=float)
            s = s.reindex(X_ref.columns).fillna(0.0)
            s.name = "shap_mean_abs"
            print("[WARN] shap unavailable; fell back to XGB gain.")
            return s
        except Exception:
            return pd.Series(0.0, index=X_ref.columns, name="shap_mean_abs")

def _beeswarm_plot(clf, X, title, out_png, max_display=40):
    try:
        import shap
        explainer = shap.TreeExplainer(clf, feature_perturbation="interventional")
        sv = explainer.shap_values(X)
        if isinstance(sv, list): sv = sv[1]
        plt.figure(figsize=(10,6))
        shap.summary_plot(sv, X, show=False, max_display=max_display)
        plt.title(title); plt.tight_layout()
        plt.savefig(out_png, dpi=220); plt.close()
    except Exception as e:
        print(f"[WARN] SHAP beeswarm skipped: {e}")

def _barplot_subspace(df_sub, title, out_png, topk=30):
    d = df_sub.sort_values("shap_mean_abs", ascending=False)
    if topk is not None and topk > 0:
        d = d.head(topk)
    plt.figure(figsize=(10, max(3.5, 0.25*len(d))))
    plt.barh(d["feature"], d["shap_mean_abs"])
    plt.gca().invert_yaxis()
    plt.xlabel("mean |SHAP| (validation set)")
    plt.title(title)
    plt.tight_layout()
    plt.savefig(out_png, dpi=220)
    plt.close()

# ---- Load per-platform details (winners) ----
if not DETAILS_CSV.exists():
    raise FileNotFoundError(
        f"Missing platform details CSV: {DETAILS_CSV}\n"
        f"Generate it first using your per-platform winners code:\n"
        f"  - BEST_in_DesignSpace_Post_per_platform.csv\n"
        f"  - BEST_in_DesignSpace_Post_per_platform_details.csv"
    )

winners = pd.read_csv(DETAILS_CSV).copy()

# Normalize/validate columns
# expected columns (from your generator):
# setup, anomaly, win, kfold, best_pct_by_median, method, auc_pr_median, roc_auc_median, ...
colmap = {}
if "best_method" in winners.columns and "method" not in winners.columns:
    colmap["best_method"] = "method"
winners = winners.rename(columns=colmap)

required_cols = ["setup","anomaly","win","kfold","best_pct_by_median","method"]
missing = [c for c in required_cols if c not in winners.columns]
if missing:
    raise KeyError(f"Platform details CSV missing required columns: {missing}. Have: {list(winners.columns)}")

# Keep only defined anomalies per platform (defensive)
ANOMALIES_BY_SETUP = {"DDR4": ["DROOP","RH"], "DDR5": ["DROOP","SPECTRE"]}
def _keep(row):
    s = str(row["setup"]).upper()
    a = str(row["anomaly"]).upper()
    return s in ANOMALIES_BY_SETUP and a in [x.upper() for x in ANOMALIES_BY_SETUP[s]]
winners = winners[winners.apply(_keep, axis=1)].copy()
winners = winners.reset_index(drop=True)

print(f"[OK] Loaded platform winners (details) rows: {len(winners)}")
print(winners[["setup","anomaly","win","kfold","best_pct_by_median","method"]].to_string(index=False))

# ---- Run explainability per platform×anomaly best config ----
all_rows = []

for _, row in winners.iterrows():
    setup   = str(row["setup"])
    anomaly = str(row["anomaly"])
    win     = int(row["win"])
    kfold   = int(row["kfold"])
    pct     = int(row["best_pct_by_median"])
    method  = str(row["method"])

    print(f"\n[RUN] SHAP explainability (BEST PLATFORM) → {setup}/{anomaly}  WIN={win}  K={kfold}  PCT={pct}  METHOD={method}")

    # 1) Build BENIGN/ANOM windows
    ben_pairs = collect_raw_pairs_by_setup(DATA_DIR, which="benign").get(setup, [])
    an_pairs  = collect_raw_pairs_by_setup(DATA_DIR, which="anomaly", anomaly=anomaly).get(setup, [])
    if not ben_pairs or not an_pairs:
        print(f"[SKIP] Missing RAW files for {setup}/{anomaly}")
        continue

    # NOTE: build_windowed_raw_means signature includes overlap_ratio in your pipeline.
    # If your helper requires it, it will be in globals() and this call will work.
    # If not, you can add overlap_ratio=... as needed.
    try:
        df_b = build_windowed_raw_means(ben_pairs, setup=setup, win=win, label="BENIGN", overlap_ratio=0.50)
        df_a = build_windowed_raw_means(an_pairs,  setup=setup, win=win, label=anomaly, overlap_ratio=(0.80 if anomaly.upper()=="DROOP" else 0.50))
    except TypeError:
        # fallback if your helper doesn't have overlap_ratio
        df_b = build_windowed_raw_means(ben_pairs, setup=setup, win=win, label="BENIGN")
        df_a = build_windowed_raw_means(an_pairs,  setup=setup, win=win, label=anomaly)

    if df_b.empty or df_a.empty:
        print(f"[SKIP] Empty windows for {setup}/{anomaly} WIN={win}")
        continue

    # 2) Robust scale on BENIGN; apply to all
    xb_cols = telemetry_cols(df_b)
    if not xb_cols:
        print(f"[SKIP] No telemetry cols for {setup}/{anomaly}")
        continue

    Xb = df_b[xb_cols].astype(float)
    mu, sd, q1, q2 = robust_scale_train(Xb.values, winsor=(2.0, 98.0))
    Xb_z = apply_robust_scale(Xb, mu, sd, q1, q2)

    Xa = df_a[[c for c in xb_cols if c in df_a.columns]].astype(float)
    Xa_z = apply_robust_scale(Xa, mu, sd, q1, q2)

    # Align columns (just in case)
    common_cols = [c for c in Xb_z.columns if c in Xa_z.columns]
    if not common_cols:
        print(f"[SKIP] No common scaled columns for {setup}/{anomaly}")
        continue
    Xb_z = Xb_z[common_cols]
    Xa_z = Xa_z[common_cols]

    X_all = pd.concat([Xb_z, Xa_z], ignore_index=True)
    y_all = np.concatenate([np.zeros(len(Xb_z), dtype=int), np.ones(len(Xa_z), dtype=int)])

    # 3) One validation mask per case
    rng = np.random.RandomState(SEED)
    idx = np.arange(len(y_all))
    val_mask = np.zeros_like(y_all, dtype=bool)
    val_mask[rng.choice(idx, size=max(1, int(0.30*len(y_all))), replace=False)] = True

    # 4) MMI selection at best pct (robust mapping against columns)
    full_lists = {sub: _read_rank_list(RANK_DIRS, setup, win, kfold, sub) for sub in SUBSPACES}

    # Your pipeline's slice_by_percent returns sel dict (not tuple). Handle both.
    sel_raw = slice_by_percent(full_lists, pct)
    if isinstance(sel_raw, tuple):
        sel_raw = sel_raw[0]

    sel_map = intersect_selection_with_columns_robust(sel_raw, set(X_all.columns))
    if isinstance(sel_map, tuple):
        sel_map = sel_map[0]

    chosen, feat_sub = [], {}
    for sub in SUBSPACES:
        feats = list(sel_map.get(sub, []) or [])
        for f in feats:
            feat_sub[f] = sub
        chosen.extend(feats)

    chosen = [c for c in dict.fromkeys(chosen) if c in X_all.columns]
    if not chosen:
        print(f"[SKIP] No features after mapping for {setup}/{anomaly} at WIN={win},K={kfold},PCT={pct}")
        continue

    X = X_all[chosen]
    X_tr, y_tr = X.loc[~val_mask], y_all[~val_mask]
    X_va, y_va = X.loc[val_mask],  y_all[val_mask]

    # 5) Train and compute SHAP
    clf     = _train_xgb_simple(X_tr, y_tr, seed=SEED)
    shap_s  = _compute_shap(clf, X_va)

    # z²-lift on validation (anomaly minus benign)
    ben_mask = (y_va == 0)
    an_mask  = (y_va == 1)
    if ben_mask.sum() == 0 or an_mask.sum() == 0:
        z2_lift = np.zeros(X_va.shape[1], dtype=float)
    else:
        z2_lift = ((X_va.values[an_mask]**2).mean(axis=0) - (X_va.values[ben_mask]**2).mean(axis=0))
    z2_s = pd.Series(z2_lift, index=X.columns, name="z2_lift")

    df_all = (
        pd.DataFrame({
            "feature": X.columns,
            "subspace": [feat_sub.get(c, "compute") for c in X.columns],
            "shap_mean_abs": shap_s.reindex(X.columns).fillna(0.0).values,
            "z2_lift": z2_s.reindex(X.columns).fillna(0.0).values,
            "setup": setup, "anomaly": anomaly, "win": win, "kfold": kfold, "pct": pct, "method": method
        })
        .sort_values("shap_mean_abs", ascending=False)
        .reset_index(drop=True)
    )

    # 6) Save outputs (full + per-subspace + plots)
    base = f"{setup}_{anomaly}_WIN{win}_KF{kfold}_PCT{pct}_M{method}"
    out_full = OUT_DIR / f"SHAP_BESTPLAT_full_{base}.csv"
    df_all.to_csv(out_full, index=False)
    print(f"[EXPL] full → {out_full}")

    for sub in SUBSPACES:
        sub_df = df_all[df_all["subspace"]==sub].copy()
        if sub_df.empty:
            continue
        out_sub = OUT_DIR / f"SHAP_BESTPLAT_{sub}_{base}.csv"
        sub_df.to_csv(out_sub, index=False)

        png = FIG_DIR / f"BAR_BESTPLAT_{sub}_{base}.png"
        _barplot_subspace(
            sub_df,
            title=f"{setup}/{anomaly} • {sub} • WIN={win} K={kfold} PCT={pct} • {method}",
            out_png=png,
            topk=30
        )
        print(f"[FIG] {png}")

    bees = FIG_DIR / f"BEE_BESTPLAT_{base}.png"
    _beeswarm_plot(
        clf, X_va,
        title=f"{setup}/{anomaly} • WIN={win} K={kfold} PCT={pct} • {method} (validation)",
        out_png=bees,
        max_display=40
    )

    all_rows.append(df_all)
    gc.collect()

# 7) Master table
if all_rows:
    master = pd.concat(all_rows, ignore_index=True)
    master_csv = OUT_DIR / "SHAP_BESTPLAT_master_all.csv"
    master.to_csv(master_csv, index=False)
    print(f"\n[OK] Master table → {master_csv}")
else:
    print("\n[WARN] No SHAP outputs were produced (check platform winners/ranks/data).")